{
 "cells": [
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d10fa50a",
   "metadata": {},
   "source": [
    "### Pre-train BERT\n",
    "* c09k_BERT_pretrain 코드에 이어, max_length를 512로 늘려서 pretrain 추가 수행\n",
    "* https://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python\n",
    "* https://huggingface.co/transformers/v3.2.0/training.html\n",
    "\n",
    "#### 이전 pre-train 결과에서 512로 늘려서 학습한 결과\n",
    "* validation loss가 더 이상 떨어지지 않고 overfit되었고, 분류 성능도 나아지지 않았다.\n",
    "* 학습 데이터 가공 과정에서 group_texts 함수를 적용하지 않은 데이터를 사용해 학습 결과를 관찰해보자\n",
    "![image-2.png](attachment:image-2.png)\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d5b238a7",
   "metadata": {},
   "source": [
    "* 문장을 일정 길이 이상으로 병합 하는 로직을 해제하여 데이터셋에 짧은 문장도 포함된 상태로 돌린 결과\n",
    "* overfit 되는 경향은 그대로 다시 나타남, c09k 전체 서브클래스의 특허 17만건을 다운받아서 pre-train을 다시 돌려보자\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e678dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "from datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.data import load\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3ddb6",
   "metadata": {},
   "source": [
    "### Tokenizer train data 생성\n",
    "* 약어 이후에 등장하는 마침표를 사용해 문장이 분리되지 않도록 조치를 해야 한다.\n",
    "* NLTK의 tokenizer를 사용해 문장 분리하기 위해 extra_abbreviations에 예외조건을 추가하여 준다.\n",
    "* 클린징이 끝난 데이터는 토크나이저 학습에 바로 사용하고, 이 데이터에서 테스트셋을 분리하여 pre-train 데이터로 사용예정\n",
    "    * https://cryptosalamander.tistory.com/140?category=1218889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8cce6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = load(\"tokenizers/punkt/english.pickle\")\n",
    "extra_abbreviations = [\n",
    "    'RE','re','pat', 'no', 'nos','vol','jan','feb','mar','apr','jun',\n",
    "    'jul','aug','sep','oct','nov','dec','eng','ser','ind','ed','pp',\n",
    "    'e.g','al','T.E.N.S', 'E.M.S','F.E','U.H.T.S.T','degree',\n",
    "    '/gm','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "    'P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "sent_tokenizer._params.abbrev_types.update(extra_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbed7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK의 tokenizer를 사용해 문장 분리(미사용)\n",
    "# https://cryptosalamander.tistory.com/140?category=1218889\n",
    "def sent_tokenize(input='./input.txt', output='./output.txt'):\n",
    "    sent_tokenizer = load(\"tokenizers/punkt/english.pickle\")\n",
    "    extra_abbreviations = [\n",
    "        'RE','re','pat', 'no', 'nos','vol','jan','feb','mar','apr','jun',\n",
    "        'jul','aug','sep','oct','nov','dec','eng','ser','ind','ed','pp',\n",
    "        'e.g','al','T.E.N.S', 'E.M.S','F.E','U.H.T.S.T','degree',\n",
    "        '/gm','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "        'P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "    sent_tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "\n",
    "    load_file=open(input,'r')\n",
    "    save_file=open(output,'w')\n",
    "    no_blank = False\n",
    "    while True:\n",
    "        line = load_file.readline()\n",
    "        if line == \"\":\n",
    "            break\n",
    "        if line.strip() == \"\":\n",
    "            if no_blank:\n",
    "                continue\n",
    "            save_file.write(f\"{line}\")\n",
    "        else:\n",
    "            print(line)\n",
    "            result_ = tokenizer.tokenize(line)\n",
    "            print(result_)\n",
    "            result  = [ f\"{cur_line}\\n\" for cur_line in result_ ]\n",
    "            for save_line in result:\n",
    "                save_file.write(save_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d86bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-84170d85970a0abc\n",
      "WARNING:datasets.builder:Reusing dataset text (/home/hdh/.cache/huggingface/datasets/text/default-84170d85970a0abc/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset = Dataset.from_text('data/c09k_corpus.txt')\n",
    "except:\n",
    "    kiwee_files = ['data/c09k_0001-1000.xlsx', 'data/c09k_1001-2000.xlsx', 'data/c09k_2001-3000.xlsx', 'data/c09k_3001-3935.xlsx']\n",
    "    with open('data/c09k_corpus.txt', 'a') as f:\n",
    "        f.truncate(0)\n",
    "        for i, fn in enumerate(kiwee_files):\n",
    "            tmp = pd.read_excel(fn).fillna('')\n",
    "            # pandas는 비어있는 컬럼의 dtype을 float로 바꿔서 인식한다. 그로 인해 토크나이징 할 데이터가 없으면 오류가 발생되어 fillna를 사용해 모두 텍스트로 인식시키도록 한다\n",
    "            # https://stackoverflow.com/questions/53953286/pandas-read-excel-blanks-in-string-columns-convert-to-floats-converting-via-st\n",
    "            col_text = ['발행번호', '발명의명칭', '요약', '대표청구항', '과제', '해결방안']\n",
    "            tmp = tmp[col_text]\n",
    "            for index, row in tmp.iterrows():\n",
    "        #         print(index, '\\n', row['발명의명칭'], row['요약'], row['대표청구항'], row['과제'], row['해결방안'], '\\n')\n",
    "                for col in col_text[1:]:\n",
    "        #             print('처리중인 데이터:', col, row[col], '\\n')\n",
    "                    if row[col].strip() == \"\":\n",
    "                        pass\n",
    "                    else:\n",
    "        #                 print(row[col].strip())\n",
    "                        row[col] = unicodedata.normalize('NFKC', row[col])\n",
    "                        # row[col] = unicodedata.normalize('NFC', row[col])  # 자음과 모음이 깨질 때는 NFC로 변환\n",
    "                        # NFD(Normalization Form Decomposition) : 자음과 모음이 분리\n",
    "                        # row[col] = unicodedata.normalize('NFKD', row[col])\n",
    "                        #     https://blog.naver.com/PostView.nhn?blogId=duswl0319&logNo=221516880642&from=search&redirect=Log&widgetTypeCall=true&directAccess=false\n",
    "                        row[col] = row[col].replace('\\n\\t',' ')\n",
    "                        row[col] = row[col].replace('\\n',' ')\n",
    "                        row[col] = row[col].replace('&lt;',' ')\n",
    "                        row[col] = row[col].replace('_x000d_',' ')\n",
    "                        row[col] = row[col].replace('\\t\\t',' ')\n",
    "                        row[col] = row[col].replace('@@',' ')\n",
    "                        row[col] = row[col].replace('.  .','.')\n",
    "                        row[col] = row[col].replace('. .','.')\n",
    "                        row[col] = row[col].replace('..','.')\n",
    "                        row[col] = row[col].replace('〜','~')\n",
    "                        row[col] = row[col].replace(' . ','.')\n",
    "                        row[col] = row[col].replace(' ． ','.')\n",
    "                        row[col] = row[col].replace('． ','.')\n",
    "                        row[col] = row[col].replace('. ','.')\n",
    "                        row[col] = row[col].replace('  ',' ')\n",
    "                        row[col] = row[col].replace('  ',' ')\n",
    "                        row[col] = row[col].replace('【과제】',' ')\n",
    "                        row[col] = row[col].replace('【요약】',' ')\n",
    "                        row[col] = row[col].replace('【해결 수단】',' ')\n",
    "                        str_tmp = sent_tokenizer.tokenize(row[col].strip())\n",
    "        #                 print('문장 분리: ', str_tmp, '\\n'*3)\n",
    "        #                 result  = [f\"{line}\\n\" for line in str_tmp]\n",
    "                        for line in str_tmp:\n",
    "                            f.write(f\"{line}\\n\")\n",
    "    dataset = Dataset.from_text('data/c09k_corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5131dc6",
   "metadata": {},
   "source": [
    "### 학습 데이터 분리/로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2272b29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading completed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('data/c09k_dataset.pkl','rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    print('dataset loading completed')\n",
    "except:\n",
    "    d = dataset.train_test_split(test_size=0.1)\n",
    "    with open('data/c09k_dataset.pkl','wb') as f:\n",
    "        pickle.dump(d, f)\n",
    "    print('dataset split/saving completed')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a042e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # d[\"train\"], d[\"test\"]\n",
    "# for t in d[\"train\"][\"text\"][:3]:\n",
    "#     print(t)\n",
    "#     print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc64d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_tokens = [\n",
    "#   \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "# ]\n",
    "# # if you want to train the tokenizer on both sets\n",
    "# # files = [\"train.txt\", \"test.txt\"]\n",
    "# # training the tokenizer on the training set\n",
    "# files = [\"data/c09k_corpus.txt\"]\n",
    "# # 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "# vocab_size = 8000\n",
    "# # maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "# max_length = 512\n",
    "# # whether to truncate\n",
    "# truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef9182b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"c09k_pretrained_bert\"\n",
    "vocab_size = 8000\n",
    "max_length = 512\n",
    "truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bf4df",
   "metadata": {},
   "source": [
    "### Pre-train data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b4658e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping some of the tokenizer config to config file, \n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "    tokenizer_cfg = {\"do_lower_case\": True,\n",
    "                     \"unk_token\": \"[UNK]\",\n",
    "                     \"sep_token\": \"[SEP]\",\n",
    "                     \"pad_token\": \"[PAD]\",\n",
    "                     \"cls_token\": \"[CLS]\",\n",
    "                     \"mask_token\": \"[MASK]\",\n",
    "                     \"model_max_length\": max_length,\n",
    "                     \"max_len\": max_length,\n",
    "                    }\n",
    "    json.dump(tokenizer_cfg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af37bfc",
   "metadata": {},
   "source": [
    "### Tokenizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81bd4e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# when the tokenizer is trained and configured, load it as BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "219f9b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "820274b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True,\n",
    "                                     max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9526a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "# encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "encode = encode_with_truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1791637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:02<00:00,  5.77ba/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/hdh/.cache/huggingface/datasets/text/default-84170d85970a0abc/0.0.0/cache-d979572c899f4700.arrow\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the train dataset\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_dataset = d[\"test\"].map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4789f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ecd7d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14769"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(train_dataset[:2])\n",
    "len(train_dataset)  # 14769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e884ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "if truncate_longer_samples:\n",
    "    # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    # remove other columns, and remain them as Python lists\n",
    "    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb19d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e6f35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ba5fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen=512로 학습한 결과가 성능이 좋지 않아, 그룹텍스트 함수를 적용하지 않고 다시 pre-train을 시도\n",
    "# def group_texts(examples):\n",
    "#     # Concatenate all texts.\n",
    "#     concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "#     total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "#     # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "#     # customize this part to your needs.\n",
    "#     if total_length >= max_length:\n",
    "#         total_length = (total_length // max_length) * max_length\n",
    "#     # Split by chunks of max_len.\n",
    "#     result = {\n",
    "#         k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "#         for k, t in concatenated_examples.items()\n",
    "#     }\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d21bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# # might be slower to preprocess.\n",
    "# #\n",
    "# # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "# if not truncate_longer_samples:\n",
    "#     train_dataset1 = train_dataset.map(group_texts, batched=True,\n",
    "#                                                                         desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "#     test_dataset1 = test_dataset.map(group_texts, batched=True,\n",
    "#                                                                     desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "#     # convert them from lists to torch tensors\n",
    "#     train_dataset1.set_format(\"torch\")\n",
    "#     test_dataset1.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9b521e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.set_format(\"torch\")\n",
    "# test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38b171d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14769, 1642)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)  # (14769, 1642)\n",
    "# len(train_dataset1), len(test_dataset1)  # (2171, 225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78f2309b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be0f7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dataset({\n",
    "# #     features: ['text', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
    "# #     num_rows: 14769\n",
    "# # })\n",
    "# len(train_dataset['special_tokens_mask'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d42a6a42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 1075, 1198, 39, 891, 7003, 16, 6244, 2185, 3629, 3826, 1623, 447, 300, 428, 750, 398, 2129, 1185, 7661, 2781, 16, 2969, 547, 414, 5272, 401, 2338, 7140, 2159, 645, 595, 616, 551, 16, 863, 1527, 389, 960, 676, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[ claim 15 ] 기판 표면이, 콜로이드 현탁액에 대한 젖어 성을 한층 높이기 위해서, 희산 또는 염기를 이용해 사전 처리 되는 것을 특징으로 하는, 청구항 14에 기재의 방법. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[21, 18, 1399, 1200, 1558, 1300, 1875, 16, 37, 4342, 283, 39, 12, 41, 13, 22, 611, 24, 1288, 9, 518, 578, 1401, 784, 17, 5718, 678, 1149, 457, 16, 37, 4342, 272, 39, 12, 42, 13, 24, 611, 4296, 9, 518, 578, 1401, 784, 17, 4488, 457, 16, 412, 37, 4342, 278, 39, 12, 43, 13, 28, 283, 611, 29, 265, 1288, 9, 518, 746, 12, 49, 13, 498, 5165, 578, 1401, 758, 1059, 18, 22, 18, 1399, 1200, 4682, 3097, 24, 611, 3359, 383, 539, 510, 16, 2314, 1826, 3097, 25, 611, 4187, 539, 954, 1059, 18, 23, 18, 1399, 1200, 1558, 1300, 1658, 382, 730, 968, 1828, 7408, 672, 1399, 1200, 746, 12, 49, 13, 518, 1094, 3377, 412, 2031, 1954, 980, 509, 389, 882, 2515, 16, 974, 1174, 448, 4340, 383, 873, 538, 230, 712, 5590, 1533, 2515, 4645, 571, 382, 7199, 1059, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1. 발명에 따른 감온 안료 조성물은, [ 0036 ] ( a ) 2 내지 4중량 % 의 적어도 1종의 전자 - 공여체 유기 염료 화합물, [ 0037 ] ( b ) 4 내지 10중량 % 의 적어도 1종의 전자 - 수용체 화합물, 및 [ 0038 ] ( c ) 86 내지 94중량 % 의 화학식 ( i ) 에 대응하는 적어도 1종의 화합물을 포함한다. 2. 발명에 따른 아릴기는 바람직하게는 4 내지 12개의 탄소 원자, 더욱더 바람직하게는 5 내지 6개의 탄소 원자를 포함한다. 3. 발명에 따른 감온 안료 조성물에서 온도 변화 조절제로서의 본 발명에 따른 화학식 ( i ) 의 화합물의 용도 및 이들의 특성규명에 관한 것인, 후술하는 설명의 보완으로부터 기인될 것인 추가의 단서 조건을 포함한다. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(train_dataset['input_ids'][i])\n",
    "    print(tokenizer.decode(train_dataset['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42187cb",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcc3326b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e84fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c426bb",
   "metadata": {},
   "source": [
    "### Pre-Training - Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fee13841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model checkpoint  : 8.30 실행 코드, max_len = 64로 학습된 모델을 로드하여 512로 변경 후 추가 학습\n",
    "# model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-7500\"))  # max_len = 64로 pre-train한 모델 로드 (8.22 완료)\n",
    "# model.config.max_length = 512  # 512로 추가 학습하기 위해 모델 설정 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80f3ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_chk = ['c09k_pretrained_bert', 'checkpoint-7500']\n",
    "save_chk = ['c09k_pretrained_bert_512_3', '']\n",
    "tok_chk = ['c09k_pretrained_bert', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b24678dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_pretrained_bert/checkpoint-7500/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert/checkpoint-4320\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_pretrained_bert/checkpoint-7500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at c09k_pretrained_bert/checkpoint-7500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(8000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=8000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(load_chk[0], load_chk[1]))  # max_len = 512로 추가 pre-train한 모델 로드 (8.30 완료)\n",
    "# model.config.max_length = 512  # 512로 추가 학습하기 위해 모델 설정 변경\n",
    "# model.config.max_position_embeddings = 512  # 512로 추가 학습하기 위해 모델 설정 변경\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_length=max_length, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c302756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize the model with the config\n",
    "# model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "# model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e2a7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "491d7a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(tok_chk[0], vocab_size=8000, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d2e5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f44b9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform predictions\n",
    "# # 인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치\n",
    "# # 본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다\n",
    "# examples = [\n",
    "#     \"인광성 유기 금속 이리듐 착체, [MASK] 소자, 발광 장치, 전자 기기, 및 조명 장치\",\n",
    "#     \"본 명세서는 화학식 1로 표시되는 [MASK] 및 이를 포함하는 유기 발광 소자에 관한 것이다\",\n",
    "#     \"인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, [MASK] 기기, 및 조명 장치\",\n",
    "#     \"본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 [MASK] 발광 소자에 관한 것이다\",\n",
    "# ]\n",
    "# for example in examples:\n",
    "#     print(fill_mask(example))\n",
    "#     for prediction in fill_mask(example):\n",
    "#         print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "#     print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc5f2ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 10000\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Num examples = 19400\n",
    "# Num Epochs = 50\n",
    "# Total optimization steps = 3750 = 750*50 = \n",
    "# 로드한 모델 추가학습\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_chk[0],          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # 'steps': evaluate each `logging_steps`, 'epoch'  : each epoch\n",
    "    overwrite_output_dir=False,     # 원래 True였으나 변경 (8.30) \n",
    "    num_train_epochs=500,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=8, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=4,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=8,  # evaluation batch size\n",
    "    logging_steps=10000,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=10000,\n",
    "    resume_from_checkpoint=True,  # 체크포인트를 이어서 학습 가능할지 확인을 위해 새로 추가(8.30)\n",
    "    load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "#     save_total_limit=5,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "418262dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.6 ms, sys: 0 ns, total: 3.6 ms\n",
      "Wall time: 2.81 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b987ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime # datetime 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01c3a7c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14769\n",
      "  Num Epochs = 500\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 230500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='53600' max='230500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 53600/230500 28:01:41 < 92:30:25, 0.53 it/s, Epoch 116.27/500]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.920500</td>\n",
       "      <td>3.150853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.183700</td>\n",
       "      <td>1.908772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.247700</td>\n",
       "      <td>1.467283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>1.299024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.566800</td>\n",
       "      <td>1.164209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1642\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_3/checkpoint-10000\n",
      "Configuration saved in c09k_pretrained_bert_512_3/checkpoint-10000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_3/checkpoint-10000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1642\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_3/checkpoint-20000\n",
      "Configuration saved in c09k_pretrained_bert_512_3/checkpoint-20000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_3/checkpoint-20000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1642\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_3/checkpoint-30000\n",
      "Configuration saved in c09k_pretrained_bert_512_3/checkpoint-30000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_3/checkpoint-30000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1642\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_3/checkpoint-40000\n",
      "Configuration saved in c09k_pretrained_bert_512_3/checkpoint-40000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_3/checkpoint-40000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: text, special_tokens_mask. If text, special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1642\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_3/checkpoint-50000\n",
      "Configuration saved in c09k_pretrained_bert_512_3/checkpoint-50000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_3/checkpoint-50000/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/trainer.py:1505\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1502\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1504\u001b[0m )\n\u001b[0;32m-> 1505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/trainer.py:1747\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1745\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1747\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1750\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1753\u001b[0m ):\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/trainer.py:2495\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2495\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()\n",
    "#  max_len=64 로드하여 epochs=10, logging_steps=50, save_steps=900,\n",
    "# load_chk = ['c09k_pretrained_bert', 'checkpoint-7500']\n",
    "# save_chk = ['c09k_pretrained_bert_512_3', '']\n",
    "# tok_chk = ['c09k_pretrained_bert', '']\n",
    "# [ 7/4610 00:09 < 2:20:59, 0.54 it/s, Epoch 0.01/10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa70dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17d3724f",
   "metadata": {},
   "source": [
    "### Masked-LM 성능 확인\n",
    "* 모델과 토크나이저를 한번은 로드 해줘야 하네. 그렇지 않으면 토크나이저가 CPU에 로드되어 있는 것으로 인식하여 에러 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee29baad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_pretrained_bert_512/checkpoint-7000/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_pretrained_bert_512/checkpoint-7000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at c09k_pretrained_bert_512/checkpoint-7000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n",
      "[{'score': 0.06595180928707123, 'token': 16, 'token_str': ',', 'sequence': '인광성 유기 금속 이리듐 착체,, 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.06593338400125504, 'token': 678, 'token_str': '유기', 'sequence': '인광성 유기 금속 이리듐 착체, 유기 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.04995021969079971, 'token': 805, 'token_str': '금속', 'sequence': '인광성 유기 금속 이리듐 착체, 금속 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.01198280043900013, 'token': 450, 'token_str': '전기', 'sequence': '인광성 유기 금속 이리듐 착체, 전기 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.01068327110260725, 'token': 412, 'token_str': '및', 'sequence': '인광성 유기 금속 이리듐 착체, 및 소자, 발광 장치, 전자 기기, 및 조명 장치'}]\n",
      "인광성 유기 금속 이리듐 착체,, 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.06595180928707123\n",
      "인광성 유기 금속 이리듐 착체, 유기 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.06593338400125504\n",
      "인광성 유기 금속 이리듐 착체, 금속 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.04995021969079971\n",
      "인광성 유기 금속 이리듐 착체, 전기 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.01198280043900013\n",
      "인광성 유기 금속 이리듐 착체, 및 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.01068327110260725\n",
      "==================================================\n",
      "[{'score': 0.08523005247116089, 'token': 18, 'token_str': '.', 'sequence': '본 명세서는 화학식 1로 표시되는. 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.05935471132397652, 'token': 882, 'token_str': '관한', 'sequence': '본 명세서는 화학식 1로 표시되는 관한 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.043748896569013596, 'token': 16, 'token_str': ',', 'sequence': '본 명세서는 화학식 1로 표시되는, 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.03933851793408394, 'token': 746, 'token_str': '화학식', 'sequence': '본 명세서는 화학식 1로 표시되는 화학식 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.024272534996271133, 'token': 1138, 'token_str': '표시되는', 'sequence': '본 명세서는 화학식 1로 표시되는 표시되는 및 이를 포함하는 유기 발광 소자에 관한 것이다'}]\n",
      "본 명세서는 화학식 1로 표시되는. 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.08523005247116089\n",
      "본 명세서는 화학식 1로 표시되는 관한 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.05935471132397652\n",
      "본 명세서는 화학식 1로 표시되는, 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.043748896569013596\n",
      "본 명세서는 화학식 1로 표시되는 화학식 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.03933851793408394\n",
      "본 명세서는 화학식 1로 표시되는 표시되는 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.024272534996271133\n",
      "==================================================\n",
      "[{'score': 0.07487452030181885, 'token': 16, 'token_str': ',', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치,, 기기, 및 조명 장치'}, {'score': 0.052736397832632065, 'token': 805, 'token_str': '금속', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 금속 기기, 및 조명 장치'}, {'score': 0.05035865306854248, 'token': 678, 'token_str': '유기', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 유기 기기, 및 조명 장치'}, {'score': 0.009527969174087048, 'token': 1193, 'token_str': '발광', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 발광 기기, 및 조명 장치'}, {'score': 0.009037600830197334, 'token': 450, 'token_str': '전기', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전기 기기, 및 조명 장치'}]\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치,, 기기, 및 조명 장치, confidence: 0.07487452030181885\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 금속 기기, 및 조명 장치, confidence: 0.052736397832632065\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 유기 기기, 및 조명 장치, confidence: 0.05035865306854248\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 발광 기기, 및 조명 장치, confidence: 0.009527969174087048\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전기 기기, 및 조명 장치, confidence: 0.009037600830197334\n",
      "==================================================\n",
      "[{'score': 0.08683860301971436, 'token': 18, 'token_str': '.', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는. 발광 소자에 관한 것이다'}, {'score': 0.059315577149391174, 'token': 882, 'token_str': '관한', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 관한 발광 소자에 관한 것이다'}, {'score': 0.034790921956300735, 'token': 746, 'token_str': '화학식', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 화학식 발광 소자에 관한 것이다'}, {'score': 0.032986629754304886, 'token': 1138, 'token_str': '표시되는', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 표시되는 발광 소자에 관한 것이다'}, {'score': 0.03141077980399132, 'token': 16, 'token_str': ',', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는, 발광 소자에 관한 것이다'}]\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는. 발광 소자에 관한 것이다, confidence: 0.08683860301971436\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 관한 발광 소자에 관한 것이다, confidence: 0.059315577149391174\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 화학식 발광 소자에 관한 것이다, confidence: 0.034790921956300735\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 표시되는 발광 소자에 관한 것이다, confidence: 0.032986629754304886\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는, 발광 소자에 관한 것이다, confidence: 0.03141077980399132\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model1 = BertForMaskedLM.from_pretrained(os.path.join('c09k_pretrained_bert_512', \"checkpoint-7000\"))\n",
    "# load the tokenizer\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "tokenizer1 = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)\n",
    "fill_mask = pipeline(\"fill-mask\", model=model1, tokenizer=tokenizer1)\n",
    "# perform predictions\n",
    "# 인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치\n",
    "# 본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다\n",
    "examples = [\n",
    "    \"인광성 유기 금속 이리듐 착체, [MASK] 소자, 발광 장치, 전자 기기, 및 조명 장치\",\n",
    "    \"본 명세서는 화학식 1로 표시되는 [MASK] 및 이를 포함하는 유기 발광 소자에 관한 것이다\",\n",
    "    \"인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, [MASK] 기기, 및 조명 장치\",\n",
    "    \"본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 [MASK] 발광 소자에 관한 것이다\",\n",
    "]\n",
    "for example in examples:\n",
    "    print(fill_mask(example))\n",
    "    for prediction in fill_mask(example):\n",
    "        print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fdb28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e010c90",
   "metadata": {},
   "source": [
    "### 그 외 참고용 자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명선 책임의 문장 분리 코드(3글자 이후 마침표가 등장하는 경우에 간혹 분리되는 경우가 있다. 미사용)\n",
    "# REG_SENT_KO=r'([ㄱ-ㅣ가-힣]+[.]|[\\n]|[:;!?])'\n",
    "# REG_SENT_EN=r'([a-zA-Z]+[.]\\s|[\\n]|[:;!?])'\n",
    "\n",
    "# def split_sentence(doc, regex):\n",
    "#     s = 0\n",
    "#     for m in re.finditer(regex, doc):\n",
    "#         sent = doc[s:m.end()].strip()\n",
    "#         s = m.end()\n",
    "#         if not sent:\n",
    "#             continue\n",
    "#         yield sent\n",
    "\n",
    "#     if s < len(doc):\n",
    "#         sent = doc[s:].strip()\n",
    "#         if sent:\n",
    "#             yield sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5420d",
   "metadata": {},
   "source": [
    "### using personal dataset\n",
    "* https://huggingface.co/docs/datasets/dataset_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you have huge custom dataset separated into files\n",
    "# # load the splitted files\n",
    "# files = [\"train1.txt\", \"train2.txt\"] # train3.txt, etc.\n",
    "# dataset = load_dataset(\"text\", data_files=files, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4154edda",
   "metadata": {},
   "source": [
    "### 문장 클린징 처리 전/후 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc03d8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이산화티탄-염화리튬 전기변색물질, 이를 이용한전기변색장치용 파우더 및 필름전극의 제조방법', '본 발명은 이산화티탄-염화리튬 전기변색물질 및 이를 이용한 전기변색장치용 파우더 및 필름전극의 제조방법에 관한 것으로, 이산화티탄에 염화리튬이 균일하게 혼합된 물질을 합성하여 제조되며 상기 이산화티탄-염화리튬에서 티탄 대 리튬의 몰 함량 비율이 1 : x (0.5 x 2) 범위를 갖는 이산화티탄-염화리튬을 졸-겔 방법으로 합성함으로써 표면적이 극대화되는 다공성화 또는 그 물질 내에 존재하는 높은 함량의 리튬 이온으로 인하여 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 매우 뛰어난 발명인 것이다.', '이산화티탄-염화리튬 파우더에 아세틸아세톤이 포함된 증류수를 첨가하여 끈적거리는 상태가 될 때까지 교반하는 단계; 상기 용액에 증류수와 트리톤 X-100을 첨가하여 완전히 균일한 용액이 될 때까지 다시 교반하는 단계; 및 상기 용액을 전도성 유리인 ITO 위에 얇게 입힌 후, 상온에서 건조한 다음 열처리하는 단계를 포함함을 특징으로 하는 전기변색장치용 이산화티탄-염화리튬 필름전극의 제조방법.', '1 발명은 상기의 제문제점을 감안하여 안출한 것으로, 본 발명에서는 이산화티탄-염화리튬을 새롭게 합성하여 전기변색 물질로서 실용화함에 그 목적이 있다.', '2 발명의 다른 목적은 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 이산화티탄-염화리튬 전기변색물질을 제공하는데 있다.']\n",
      "['전기 화학 디바이스 및 그것을 형성하는 방법', '전기 화학 디바이스에 대해서, 개시한다.전기 화학 디바이스는, 제 1의 투명 도전층, 제 1의 투명 도전층 위에 있는 에렉트로 크로믹층, 에렉트로 크로믹층 위에 있는 대향 전극층, 제 2의 투명 도전층을 갖추어 23°C으로 0.68초/mm이하의 스위칭 속도 파라미터를 가진다.', '제 1의 투명 도전층과 전기 제 1의 투명 도전층 위에 있는 음극 전기 화학층과 전기 음극 전기 화학층 위에 있는 양극 전기 화학층과 제 2의 투명 도전층과 (을)를 갖추는 전기 화학 디바이스이며, 전기 전기 화학 디바이스는, 23°C으로 0.68초/mm이하인 제 1의 스위칭 속도 파라미터, -20°C으로 1.0초/mm이하인 제 2의 스위칭 속도 파라미터, 1.5/log(오옴) 미만인 log|Z|의 impedance파라미터, 또는 -20°C으로 8%미만인 착색 투과률 파라미터 중 적어도 1개를 가지는, 전기 화학 디바이스.', '이산화 바나듐 함유 입자, 사모 크로믹 필름 및 이산화 바나듐 함유 입자의 제조 방법', '사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.65~2.05의 범위내인 원소 A와 폴링의 전기 음성도가 2.10~2.54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['text'][0:5])\n",
    "print(dataset['text'][-5:])\n",
    "# 문장 클린징 처리 전\n",
    "# ['이산화티탄-염화리튬 전기변색물질, 이를 이용한전기변색장치용 파우더 및 필름전극의 제조방법', '본 발명은 이산화티탄-염화리튬 전기변색물질 및 이를 이용한 전기변색장치용 파우더 및 필름전극의 제조방법에 관한 것으로, 이산화티탄에 염화리튬이 균일하게 혼합된 물질을 합성하여 제조되며 상기 이산화티탄-염화리튬에서 티탄 대 리튬의 몰 함량 비율이 1 : x (0.5 &lt; x &lt; 2) 범위를 갖는 이산화티탄-염화리튬을 졸-겔 방법으로 합성함으로써 표면적이 극대화되는 다공성화 또는 그 물질 내에 존재하는 높은 함량의 리튬 이온으로 인하여 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 매우 뛰어난 발명인 것이다.', '이산화티탄-염화리튬 파우더에 아세틸아세톤이 포함된 증류수를 첨가하여 끈적거리는 상태가 될 때까지 교반하는 단계; 상기 용액에 증류수와 트리톤 X-100을 첨가하여 완전히 균일한 용액이 될 때까지 다시 교반하는 단계; 및 상기 용액을 전도성 유리인 ITO 위에 얇게 입힌 후, 상온에서 건조한 다음 열처리하는 단계를 포함함을 특징으로 하는 전기변색장치용 이산화티탄-염화리튬 필름전극의 제조방법.', '1@@발명은 상기의 제문제점을 감안하여 안출한 것으로, 본 발명에서는 이산화티탄-염화리튬을 새롭게 합성하여 전기변색 물질로서 실용화함에 그 목적이 있다.', '2@@발명의 다른 목적은 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 이산화티탄-염화리튬 전기변색물질을 제공하는데 있다.']\n",
    "# ['사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며,', '산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.', '６５〜２． 05의 범위내인 원소 A와 폴링의 전기 음성도가 2.', '１０〜２． 54의 범위내인 원소 B만이 함유 되어', '바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\n",
    "# 처리 후에는 1문장으로 처리될 데이터가 무려 5문장으로 나눠진다!\n",
    "\n",
    "# 문장 클린징 처리 후\n",
    "# ['이산화티탄-염화리튬 전기변색물질, 이를 이용한전기변색장치용 파우더 및 필름전극의 제조방법', '본 발명은 이산화티탄-염화리튬 전기변색물질 및 이를 이용한 전기변색장치용 파우더 및 필름전극의 제조방법에 관한 것으로, 이산화티탄에 염화리튬이 균일하게 혼합된 물질을 합성하여 제조되며 상기 이산화티탄-염화리튬에서 티탄 대 리튬의 몰 함량 비율이 1 : x (0.5 x 2) 범위를 갖는 이산화티탄-염화리튬을 졸-겔 방법으로 합성함으로써 표면적이 극대화되는 다공성화 또는 그 물질 내에 존재하는 높은 함량의 리튬 이온으로 인하여 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 매우 뛰어난 발명인 것이다.', '이산화티탄-염화리튬 파우더에 아세틸아세톤이 포함된 증류수를 첨가하여 끈적거리는 상태가 될 때까지 교반하는 단계; 상기 용액에 증류수와 트리톤 X-100을 첨가하여 완전히 균일한 용액이 될 때까지 다시 교반하는 단계; 및 상기 용액을 전도성 유리인 ITO 위에 얇게 입힌 후, 상온에서 건조한 다음 열처리하는 단계를 포함함을 특징으로 하는 전기변색장치용 이산화티탄-염화리튬 필름전극의 제조방법.', '1 발명은 상기의 제문제점을 감안하여 안출한 것으로, 본 발명에서는 이산화티탄-염화리튬을 새롭게 합성하여 전기변색 물질로서 실용화함에 그 목적이 있다.', '2 발명의 다른 목적은 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 이산화티탄-염화리튬 전기변색물질을 제공하는데 있다.']\n",
    "# ['전기 화학 디바이스 및 그것을 형성하는 방법', '전기 화학 디바이스에 대해서, 개시한다.전기 화학 디바이스는, 제 1의 투명 도전층, 제 1의 투명 도전층 위에 있는 에렉트로 크로믹층, 에렉트로 크로믹층 위에 있는 대향 전극층, 제 2의 투명 도전층을 갖추어 23°C으로 0.68초/mm이하의 스위칭 속도 파라미터를 가진다.', '제 1의 투명 도전층과 전기 제 1의 투명 도전층 위에 있는 음극 전기 화학층과 전기 음극 전기 화학층 위에 있는 양극 전기 화학층과 제 2의 투명 도전층과 (을)를 갖추는 전기 화학 디바이스이며, 전기 전기 화학 디바이스는, 23°C으로 0.68초/mm이하인 제 1의 스위칭 속도 파라미터, -20°C으로 1.0초/mm이하인 제 2의 스위칭 속도 파라미터, 1.5/log(오옴) 미만인 log|Z|의 impedance파라미터, 또는 -20°C으로 8%미만인 착색 투과률 파라미터 중 적어도 1개를 가지는, 전기 화학 디바이스.', '이산화 바나듐 함유 입자, 사모 크로믹 필름 및 이산화 바나듐 함유 입자의 제조 방법', \n",
    "#  '사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.65〜2.05의 범위내인 원소 A와 폴링의 전기 음성도가 2.10〜2.54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f00a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# ['사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.',\n",
    "#  '６５〜２． 05의 범위내인 원소 A와 폴링의 전기 음성도가 2.',\n",
    "#  '１０〜２． 54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\n",
    "#  위와 같이 소수점 앞 뒤로 공백이 존재하는데, 이로 인해 소수점에서 문장 분리가 되버린다. 오류 조치가 필요\n",
    "#  \"\"\"\n",
    "# 아래와 같이 구두점 앞뒤 블랭크를 제거하여 일단 구분\n",
    "# row[col] = unicodedata.normalize('NFKC', row[col])\n",
    "# https://blog.naver.com/PostView.nhn?blogId=duswl0319&logNo=221516880642&from=search&redirect=Log&widgetTypeCall=true&directAccess=false\n",
    "# row[col] = row[col].replace(' . ','.')\n",
    "# row[col] = row[col].replace(' ． ','.')\n",
    "# row[col] = row[col].replace('． ','.')\n",
    "# row[col] = row[col].replace('. ','.')\n",
    "# row[col] = row[col].replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c46427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train the tokenizer from scratch (especially if you have custom\n",
    "# dataset loaded as datasets object), then run this cell to save it as files\n",
    "# but if you already have your custom data as text files, there is no point using this\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    \"\"\"Utility function to save dataset text to disk,\n",
    "    useful for using the texts to train the tokenizer \n",
    "    (as the tokenizer accepts files)\"\"\"\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_dataset.features.keys()\n",
    "# # dict_keys(['text', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'])\n",
    "# total_length = len(list(chain(train_dataset['text'])))  # total_length = 14769\n",
    "# max_length  # 512\n",
    "# total_length = (total_length // max_length) * max_length\n",
    "# # total_length  # 14336, total_length // max_length = 28.8457..., 28 * 512 = 14336"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_pretrain",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
