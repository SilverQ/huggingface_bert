{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fadbc0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://krishansubudhi.github.io/deeplearning/2019/09/26/BertAttention.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580b7d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/huggingface_bert/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from pytorch_transformers import BertConfig, BertTokenizer,  BertModel\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "from datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import nltk\n",
    "from nltk.data import load\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "485fa630",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"c09k_finetuned_bert2\"\n",
    "tokenizer_path = 'c09k_tokenizer_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb9802a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "?BertForSequenceClassification.from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a17a33cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_finetuned_bert2/checkpoint-6000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_finetuned_bert/checkpoint-3000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_attentions\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert2/checkpoint-6000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert2/checkpoint-6000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "# config = BertConfig.from_pretrained(model_path, \"checkpoint-6000\")\n",
    "# config.output_attentions = True\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    os.path.join(model_path, \"checkpoint-6000\"), return_dict=True, num_labels=18,\n",
    "    output_attentions=True).to('cuda')\n",
    "# load the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c56036e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file c09k_tokenizer_2/added_tokens.json. We won't load it.\n",
      "Didn't find file c09k_tokenizer_2/special_tokens_map.json. We won't load it.\n",
      "Didn't find file c09k_tokenizer_2/tokenizer_config.json. We won't load it.\n",
      "loading file c09k_tokenizer_2/vocab.txt\n",
      "loading file c09k_tokenizer_2/tokenizer.json\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "/home/hdh/PycharmProjects/huggingface_bert/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_path, vocab_size=8000, local_files_only=True)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "# optimizer = A AdamW(model1.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3696982",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9796d8de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9881\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5203\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = pd.read_csv('data/train_C09K11_220715.txt', sep='\\t')  # text와 라벨 파일\n",
    "test_dataset_df = pd.read_csv('data/test_C09K11_220715.txt', sep='\\t')\n",
    "train_dataset = Dataset.from_pandas(train_data_df)  # Dataset 객체 생성\n",
    "test_dataset = Dataset.from_pandas(test_dataset_df)\n",
    "finetune_dataset = DatasetDict()  # DatasetDict 객체 생성\n",
    "finetune_dataset['train'] = train_dataset\n",
    "finetune_dataset['test'] = test_dataset\n",
    "finetune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d64af72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune_dataset['test']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbba3091",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(finetune_dataset['test']['text'][:16], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94f8b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 함수 객체 생성\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=True)\n",
    "def preprocess_function1(examples):\n",
    "    return examples['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fb2c027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.02ba/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  7.03ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_finetune_dataset = finetune_dataset.map(preprocess_function, batched=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19c9fa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['보안 인쇄물의 위변조 확인 방법', '보안 인쇄물의 위변조 확인 방법']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_finetune_dataset['test']['text'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedec835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "173386c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# training_args = TrainingArguments(output_dir=\"c09k_finetuned_bert\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"c09k_finetuned_bert_512\",\n",
    "    evaluation_strategy=\"epoch\",    # 'steps': evaluate each `logging_steps`, 'epoch'  : each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c436bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '보안 인쇄물의 위변조 확인 방법'\n",
    "tok = tokenizer(sample_text, truncation=True, max_length=512, padding=True)\n",
    "# tok\n",
    "ids = torch.tensor(tokenizer.convert_tokens_to_ids(tok)).unsqueeze(0).to('cuda')\n",
    "with torch.no_grad():\n",
    "    output = model(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "901560c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2548, 0.4683, 0.2769],\n",
       "          [0.2732, 0.4830, 0.2438],\n",
       "          [0.2737, 0.4782, 0.2480]],\n",
       "\n",
       "         [[0.5247, 0.1906, 0.2847],\n",
       "          [0.4911, 0.1865, 0.3224],\n",
       "          [0.5113, 0.1720, 0.3167]],\n",
       "\n",
       "         [[0.2249, 0.5777, 0.1974],\n",
       "          [0.2619, 0.5478, 0.1903],\n",
       "          [0.2197, 0.5911, 0.1892]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.3967, 0.3252, 0.2781],\n",
       "          [0.3909, 0.3510, 0.2581],\n",
       "          [0.3977, 0.3278, 0.2746]],\n",
       "\n",
       "         [[0.1652, 0.3719, 0.4629],\n",
       "          [0.1792, 0.3443, 0.4765],\n",
       "          [0.1594, 0.3679, 0.4728]],\n",
       "\n",
       "         [[0.3092, 0.5859, 0.1049],\n",
       "          [0.3665, 0.5183, 0.1152],\n",
       "          [0.3536, 0.5253, 0.1211]]],\n",
       "\n",
       "\n",
       "        [[[0.4473, 0.2110, 0.3417],\n",
       "          [0.4447, 0.2051, 0.3502],\n",
       "          [0.4582, 0.2106, 0.3312]],\n",
       "\n",
       "         [[0.4159, 0.3488, 0.2354],\n",
       "          [0.4499, 0.3128, 0.2372],\n",
       "          [0.4364, 0.3445, 0.2191]],\n",
       "\n",
       "         [[0.3453, 0.2429, 0.4119],\n",
       "          [0.3677, 0.2574, 0.3749],\n",
       "          [0.4157, 0.2385, 0.3458]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.3855, 0.3240, 0.2905],\n",
       "          [0.4321, 0.2943, 0.2736],\n",
       "          [0.3798, 0.3264, 0.2938]],\n",
       "\n",
       "         [[0.2868, 0.4142, 0.2991],\n",
       "          [0.2985, 0.3574, 0.3442],\n",
       "          [0.3094, 0.4090, 0.2816]],\n",
       "\n",
       "         [[0.4975, 0.2245, 0.2780],\n",
       "          [0.5207, 0.2245, 0.2548],\n",
       "          [0.5047, 0.2602, 0.2351]]],\n",
       "\n",
       "\n",
       "        [[[0.3544, 0.2694, 0.3762],\n",
       "          [0.3669, 0.2656, 0.3675],\n",
       "          [0.3698, 0.2916, 0.3386]],\n",
       "\n",
       "         [[0.3667, 0.3752, 0.2581],\n",
       "          [0.4723, 0.2798, 0.2478],\n",
       "          [0.4174, 0.3652, 0.2174]],\n",
       "\n",
       "         [[0.3610, 0.3347, 0.3043],\n",
       "          [0.3509, 0.3138, 0.3353],\n",
       "          [0.3682, 0.3153, 0.3166]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.2571, 0.3750, 0.3680],\n",
       "          [0.2632, 0.3244, 0.4124],\n",
       "          [0.2423, 0.3766, 0.3811]],\n",
       "\n",
       "         [[0.2676, 0.3989, 0.3334],\n",
       "          [0.3166, 0.3425, 0.3410],\n",
       "          [0.2676, 0.3919, 0.3405]],\n",
       "\n",
       "         [[0.3271, 0.3532, 0.3197],\n",
       "          [0.3727, 0.2858, 0.3415],\n",
       "          [0.3563, 0.2877, 0.3560]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.3220, 0.3364, 0.3415],\n",
       "          [0.3200, 0.3337, 0.3463],\n",
       "          [0.3207, 0.3333, 0.3460]],\n",
       "\n",
       "         [[0.3414, 0.3478, 0.3108],\n",
       "          [0.3423, 0.3444, 0.3133],\n",
       "          [0.3423, 0.3411, 0.3166]],\n",
       "\n",
       "         [[0.3318, 0.3522, 0.3160],\n",
       "          [0.3333, 0.3484, 0.3182],\n",
       "          [0.3312, 0.3491, 0.3198]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.3113, 0.3777, 0.3110],\n",
       "          [0.3132, 0.3707, 0.3162],\n",
       "          [0.3107, 0.3716, 0.3177]],\n",
       "\n",
       "         [[0.3522, 0.3565, 0.2913],\n",
       "          [0.3453, 0.3517, 0.3031],\n",
       "          [0.3459, 0.3483, 0.3057]],\n",
       "\n",
       "         [[0.3245, 0.3394, 0.3361],\n",
       "          [0.3231, 0.3374, 0.3395],\n",
       "          [0.3250, 0.3359, 0.3391]]],\n",
       "\n",
       "\n",
       "        [[[0.3496, 0.3332, 0.3172],\n",
       "          [0.3454, 0.3343, 0.3203],\n",
       "          [0.3452, 0.3335, 0.3213]],\n",
       "\n",
       "         [[0.3533, 0.3586, 0.2881],\n",
       "          [0.3538, 0.3518, 0.2943],\n",
       "          [0.3508, 0.3536, 0.2955]],\n",
       "\n",
       "         [[0.3312, 0.3426, 0.3262],\n",
       "          [0.3304, 0.3413, 0.3284],\n",
       "          [0.3308, 0.3403, 0.3289]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.3562, 0.3303, 0.3135],\n",
       "          [0.3539, 0.3277, 0.3184],\n",
       "          [0.3543, 0.3264, 0.3193]],\n",
       "\n",
       "         [[0.3420, 0.3466, 0.3115],\n",
       "          [0.3416, 0.3411, 0.3173],\n",
       "          [0.3404, 0.3407, 0.3189]],\n",
       "\n",
       "         [[0.3616, 0.3272, 0.3111],\n",
       "          [0.3610, 0.3252, 0.3138],\n",
       "          [0.3594, 0.3268, 0.3139]]],\n",
       "\n",
       "\n",
       "        [[[0.3640, 0.3518, 0.2842],\n",
       "          [0.3597, 0.3490, 0.2913],\n",
       "          [0.3601, 0.3491, 0.2909]],\n",
       "\n",
       "         [[0.3437, 0.3473, 0.3090],\n",
       "          [0.3417, 0.3459, 0.3124],\n",
       "          [0.3411, 0.3458, 0.3131]],\n",
       "\n",
       "         [[0.3313, 0.3348, 0.3338],\n",
       "          [0.3306, 0.3338, 0.3355],\n",
       "          [0.3307, 0.3332, 0.3361]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.3261, 0.3479, 0.3260],\n",
       "          [0.3272, 0.3439, 0.3289],\n",
       "          [0.3265, 0.3445, 0.3290]],\n",
       "\n",
       "         [[0.3400, 0.3525, 0.3075],\n",
       "          [0.3386, 0.3496, 0.3117],\n",
       "          [0.3385, 0.3490, 0.3125]],\n",
       "\n",
       "         [[0.3381, 0.3491, 0.3128],\n",
       "          [0.3356, 0.3467, 0.3177],\n",
       "          [0.3354, 0.3468, 0.3177]]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output['logits']\n",
    "# output['attentions']\n",
    "attentions = torch.cat(output['attentions']).to('cpu')\n",
    "attentions.shape\n",
    "attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41abb31f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabdbe6",
   "metadata": {},
   "source": [
    "### Plot Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac58828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface_bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
