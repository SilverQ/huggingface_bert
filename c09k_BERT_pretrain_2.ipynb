{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10fa50a",
   "metadata": {},
   "source": [
    "### Pre-train BERT\n",
    "* c09k_BERT_pretrain 코드에 이어, max_length를 512로 늘려서 pretrain 추가 수행\n",
    "* https://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python\n",
    "* https://huggingface.co/transformers/v3.2.0/training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e678dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "from datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.data import load\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3ddb6",
   "metadata": {},
   "source": [
    "### Tokenizer train data 생성\n",
    "* 약어 이후에 등장하는 마침표를 사용해 문장이 분리되지 않도록 조치를 해야 한다.\n",
    "* NLTK의 tokenizer를 사용해 문장 분리하기 위해 extra_abbreviations에 예외조건을 추가하여 준다.\n",
    "* 클린징이 끝난 데이터는 토크나이저 학습에 바로 사용하고, 이 데이터에서 테스트셋을 분리하여 pre-train 데이터로 사용예정\n",
    "    * https://cryptosalamander.tistory.com/140?category=1218889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8cce6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = load(\"tokenizers/punkt/english.pickle\")\n",
    "extra_abbreviations = [\n",
    "    'RE','re','pat', 'no', 'nos','vol','jan','feb','mar','apr','jun',\n",
    "    'jul','aug','sep','oct','nov','dec','eng','ser','ind','ed','pp',\n",
    "    'e.g','al','T.E.N.S', 'E.M.S','F.E','U.H.T.S.T','degree',\n",
    "    '/gm','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "    'P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "sent_tokenizer._params.abbrev_types.update(extra_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbed7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK의 tokenizer를 사용해 문장 분리(미사용)\n",
    "# https://cryptosalamander.tistory.com/140?category=1218889\n",
    "def sent_tokenize(input='./input.txt', output='./output.txt'):\n",
    "    sent_tokenizer = load(\"tokenizers/punkt/english.pickle\")\n",
    "    extra_abbreviations = [\n",
    "        'RE','re','pat', 'no', 'nos','vol','jan','feb','mar','apr','jun',\n",
    "        'jul','aug','sep','oct','nov','dec','eng','ser','ind','ed','pp',\n",
    "        'e.g','al','T.E.N.S', 'E.M.S','F.E','U.H.T.S.T','degree',\n",
    "        '/gm','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "        'P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "    sent_tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "\n",
    "    load_file=open(input,'r')\n",
    "    save_file=open(output,'w')\n",
    "    no_blank = False\n",
    "    while True:\n",
    "        line = load_file.readline()\n",
    "        if line == \"\":\n",
    "            break\n",
    "        if line.strip() == \"\":\n",
    "            if no_blank:\n",
    "                continue\n",
    "            save_file.write(f\"{line}\")\n",
    "        else:\n",
    "            print(line)\n",
    "            result_ = tokenizer.tokenize(line)\n",
    "            print(result_)\n",
    "            result  = [ f\"{cur_line}\\n\" for cur_line in result_ ]\n",
    "            for save_line in result:\n",
    "                save_file.write(save_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d86bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-84170d85970a0abc\n",
      "WARNING:datasets.builder:Reusing dataset text (/home/hdh/.cache/huggingface/datasets/text/default-84170d85970a0abc/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset = Dataset.from_text('data/c09k_corpus.txt')\n",
    "except:\n",
    "    kiwee_files = ['data/c09k_0001-1000.xlsx', 'data/c09k_1001-2000.xlsx', 'data/c09k_2001-3000.xlsx', 'data/c09k_3001-3935.xlsx']\n",
    "    with open('data/c09k_corpus.txt', 'a') as f:\n",
    "        f.truncate(0)\n",
    "        for i, fn in enumerate(kiwee_files):\n",
    "            tmp = pd.read_excel(fn).fillna('')\n",
    "            # pandas는 비어있는 컬럼의 dtype을 float로 바꿔서 인식한다. 그로 인해 토크나이징 할 데이터가 없으면 오류가 발생되어 fillna를 사용해 모두 텍스트로 인식시키도록 한다\n",
    "            # https://stackoverflow.com/questions/53953286/pandas-read-excel-blanks-in-string-columns-convert-to-floats-converting-via-st\n",
    "            col_text = ['발행번호', '발명의명칭', '요약', '대표청구항', '과제', '해결방안']\n",
    "            tmp = tmp[col_text]\n",
    "            for index, row in tmp.iterrows():\n",
    "        #         print(index, '\\n', row['발명의명칭'], row['요약'], row['대표청구항'], row['과제'], row['해결방안'], '\\n')\n",
    "                for col in col_text[1:]:\n",
    "        #             print('처리중인 데이터:', col, row[col], '\\n')\n",
    "                    if row[col].strip() == \"\":\n",
    "                        pass\n",
    "                    else:\n",
    "        #                 print(row[col].strip())\n",
    "                        row[col] = unicodedata.normalize('NFKC', row[col])\n",
    "                        # row[col] = unicodedata.normalize('NFC', row[col])  # 자음과 모음이 깨질 때는 NFC로 변환\n",
    "                        # NFD(Normalization Form Decomposition) : 자음과 모음이 분리\n",
    "                        # row[col] = unicodedata.normalize('NFKD', row[col])\n",
    "                        #     https://blog.naver.com/PostView.nhn?blogId=duswl0319&logNo=221516880642&from=search&redirect=Log&widgetTypeCall=true&directAccess=false\n",
    "                        row[col] = row[col].replace('\\n\\t',' ')\n",
    "                        row[col] = row[col].replace('\\n',' ')\n",
    "                        row[col] = row[col].replace('&lt;',' ')\n",
    "                        row[col] = row[col].replace('_x000d_',' ')\n",
    "                        row[col] = row[col].replace('\\t\\t',' ')\n",
    "                        row[col] = row[col].replace('@@',' ')\n",
    "                        row[col] = row[col].replace('.  .','.')\n",
    "                        row[col] = row[col].replace('. .','.')\n",
    "                        row[col] = row[col].replace('..','.')\n",
    "                        row[col] = row[col].replace('〜','~')\n",
    "                        row[col] = row[col].replace(' . ','.')\n",
    "                        row[col] = row[col].replace(' ． ','.')\n",
    "                        row[col] = row[col].replace('． ','.')\n",
    "                        row[col] = row[col].replace('. ','.')\n",
    "                        row[col] = row[col].replace('  ',' ')\n",
    "                        row[col] = row[col].replace('  ',' ')\n",
    "                        row[col] = row[col].replace('【과제】',' ')\n",
    "                        row[col] = row[col].replace('【요약】',' ')\n",
    "                        row[col] = row[col].replace('【해결 수단】',' ')\n",
    "                        str_tmp = sent_tokenizer.tokenize(row[col].strip())\n",
    "        #                 print('문장 분리: ', str_tmp, '\\n'*3)\n",
    "        #                 result  = [f\"{line}\\n\" for line in str_tmp]\n",
    "                        for line in str_tmp:\n",
    "                            f.write(f\"{line}\\n\")\n",
    "    dataset = Dataset.from_text('data/c09k_corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5131dc6",
   "metadata": {},
   "source": [
    "### 학습 데이터 분리/로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2272b29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading completed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('data/c09k_dataset.pkl','rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    print('dataset loading completed')\n",
    "except:\n",
    "    d = dataset.train_test_split(test_size=0.1)\n",
    "    with open('data/c09k_dataset.pkl','wb') as f:\n",
    "        pickle.dump(d, f)\n",
    "    print('dataset split/saving completed')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58a042e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # d[\"train\"], d[\"test\"]\n",
    "# for t in d[\"train\"][\"text\"][:3]:\n",
    "#     print(t)\n",
    "#     print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc64d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_tokens = [\n",
    "#   \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "# ]\n",
    "# # if you want to train the tokenizer on both sets\n",
    "# # files = [\"train.txt\", \"test.txt\"]\n",
    "# # training the tokenizer on the training set\n",
    "# files = [\"data/c09k_corpus.txt\"]\n",
    "# # 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "# vocab_size = 8000\n",
    "# # maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "# max_length = 512\n",
    "# # whether to truncate\n",
    "# truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9182b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"c09k_pretrained_bert\"\n",
    "vocab_size = 8000\n",
    "max_length = 512\n",
    "truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bf4df",
   "metadata": {},
   "source": [
    "### Pre-train data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4658e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping some of the tokenizer config to config file, \n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "    tokenizer_cfg = {\"do_lower_case\": True,\n",
    "                     \"unk_token\": \"[UNK]\",\n",
    "                     \"sep_token\": \"[SEP]\",\n",
    "                     \"pad_token\": \"[PAD]\",\n",
    "                     \"cls_token\": \"[CLS]\",\n",
    "                     \"mask_token\": \"[MASK]\",\n",
    "                     \"model_max_length\": max_length,\n",
    "                     \"max_len\": max_length,\n",
    "                    }\n",
    "    json.dump(tokenizer_cfg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af37bfc",
   "metadata": {},
   "source": [
    "### Tokenizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81bd4e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# when the tokenizer is trained and configured, load it as BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "820274b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                                     max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9526a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1791637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/hdh/.cache/huggingface/datasets/text/default-84170d85970a0abc/0.0.0/cache-dc76135578d7db5f.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/hdh/.cache/huggingface/datasets/text/default-84170d85970a0abc/0.0.0/cache-651ed7d5cd6a7627.arrow\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the train dataset\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_dataset = d[\"test\"].map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4789f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ecd7d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14769"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(train_dataset[:2])\n",
    "len(train_dataset)  # 14769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e884ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "if truncate_longer_samples:\n",
    "    # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    # remove other columns, and remain them as Python lists\n",
    "    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb19d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e6f35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ba5fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d21bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/hdh/.cache/huggingface/datasets/text/default-84170d85970a0abc/0.0.0/cache-1852973d933c743e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/hdh/.cache/huggingface/datasets/text/default-84170d85970a0abc/0.0.0/cache-e54ecc20ac3a91f0.arrow\n"
     ]
    }
   ],
   "source": [
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "if not truncate_longer_samples:\n",
    "    train_dataset1 = train_dataset.map(group_texts, batched=True,\n",
    "                                                                        desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    test_dataset1 = test_dataset.map(group_texts, batched=True,\n",
    "                                                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    # convert them from lists to torch tensors\n",
    "    train_dataset1.set_format(\"torch\")\n",
    "    test_dataset1.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38b171d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2423, 251)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_dataset), len(test_dataset)  # (14769, 1642)\n",
    "len(train_dataset1), len(test_dataset1)  # (2171, 225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f2309b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset1['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d42a6a42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  37, 1075, 1198,   39,  891, 7003,   16, 6244, 2185, 3629, 3826, 1623,\n",
      "         447,  300,  428,  750,  398, 2129, 1185, 7661, 2781,   16, 2969,  547,\n",
      "         414, 5272,  401, 2338, 7140, 2159,  645,  595,  616,  551,   16,  863,\n",
      "        1527,  389,  960,  676,   18,   21,   18, 1399, 1200, 1558, 1300, 1875,\n",
      "          16,   37, 4342,  283,   39,   12,   41,   13,   22,  611,   24, 1288,\n",
      "           9,  518,  578, 1401,  784,   17, 5718,  678, 1149,  457,   16,   37,\n",
      "        4342,  272,   39,   12,   42,   13,   24,  611, 4296,    9,  518,  578,\n",
      "        1401,  784,   17, 4488,  457,   16,  412,   37, 4342,  278,   39,   12,\n",
      "          43,   13,   28,  283,  611,   29,  265, 1288,    9,  518,  746,   12,\n",
      "          49,   13,  498, 5165,  578, 1401,  758, 1059,   18,   22,   18, 1399,\n",
      "        1200, 4682, 3097,   24,  611, 3359,  383,  539,  510,   16, 2314, 1826,\n",
      "        3097,   25,  611, 4187,  539,  954, 1059,   18,   23,   18, 1399, 1200,\n",
      "        1558, 1300, 1658,  382,  730,  968, 1828, 7408,  672, 1399, 1200,  746,\n",
      "          12,   49,   13,  518, 1094, 3377,  412, 2031, 1954,  980,  509,  389,\n",
      "         882, 2515,   16,  974, 1174,  448, 4340,  383,  873,  538,  230,  712,\n",
      "        5590, 1533, 2515, 4645,  571,  382, 7199, 1059,   18,   37, 1075,   22,\n",
      "          39, 5356, 4125,   16,  556,   12,   49,   13,   37, 1119,   16, 1312,\n",
      "        1412, 6378, 7935, 2303,   16,  414, 3609,   17, 6054, 3657,   17, 1022,\n",
      "         843, 2307, 3785,  478,   16, 2599,   55,   16,   59,   16,   54,   17,\n",
      "         897,   16, 2059,  278, 1021, 2059,  278, 2233,  401,  478,   16, 5486,\n",
      "          55,   16,   59,   16, 6489, 1021,   54,   17, 1005,  398,  478,   16,\n",
      "        1462,   54,  414,   43,   17,  846,  401,  478,   16,  532,   16,  637,\n",
      "          16,  897, 1412, 3234,  843, 2307,  460,   70, 4151,   17,  496,   16,\n",
      "         807,   70,  661,   17, 1579,   16, 1868,   70, 1264,   17,  857,  496,\n",
      "         414, 1264,   70, 4151,   17, 2665,  398,  478,   16,  751,   16,  814,\n",
      "        1412, 1502,  843, 2307,  566,   16,  460,   70, 4151,   17,  496,  414,\n",
      "         906,  401, 2303,   16,  414,  291,   33,   20, 1412,   56,   34, 6136,\n",
      "         842,   16, 6054, 2637, 1054,  157,   12,  992,   13,   22,   17,   16,\n",
      "          17,   12,  992,   13,   23,   17,  414,  157,   12,  992,   13,   24,\n",
      "          17, 3080, 2303,   16,  414,  291,   33,   20, 1412,   56,   33, 6136,\n",
      "         842,   16, 6054, 1502, 1054,  157,   12,  992,   13,   22,   17,   16,\n",
      "          17,   12,  992,   13,   23,   17,  414,  157,   12,  992,   13,   24,\n",
      "          17, 3080, 2303,   16, 5058,   33, 6136,  842,   16, 7906, 1502, 1054,\n",
      "         157,   12,  992,   13,   22,   17,   16,   17,   12,  992,   13,   23,\n",
      "          17,  414,  157,   12,  992,   13,   24,   17, 3080,  478,   16, 1063,\n",
      "          16, 1208, 1412, 4589,  843, 2307,  566,  414,  460,   70, 4151,   17,\n",
      "        4155, 2303,   16,  414,  259, 4438, 2233,  377, 3436, 1873, 1388,  478,\n",
      "          16,  571,   16, 6785,   12,   14,   13,  518, 1051,  416,  570, 1250,\n",
      "        4890,  442,  524,  421,  687,  291,  412, 1233,  843, 2307, 2612, 3121,\n",
      "         478,   16, 2111,   20,   16, 1843, 3426,  478,   16, 1312,   16, 2240,\n",
      "         379, 1312, 1412, 2240,  389, 3023, 1388,  648, 6294,   16,  412, 3657,\n",
      "          16, 4102,  379, 3657, 1412, 4102,  389, 3023, 1388,  648, 6974,   16,\n",
      "         843, 2307, 2325,  414, 4967, 2329,  660, 2471])\n",
      "[ claim 15 ] 기판 표면이, 콜로이드 현탁액에 대한 젖어 성을 한층 높이기 위해서, 희산 또는 염기를 이용해 사전 처리 되는 것을 특징으로 하는, 청구항 14에 기재의 방법. 1. 발명에 따른 감온 안료 조성물은, [ 0036 ] ( a ) 2 내지 4중량 % 의 적어도 1종의 전자 - 공여체 유기 염료 화합물, [ 0037 ] ( b ) 4 내지 10중량 % 의 적어도 1종의 전자 - 수용체 화합물, 및 [ 0038 ] ( c ) 86 내지 94중량 % 의 화학식 ( i ) 에 대응하는 적어도 1종의 화합물을 포함한다. 2. 발명에 따른 아릴기는 바람직하게는 4 내지 12개의 탄소 원자, 더욱더 바람직하게는 5 내지 6개의 탄소 원자를 포함한다. 3. 발명에 따른 감온 안료 조성물에서 온도 변화 조절제로서의 본 발명에 따른 화학식 ( i ) 의 화합물의 용도 및 이들의 특성규명에 관한 것인, 후술하는 설명의 보완으로부터 기인될 것인 추가의 단서 조건을 포함한다. [ claim 2 ] 시아닌 색소가, 식 ( i ) [ 식중, x1및 x3은 질소를 나타내는지, 또는x1 - r1및 x3 - r2는 서로 무관계하게 s를 나타내, x2는 o, s, n - r6, cr8또는 cr8r9를 나타내, x4는 o, s, cr10또는 n - r7을 나타내, y는 n 또는 c - r5를 나타내, r1, r2, r6및 r7은 서로 무관계하게 c1 ~ c16 - 알킬, c3 ~ c6 - 알케닐, c5 ~ c7 - 시클로 알킬 또는 c7 ~ c16 - 아랄킬을 나타내, r3, r4및 r5는 서로 무관계하게 수소, c1 ~ c16 - 알킬 또는 시아노를 나타내는지, 또는m = 0및 p > 0의 경우, r1및 r3은 함께 ― ( ch2 ) 2 -, - ( ch2 ) 3 - 또는 ― ( ch2 ) 4 - 가교를 나타내는지, 또는m = 0및 p = 0의 경우, r1및 r5는 함께 ― ( ch2 ) 2 -, - ( ch2 ) 3 - 또는 ― ( ch2 ) 4 - 가교를 나타내는지, 또는n = 0의 경우, r2및 r5는 함께 ― ( ch2 ) 2 -, - ( ch2 ) 3 - 또는 ― ( ch2 ) 4 - 가교를 나타내, r8, r9및 r10은 서로 무관계하게 수소 또는 c1 ~ c16 - 알킬을 나타내는지, 또는cr8r9는 식의 2값의 기를 나타내, 단, asterisk ( * ) 의 환원자로부터 2개의 결합이 나오고 있어m 및 n는 서로 무관계하게 0또는 1을 나타내, p는 0, 1또는 2를 나타내, x1, x2로 x1및 x2에 결합한 기를 포함한 환a, 및 x3, x4로 x3및 x4에 결합한 기를 포함한 환b는, 서로 무관계하게 5원 또는 6원의 방향족복소환 혹은 준\n",
      "tensor([2532, 1335,  660, 1155,  566,  393, 1335,  398,  478,   16, 4960,   21,\n",
      "          70, 3101,  964,  954,  970, 1370,   12, 1338,   13,   16,  412,   19,\n",
      "         414, 1618, 1296,  414, 2821, 1296, 1798,  687, 1370,   12, 1338,   13,\n",
      "          16,  412,   19,  414, 5740, 4553, 1181,  687, 1370,   12, 1338,   13,\n",
      "          16,  523, 1034,   16, 6294,  412, 1650, 3865,  835,  395, 2079,   16,\n",
      "        3880,   17,  777, 2736,  729,   39,  498, 4976,   16,  863,   21,  960,\n",
      "         745, 2259,  950,  927,   18,   12, 1038,   13,  187,  870,  188, 2290,\n",
      "         837, 4248,  605,  758, 5111,  389, 1106,  648, 5013,  645,  595,  616,\n",
      "         551, 3488,  562,   18,  187, 1438,  188,  672, 1399,  666,   16, 2048,\n",
      "        6462,  506, 1100, 3488, 1068, 3851,  440,  821,   18, 1122, 6023,  485,\n",
      "        4349, 6023,  398, 1820,  260,  410,  811,  399, 2235,  396,  472,  551,\n",
      "         595,  616,  551, 1273,  583, 1491,  623,  676,   18,  692,  719,   12,\n",
      "          21,   13,  187,  417,   21,  188,  189,  571,   16,  532,   16,  637,\n",
      "          16,  751,  412, 1245,  572,   16, 3698,  414, 3374,  566,  510,   16,\n",
      "         560,   16,  936,   16, 1375,   16, 2865,   16, 1303,   16,  465, 1148,\n",
      "          16,  768,   16, 2123, 4080,  381,   16, 1466,   16, 2167,   16,  686,\n",
      "        2756,   16,   41,   16,   42,  412, 3655,   16,  572, 7334,   16,  637,\n",
      "         412, 5267,  864, 3201,  478,   16, 1843, 7632,   18,  190,  907,  922,\n",
      "        1395,  457,   18,   12,  390,   13,  784, 1960,  678,  457,   16,   12,\n",
      "         740,   13,  784, 1525,  457,   16,   12,  436,   13,  450,   12,  390,\n",
      "          13,  898,   12,  740,   13,  518, 3149, 3853, 1625, 2724, 5287,  776,\n",
      "        2902, 1231,   16, 5477,   17,  730, 4027,  639,  383, 7203,  687,   16,\n",
      "         779, 2441, 3186, 4243,  448, 6766,  382, 1112, 1599,  730,   12, 6937,\n",
      "          13,  498, 5914, 1112,  990, 6655,  443, 2369, 1112,  730,   12, 5824,\n",
      "          13, 6291,  377, 5555, 1112,  443,   16, 1112, 2441, 3186,  436, 2600,\n",
      "         448, 6766,  382,  779, 1599,  730,   12, 4781,   13,  498, 5914,  779,\n",
      "         990, 6655,  443, 2369,  779,  730,   12, 5176,   13, 5976,  382,  377,\n",
      "        5555, 3437,  698, 6857, 1040, 1458, 1658,  687,   16,   12,  740,   13,\n",
      "         784, 1525, 2127,   24,   16,   24,   17, 1007, 4426,  398, 2044, 1172,\n",
      "          16,  779, 1599,  730,   12, 4781,   13, 1263, 3582,   16,  779, 2441,\n",
      "        6293,  502,  708, 3556, 4046, 6293, 1112, 2689,   12,   64,   13,  627,\n",
      "         779, 2458, 2394, 4171,  779, 2394, 1567, 2689,   12,   65,   13,  820,\n",
      "         605,  595,  616,  551, 1458,  557,   18,  907,  910, 1770,  887,  412,\n",
      "         523, 6020,   18,  409,   16,  523, 1770, 3076,  645,  622,  562,   18,\n",
      "          37, 1075,  638,   39,  556, 2097,  293, 4239, 4438,   12, 1005, 1412,\n",
      "        3235, 3708,  672,  595,  729,   13,  518, 5803, 3098,  412, 1124,  666,\n",
      "         686,  412,   19,  414,  591,  389,  666,  836,  908,   17, 3677,  398,\n",
      "         760, 1311,   16, 1348, 3343, 1412, 4932,  379, 4537,  595,  616,  551,\n",
      "         863,   29,  960, 1094,  623,  676,   18,  672, 1399, 2176,  805, 1261,\n",
      "        4374, 5433,  471, 1423,  663, 1031, 1068,  832, 1849,   16,  805, 1261,\n",
      "        1847,  939, 3269, 6664,  972, 1828,  448,  483,  868,  434,  407, 1728,\n",
      "        2131,  453, 2376,  703, 1443, 4726, 2673,  734,   16, 1186,  412, 1112,\n",
      "        6726, 4092, 7867,  440,  633,  663, 1031, 1068])\n",
      "##방향족복소환 혹은 부분 수소화복소환을 나타내, 이것은 1 ~ 4개의 이질 원자를 가져 자주 ( 잘 ), 및 / 또는 벤젠 축합 또는 나프타렌 축합 되고 있어 자주 ( 잘 ), 및 / 또는 비이온성의 기본으로보다 치환되고 있어 자주 ( 잘 ), 그 때, 환a 및 b는 유리하게 같지 않고, an - 는 음이온을 나타낸다 ] 에 상당하는, 청구항 1 기재의 광학 데이터 기록 매체. ( 57 ) 【 구성 】 2종 이상의 포토크로미즘성을 가지는 화합물을 공유결합에 의해서 포함한 폴리머로부터 되는 것을 특징으로 하는 포토크로미즘성 재료. 【 효과 】 본 발명에 의해, 임의의 색조를 선택 가능한 포토크로미즘성 재료를 제공할 수 있다. 바나듐 타겟과 몰리브덴 타겟을 반응성2 전 동시 스팩터 하는 것을 특징으로 하는 사모 크로믹 재료의 제조 방법. 하기 일반식 ( 1 ) 【 화 1 】 〔 단, r1, r2, r3 및 r4는 각각, 동종 또는 이종의 수소 원자, 알킬기, 알콕시기, 아랄킬기, 아실기, 시아노기, 치환 아미노기, 아릴기, 아실록시기, 니트로기, 히드록실기, 할로겐 원자이며, a, b 및 c는, 각각 치환기r1, r2 및 r3의 개수를 나타내, 1또는 2이다. 〕 그리고 나타나는 크로멘 화합물. ( 이 ) 전자 공여성정색성 유기 화합물, ( 로 ) 전자 수용성 화합물, ( 하 ) 전기 ( 이 ) 와 ( 로 ) 의 정색 반응의 발생 온도를 결정하는 반응 매체를 포함해, 색농도 - 온도 곡선의 관계에 있어, 발색 상태로부터 온도가 상승하는 과정에서 소색 개시 온도 ( t3 ) 에 이르면 소색하기 시작해 완전 소색 온도 ( t4 ) 이상에서는 완전하게 소색해, 소색 상태로부터 온도가 하강하는 과정에서 발색 개시 온도 ( t2 ) 에 이르면 발색하기 시작해 완전 발색 온도 ( t1 ) 이하에서는 완전하게 발색하는 변색 거동을 나타내는 가역열변색성 조성물에 있어, ( 로 ) 전자 수용성 화합물로서 4, 4 - 옥시 비스페놀을 포함해서 되어, 발색 개시 온도 ( t2 ) 이하의 온도역에, 발색 상태로부터 자연스럽게 소색하는 자연 소색 온도역 ( x ) 과 발색 상태를 보관 유지하는 발색 보관 유지 온도역 ( y ) 을 가지는 것을 특징으로 하는 가역열변색성 조성물. 그리고 나타내지는 지아리르에텐 유도체 및 그 이성체. 또, 그 지아리르에텐 유도체로부터 되는 포토크로미즘 재료. [ claim 10 ] 식 so2nr7r8 ( r7및 r8은 앞에서 본 것을 나타낸다 ) 의 술혼 아미드기 및 경우에 의해 할로겐 및 / 또는 알콕시에 의해 치환된 co - 프타로시아닌을 산화시켜, 아민 l1및 l2로 반응시키는 것을 특징으로 하는 청구항 9 기재의 화합물의 제조 방법. 본 발명에 따라서 금속 나노 입자가 함침된 전도성 전기변색 고분자 재료를 이용하면, 금속 나노 입자의 함량을 단순히 조절하는 것만으로도 전압 인가 시 다양한 색깔로 발현되고, 착색 및 소색 시간을 최소화할 수 있는 전기변색 고분자 재료를\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(train_dataset1['input_ids'][i])\n",
    "    print(tokenizer.decode(train_dataset1['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42187cb",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcc3326b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e84fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c426bb",
   "metadata": {},
   "source": [
    "### Pre-Training - Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee13841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model checkpoint  : 8.30 실행 코드, max_len = 64로 학습된 모델을 로드하여 512로 변경 후 추가 학습\n",
    "# model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-7500\"))  # max_len = 64로 pre-train한 모델 로드 (8.22 완료)\n",
    "# model.config.max_length = 512  # 512로 추가 학습하기 위해 모델 설정 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b24678dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_pretrained_bert_512/checkpoint-15000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert_512/checkpoint-7000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_pretrained_bert_512/checkpoint-15000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at c09k_pretrained_bert_512/checkpoint-15000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(os.path.join('c09k_pretrained_bert_512', \"checkpoint-15000\"))  # max_len = 512로 추가 pre-train한 모델 로드 (8.30 완료)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c302756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize the model with the config\n",
    "# model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "# model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e2a7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "491d7a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d2e5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f44b9985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.35562488436698914, 'token': 1193, 'token_str': '발광', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.2317202389240265, 'token': 1125, 'token_str': '일렉트로크로믹', 'sequence': '인광성 유기 금속 이리듐 착체, 일렉트로크로믹 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.11168079078197479, 'token': 737, 'token_str': '표시', 'sequence': '인광성 유기 금속 이리듐 착체, 표시 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.05203559994697571, 'token': 3316, 'token_str': '광기능', 'sequence': '인광성 유기 금속 이리듐 착체, 광기능 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.03952431306242943, 'token': 745, 'token_str': '광학', 'sequence': '인광성 유기 금속 이리듐 착체, 광학 소자, 발광 장치, 전자 기기, 및 조명 장치'}]\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.35562488436698914\n",
      "인광성 유기 금속 이리듐 착체, 일렉트로크로믹 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.2317202389240265\n",
      "인광성 유기 금속 이리듐 착체, 표시 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.11168079078197479\n",
      "인광성 유기 금속 이리듐 착체, 광기능 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.05203559994697571\n",
      "인광성 유기 금속 이리듐 착체, 광학 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.03952431306242943\n",
      "==================================================\n",
      "[{'score': 0.7745561599731445, 'token': 457, 'token_str': '화합물', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.09165266901254654, 'token': 700, 'token_str': '물질', 'sequence': '본 명세서는 화학식 1로 표시되는 물질 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.026326006278395653, 'token': 1149, 'token_str': '염료', 'sequence': '본 명세서는 화학식 1로 표시되는 염료 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.01624906249344349, 'token': 909, 'token_str': '성분', 'sequence': '본 명세서는 화학식 1로 표시되는 성분 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.010347452014684677, 'token': 1169, 'token_str': '색소', 'sequence': '본 명세서는 화학식 1로 표시되는 색소 및 이를 포함하는 유기 발광 소자에 관한 것이다'}]\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.7745561599731445\n",
      "본 명세서는 화학식 1로 표시되는 물질 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.09165266901254654\n",
      "본 명세서는 화학식 1로 표시되는 염료 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.026326006278395653\n",
      "본 명세서는 화학식 1로 표시되는 성분 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.01624906249344349\n",
      "본 명세서는 화학식 1로 표시되는 색소 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.010347452014684677\n",
      "==================================================\n",
      "[{'score': 0.1203656941652298, 'token': 1193, 'token_str': '발광', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 발광 기기, 및 조명 장치'}, {'score': 0.03443207964301109, 'token': 1820, 'token_str': '반응성', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 반응성 기기, 및 조명 장치'}, {'score': 0.01768237166106701, 'token': 4361, 'token_str': '조명', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 조명 기기, 및 조명 장치'}, {'score': 0.016315322369337082, 'token': 2541, 'token_str': '이소시아네이트', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 이소시아네이트 기기, 및 조명 장치'}, {'score': 0.014037780463695526, 'token': 6401, 'token_str': '페나진', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 페나진 기기, 및 조명 장치'}]\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 발광 기기, 및 조명 장치, confidence: 0.1203656941652298\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 반응성 기기, 및 조명 장치, confidence: 0.03443207964301109\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 조명 기기, 및 조명 장치, confidence: 0.01768237166106701\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 이소시아네이트 기기, 및 조명 장치, confidence: 0.016315322369337082\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 페나진 기기, 및 조명 장치, confidence: 0.014037780463695526\n",
      "==================================================\n",
      "[{'score': 0.8182970881462097, 'token': 678, 'token_str': '유기', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.07335758209228516, 'token': 4326, 'token_str': '전계', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전계 발광 소자에 관한 것이다'}, {'score': 0.019620267674326897, 'token': 1617, 'token_str': '고체', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 고체 발광 소자에 관한 것이다'}, {'score': 0.010477273724973202, 'token': 450, 'token_str': '전기', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전기 발광 소자에 관한 것이다'}, {'score': 0.00843347143381834, 'token': 16, 'token_str': ',', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는, 발광 소자에 관한 것이다'}]\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.8182970881462097\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전계 발광 소자에 관한 것이다, confidence: 0.07335758209228516\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 고체 발광 소자에 관한 것이다, confidence: 0.019620267674326897\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전기 발광 소자에 관한 것이다, confidence: 0.010477273724973202\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는, 발광 소자에 관한 것이다, confidence: 0.00843347143381834\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "# 인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치\n",
    "# 본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다\n",
    "examples = [\n",
    "    \"인광성 유기 금속 이리듐 착체, [MASK] 소자, 발광 장치, 전자 기기, 및 조명 장치\",\n",
    "    \"본 명세서는 화학식 1로 표시되는 [MASK] 및 이를 포함하는 유기 발광 소자에 관한 것이다\",\n",
    "    \"인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, [MASK] 기기, 및 조명 장치\",\n",
    "    \"본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 [MASK] 발광 소자에 관한 것이다\",\n",
    "]\n",
    "for example in examples:\n",
    "    print(fill_mask(example))\n",
    "    for prediction in fill_mask(example):\n",
    "        print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0f5301e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c09k_pretrained_bert'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc5f2ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 3000\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Num examples = 19400\n",
    "# Num Epochs = 50\n",
    "# Total optimization steps = 3750 = 750*50 = \n",
    "# 로드한 모델 추가학습\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='c09k_pretrained_bert_512_2',          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # 'steps': evaluate each `logging_steps`, 'epoch'  : each epoch\n",
    "    overwrite_output_dir=False,     # 원래 True였으나 변경 (8.30) \n",
    "    num_train_epochs=3000.,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=8, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=4,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=8,  # evaluation batch size\n",
    "    logging_steps=3000,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=3000,\n",
    "    resume_from_checkpoint=True,  # 체크포인트를 이어서 학습 가능할지 확인을 위해 새로 추가(8.30)\n",
    "    load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "#     save_total_limit=5,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "418262dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset1,\n",
    "    eval_dataset=test_dataset1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01c3a7c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2423\n",
      "  Num Epochs = 3000\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 225000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180703' max='225000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180703/225000 97:34:17 < 23:55:07, 0.51 it/s, Epoch 2409.36/3000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.025300</td>\n",
       "      <td>1.588253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>1.519794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.537100</td>\n",
       "      <td>1.499150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.410600</td>\n",
       "      <td>1.501127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.329600</td>\n",
       "      <td>1.495798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.286800</td>\n",
       "      <td>1.503644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>1.515726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.208700</td>\n",
       "      <td>1.555980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>1.583822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.161200</td>\n",
       "      <td>1.549630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>1.644110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>1.586778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.118300</td>\n",
       "      <td>1.633028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>1.625548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>1.673790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.092200</td>\n",
       "      <td>1.683287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.085400</td>\n",
       "      <td>1.688021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>1.784993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>1.692290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>1.748456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>1.760638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.061700</td>\n",
       "      <td>1.739960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>1.758183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>1.825747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>1.833746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>1.850350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>1.884638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>1.820617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87000</td>\n",
       "      <td>0.042400</td>\n",
       "      <td>1.841540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>1.861030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93000</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>1.920398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>0.037100</td>\n",
       "      <td>1.841045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99000</td>\n",
       "      <td>0.035400</td>\n",
       "      <td>1.912804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102000</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>1.920784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>1.940987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108000</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>1.940960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111000</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>1.930260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114000</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>1.985002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117000</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>1.958150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>1.987964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123000</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>1.935473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126000</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>1.994659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129000</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>1.973529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132000</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>1.901448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>0.021600</td>\n",
       "      <td>1.996663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138000</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>1.971123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141000</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>2.026968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144000</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>2.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147000</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>1.974892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.017900</td>\n",
       "      <td>2.016984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153000</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>2.042046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156000</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>2.092182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159000</td>\n",
       "      <td>0.015800</td>\n",
       "      <td>2.016220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162000</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>2.048853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>2.042943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>1.972030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171000</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>2.027155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174000</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>2.063563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>2.064473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>2.062644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-3000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-3000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-3000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-6000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-6000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-6000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-9000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-9000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-9000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-12000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-12000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-12000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-15000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-15000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-15000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-18000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-18000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-18000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-21000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-21000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-21000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-24000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-24000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-24000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-27000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-27000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-27000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-30000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-30000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-30000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-33000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-33000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-33000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-36000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-36000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-36000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-39000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-39000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-39000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-42000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-42000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-42000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-45000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-45000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-45000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-48000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-48000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-48000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-51000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-51000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-51000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-54000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-54000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-54000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-57000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-57000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-57000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-60000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-60000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-60000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-63000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-63000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-63000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-66000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-66000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-66000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-69000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-69000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-69000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-72000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-72000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-72000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-75000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-75000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-75000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-78000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-78000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-78000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-81000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-81000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-81000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-84000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-84000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-84000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-87000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-87000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-87000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-90000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-90000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-90000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-93000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-93000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-93000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-96000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-96000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-96000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-99000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-99000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-99000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-102000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-102000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-102000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-105000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-105000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-105000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-108000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-108000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-108000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-111000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-111000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-111000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-114000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-114000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-114000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-117000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-117000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-117000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-120000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-120000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-120000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-123000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-123000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-123000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-126000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-126000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-126000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-129000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-129000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-129000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-132000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-132000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-132000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-135000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-135000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-135000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-138000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-138000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-138000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-141000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-141000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-141000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-144000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-144000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-144000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-147000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-147000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-147000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-150000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-150000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-150000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-153000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-153000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-153000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-156000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-156000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-156000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-159000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-159000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-159000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-162000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-162000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-162000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-165000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-165000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-165000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-168000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-168000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-168000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-171000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-171000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-171000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-174000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-174000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-174000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-177000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-177000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-177000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 251\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert_512_2/checkpoint-180000\n",
      "Configuration saved in c09k_pretrained_bert_512_2/checkpoint-180000/config.json\n",
      "Model weights saved in c09k_pretrained_bert_512_2/checkpoint-180000/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/trainer.py:1505\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1502\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1504\u001b[0m )\n\u001b[0;32m-> 1505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/trainer.py:1747\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1745\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1747\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1750\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1753\u001b[0m ):\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/trainer.py:2495\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2495\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa70dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17d3724f",
   "metadata": {},
   "source": [
    "### Masked-LM 성능 확인\n",
    "* 모델과 토크나이저를 한번은 로드 해줘야 하네. 그렇지 않으면 토크나이저가 CPU에 로드되어 있는 것으로 인식하여 에러 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ee29baad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_pretrained_bert_512/checkpoint-7000/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_pretrained_bert_512/checkpoint-7000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at c09k_pretrained_bert_512/checkpoint-7000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n",
      "[{'score': 0.06595180928707123, 'token': 16, 'token_str': ',', 'sequence': '인광성 유기 금속 이리듐 착체,, 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.06593338400125504, 'token': 678, 'token_str': '유기', 'sequence': '인광성 유기 금속 이리듐 착체, 유기 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.04995021969079971, 'token': 805, 'token_str': '금속', 'sequence': '인광성 유기 금속 이리듐 착체, 금속 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.01198280043900013, 'token': 450, 'token_str': '전기', 'sequence': '인광성 유기 금속 이리듐 착체, 전기 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.01068327110260725, 'token': 412, 'token_str': '및', 'sequence': '인광성 유기 금속 이리듐 착체, 및 소자, 발광 장치, 전자 기기, 및 조명 장치'}]\n",
      "인광성 유기 금속 이리듐 착체,, 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.06595180928707123\n",
      "인광성 유기 금속 이리듐 착체, 유기 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.06593338400125504\n",
      "인광성 유기 금속 이리듐 착체, 금속 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.04995021969079971\n",
      "인광성 유기 금속 이리듐 착체, 전기 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.01198280043900013\n",
      "인광성 유기 금속 이리듐 착체, 및 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.01068327110260725\n",
      "==================================================\n",
      "[{'score': 0.08523005247116089, 'token': 18, 'token_str': '.', 'sequence': '본 명세서는 화학식 1로 표시되는. 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.05935471132397652, 'token': 882, 'token_str': '관한', 'sequence': '본 명세서는 화학식 1로 표시되는 관한 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.043748896569013596, 'token': 16, 'token_str': ',', 'sequence': '본 명세서는 화학식 1로 표시되는, 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.03933851793408394, 'token': 746, 'token_str': '화학식', 'sequence': '본 명세서는 화학식 1로 표시되는 화학식 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.024272534996271133, 'token': 1138, 'token_str': '표시되는', 'sequence': '본 명세서는 화학식 1로 표시되는 표시되는 및 이를 포함하는 유기 발광 소자에 관한 것이다'}]\n",
      "본 명세서는 화학식 1로 표시되는. 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.08523005247116089\n",
      "본 명세서는 화학식 1로 표시되는 관한 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.05935471132397652\n",
      "본 명세서는 화학식 1로 표시되는, 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.043748896569013596\n",
      "본 명세서는 화학식 1로 표시되는 화학식 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.03933851793408394\n",
      "본 명세서는 화학식 1로 표시되는 표시되는 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.024272534996271133\n",
      "==================================================\n",
      "[{'score': 0.07487452030181885, 'token': 16, 'token_str': ',', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치,, 기기, 및 조명 장치'}, {'score': 0.052736397832632065, 'token': 805, 'token_str': '금속', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 금속 기기, 및 조명 장치'}, {'score': 0.05035865306854248, 'token': 678, 'token_str': '유기', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 유기 기기, 및 조명 장치'}, {'score': 0.009527969174087048, 'token': 1193, 'token_str': '발광', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 발광 기기, 및 조명 장치'}, {'score': 0.009037600830197334, 'token': 450, 'token_str': '전기', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전기 기기, 및 조명 장치'}]\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치,, 기기, 및 조명 장치, confidence: 0.07487452030181885\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 금속 기기, 및 조명 장치, confidence: 0.052736397832632065\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 유기 기기, 및 조명 장치, confidence: 0.05035865306854248\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 발광 기기, 및 조명 장치, confidence: 0.009527969174087048\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전기 기기, 및 조명 장치, confidence: 0.009037600830197334\n",
      "==================================================\n",
      "[{'score': 0.08683860301971436, 'token': 18, 'token_str': '.', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는. 발광 소자에 관한 것이다'}, {'score': 0.059315577149391174, 'token': 882, 'token_str': '관한', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 관한 발광 소자에 관한 것이다'}, {'score': 0.034790921956300735, 'token': 746, 'token_str': '화학식', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 화학식 발광 소자에 관한 것이다'}, {'score': 0.032986629754304886, 'token': 1138, 'token_str': '표시되는', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 표시되는 발광 소자에 관한 것이다'}, {'score': 0.03141077980399132, 'token': 16, 'token_str': ',', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는, 발광 소자에 관한 것이다'}]\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는. 발광 소자에 관한 것이다, confidence: 0.08683860301971436\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 관한 발광 소자에 관한 것이다, confidence: 0.059315577149391174\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 화학식 발광 소자에 관한 것이다, confidence: 0.034790921956300735\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 표시되는 발광 소자에 관한 것이다, confidence: 0.032986629754304886\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는, 발광 소자에 관한 것이다, confidence: 0.03141077980399132\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model1 = BertForMaskedLM.from_pretrained(os.path.join('c09k_pretrained_bert_512', \"checkpoint-7000\"))\n",
    "# load the tokenizer\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "tokenizer1 = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)\n",
    "fill_mask = pipeline(\"fill-mask\", model=model1, tokenizer=tokenizer1)\n",
    "# perform predictions\n",
    "# 인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치\n",
    "# 본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다\n",
    "examples = [\n",
    "    \"인광성 유기 금속 이리듐 착체, [MASK] 소자, 발광 장치, 전자 기기, 및 조명 장치\",\n",
    "    \"본 명세서는 화학식 1로 표시되는 [MASK] 및 이를 포함하는 유기 발광 소자에 관한 것이다\",\n",
    "    \"인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, [MASK] 기기, 및 조명 장치\",\n",
    "    \"본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 [MASK] 발광 소자에 관한 것이다\",\n",
    "]\n",
    "for example in examples:\n",
    "    print(fill_mask(example))\n",
    "    for prediction in fill_mask(example):\n",
    "        print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fdb28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e010c90",
   "metadata": {},
   "source": [
    "### 그 외 참고용 자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명선 책임의 문장 분리 코드(3글자 이후 마침표가 등장하는 경우에 간혹 분리되는 경우가 있다. 미사용)\n",
    "# REG_SENT_KO=r'([ㄱ-ㅣ가-힣]+[.]|[\\n]|[:;!?])'\n",
    "# REG_SENT_EN=r'([a-zA-Z]+[.]\\s|[\\n]|[:;!?])'\n",
    "\n",
    "# def split_sentence(doc, regex):\n",
    "#     s = 0\n",
    "#     for m in re.finditer(regex, doc):\n",
    "#         sent = doc[s:m.end()].strip()\n",
    "#         s = m.end()\n",
    "#         if not sent:\n",
    "#             continue\n",
    "#         yield sent\n",
    "\n",
    "#     if s < len(doc):\n",
    "#         sent = doc[s:].strip()\n",
    "#         if sent:\n",
    "#             yield sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5420d",
   "metadata": {},
   "source": [
    "### using personal dataset\n",
    "* https://huggingface.co/docs/datasets/dataset_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you have huge custom dataset separated into files\n",
    "# # load the splitted files\n",
    "# files = [\"train1.txt\", \"train2.txt\"] # train3.txt, etc.\n",
    "# dataset = load_dataset(\"text\", data_files=files, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4154edda",
   "metadata": {},
   "source": [
    "### 문장 클린징 처리 전/후 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc03d8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이산화티탄-염화리튬 전기변색물질, 이를 이용한전기변색장치용 파우더 및 필름전극의 제조방법', '본 발명은 이산화티탄-염화리튬 전기변색물질 및 이를 이용한 전기변색장치용 파우더 및 필름전극의 제조방법에 관한 것으로, 이산화티탄에 염화리튬이 균일하게 혼합된 물질을 합성하여 제조되며 상기 이산화티탄-염화리튬에서 티탄 대 리튬의 몰 함량 비율이 1 : x (0.5 x 2) 범위를 갖는 이산화티탄-염화리튬을 졸-겔 방법으로 합성함으로써 표면적이 극대화되는 다공성화 또는 그 물질 내에 존재하는 높은 함량의 리튬 이온으로 인하여 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 매우 뛰어난 발명인 것이다.', '이산화티탄-염화리튬 파우더에 아세틸아세톤이 포함된 증류수를 첨가하여 끈적거리는 상태가 될 때까지 교반하는 단계; 상기 용액에 증류수와 트리톤 X-100을 첨가하여 완전히 균일한 용액이 될 때까지 다시 교반하는 단계; 및 상기 용액을 전도성 유리인 ITO 위에 얇게 입힌 후, 상온에서 건조한 다음 열처리하는 단계를 포함함을 특징으로 하는 전기변색장치용 이산화티탄-염화리튬 필름전극의 제조방법.', '1 발명은 상기의 제문제점을 감안하여 안출한 것으로, 본 발명에서는 이산화티탄-염화리튬을 새롭게 합성하여 전기변색 물질로서 실용화함에 그 목적이 있다.', '2 발명의 다른 목적은 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 이산화티탄-염화리튬 전기변색물질을 제공하는데 있다.']\n",
      "['전기 화학 디바이스 및 그것을 형성하는 방법', '전기 화학 디바이스에 대해서, 개시한다.전기 화학 디바이스는, 제 1의 투명 도전층, 제 1의 투명 도전층 위에 있는 에렉트로 크로믹층, 에렉트로 크로믹층 위에 있는 대향 전극층, 제 2의 투명 도전층을 갖추어 23°C으로 0.68초/mm이하의 스위칭 속도 파라미터를 가진다.', '제 1의 투명 도전층과 전기 제 1의 투명 도전층 위에 있는 음극 전기 화학층과 전기 음극 전기 화학층 위에 있는 양극 전기 화학층과 제 2의 투명 도전층과 (을)를 갖추는 전기 화학 디바이스이며, 전기 전기 화학 디바이스는, 23°C으로 0.68초/mm이하인 제 1의 스위칭 속도 파라미터, -20°C으로 1.0초/mm이하인 제 2의 스위칭 속도 파라미터, 1.5/log(오옴) 미만인 log|Z|의 impedance파라미터, 또는 -20°C으로 8%미만인 착색 투과률 파라미터 중 적어도 1개를 가지는, 전기 화학 디바이스.', '이산화 바나듐 함유 입자, 사모 크로믹 필름 및 이산화 바나듐 함유 입자의 제조 방법', '사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.65~2.05의 범위내인 원소 A와 폴링의 전기 음성도가 2.10~2.54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['text'][0:5])\n",
    "print(dataset['text'][-5:])\n",
    "# 문장 클린징 처리 전\n",
    "# ['이산화티탄-염화리튬 전기변색물질, 이를 이용한전기변색장치용 파우더 및 필름전극의 제조방법', '본 발명은 이산화티탄-염화리튬 전기변색물질 및 이를 이용한 전기변색장치용 파우더 및 필름전극의 제조방법에 관한 것으로, 이산화티탄에 염화리튬이 균일하게 혼합된 물질을 합성하여 제조되며 상기 이산화티탄-염화리튬에서 티탄 대 리튬의 몰 함량 비율이 1 : x (0.5 &lt; x &lt; 2) 범위를 갖는 이산화티탄-염화리튬을 졸-겔 방법으로 합성함으로써 표면적이 극대화되는 다공성화 또는 그 물질 내에 존재하는 높은 함량의 리튬 이온으로 인하여 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 매우 뛰어난 발명인 것이다.', '이산화티탄-염화리튬 파우더에 아세틸아세톤이 포함된 증류수를 첨가하여 끈적거리는 상태가 될 때까지 교반하는 단계; 상기 용액에 증류수와 트리톤 X-100을 첨가하여 완전히 균일한 용액이 될 때까지 다시 교반하는 단계; 및 상기 용액을 전도성 유리인 ITO 위에 얇게 입힌 후, 상온에서 건조한 다음 열처리하는 단계를 포함함을 특징으로 하는 전기변색장치용 이산화티탄-염화리튬 필름전극의 제조방법.', '1@@발명은 상기의 제문제점을 감안하여 안출한 것으로, 본 발명에서는 이산화티탄-염화리튬을 새롭게 합성하여 전기변색 물질로서 실용화함에 그 목적이 있다.', '2@@발명의 다른 목적은 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 이산화티탄-염화리튬 전기변색물질을 제공하는데 있다.']\n",
    "# ['사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며,', '산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.', '６５〜２． 05의 범위내인 원소 A와 폴링의 전기 음성도가 2.', '１０〜２． 54의 범위내인 원소 B만이 함유 되어', '바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\n",
    "# 처리 후에는 1문장으로 처리될 데이터가 무려 5문장으로 나눠진다!\n",
    "\n",
    "# 문장 클린징 처리 후\n",
    "# ['이산화티탄-염화리튬 전기변색물질, 이를 이용한전기변색장치용 파우더 및 필름전극의 제조방법', '본 발명은 이산화티탄-염화리튬 전기변색물질 및 이를 이용한 전기변색장치용 파우더 및 필름전극의 제조방법에 관한 것으로, 이산화티탄에 염화리튬이 균일하게 혼합된 물질을 합성하여 제조되며 상기 이산화티탄-염화리튬에서 티탄 대 리튬의 몰 함량 비율이 1 : x (0.5 x 2) 범위를 갖는 이산화티탄-염화리튬을 졸-겔 방법으로 합성함으로써 표면적이 극대화되는 다공성화 또는 그 물질 내에 존재하는 높은 함량의 리튬 이온으로 인하여 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 매우 뛰어난 발명인 것이다.', '이산화티탄-염화리튬 파우더에 아세틸아세톤이 포함된 증류수를 첨가하여 끈적거리는 상태가 될 때까지 교반하는 단계; 상기 용액에 증류수와 트리톤 X-100을 첨가하여 완전히 균일한 용액이 될 때까지 다시 교반하는 단계; 및 상기 용액을 전도성 유리인 ITO 위에 얇게 입힌 후, 상온에서 건조한 다음 열처리하는 단계를 포함함을 특징으로 하는 전기변색장치용 이산화티탄-염화리튬 필름전극의 제조방법.', '1 발명은 상기의 제문제점을 감안하여 안출한 것으로, 본 발명에서는 이산화티탄-염화리튬을 새롭게 합성하여 전기변색 물질로서 실용화함에 그 목적이 있다.', '2 발명의 다른 목적은 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 이산화티탄-염화리튬 전기변색물질을 제공하는데 있다.']\n",
    "# ['전기 화학 디바이스 및 그것을 형성하는 방법', '전기 화학 디바이스에 대해서, 개시한다.전기 화학 디바이스는, 제 1의 투명 도전층, 제 1의 투명 도전층 위에 있는 에렉트로 크로믹층, 에렉트로 크로믹층 위에 있는 대향 전극층, 제 2의 투명 도전층을 갖추어 23°C으로 0.68초/mm이하의 스위칭 속도 파라미터를 가진다.', '제 1의 투명 도전층과 전기 제 1의 투명 도전층 위에 있는 음극 전기 화학층과 전기 음극 전기 화학층 위에 있는 양극 전기 화학층과 제 2의 투명 도전층과 (을)를 갖추는 전기 화학 디바이스이며, 전기 전기 화학 디바이스는, 23°C으로 0.68초/mm이하인 제 1의 스위칭 속도 파라미터, -20°C으로 1.0초/mm이하인 제 2의 스위칭 속도 파라미터, 1.5/log(오옴) 미만인 log|Z|의 impedance파라미터, 또는 -20°C으로 8%미만인 착색 투과률 파라미터 중 적어도 1개를 가지는, 전기 화학 디바이스.', '이산화 바나듐 함유 입자, 사모 크로믹 필름 및 이산화 바나듐 함유 입자의 제조 방법', \n",
    "#  '사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.65〜2.05의 범위내인 원소 A와 폴링의 전기 음성도가 2.10〜2.54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f00a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# ['사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.',\n",
    "#  '６５〜２． 05의 범위내인 원소 A와 폴링의 전기 음성도가 2.',\n",
    "#  '１０〜２． 54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\n",
    "#  위와 같이 소수점 앞 뒤로 공백이 존재하는데, 이로 인해 소수점에서 문장 분리가 되버린다. 오류 조치가 필요\n",
    "#  \"\"\"\n",
    "# 아래와 같이 구두점 앞뒤 블랭크를 제거하여 일단 구분\n",
    "# row[col] = unicodedata.normalize('NFKC', row[col])\n",
    "# https://blog.naver.com/PostView.nhn?blogId=duswl0319&logNo=221516880642&from=search&redirect=Log&widgetTypeCall=true&directAccess=false\n",
    "# row[col] = row[col].replace(' . ','.')\n",
    "# row[col] = row[col].replace(' ． ','.')\n",
    "# row[col] = row[col].replace('． ','.')\n",
    "# row[col] = row[col].replace('. ','.')\n",
    "# row[col] = row[col].replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c46427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train the tokenizer from scratch (especially if you have custom\n",
    "# dataset loaded as datasets object), then run this cell to save it as files\n",
    "# but if you already have your custom data as text files, there is no point using this\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    \"\"\"Utility function to save dataset text to disk,\n",
    "    useful for using the texts to train the tokenizer \n",
    "    (as the tokenizer accepts files)\"\"\"\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_dataset.features.keys()\n",
    "# # dict_keys(['text', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'])\n",
    "# total_length = len(list(chain(train_dataset['text'])))  # total_length = 14769\n",
    "# max_length  # 512\n",
    "# total_length = (total_length // max_length) * max_length\n",
    "# # total_length  # 14336, total_length // max_length = 28.8457..., 28 * 512 = 14336"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beautifulsoup",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
