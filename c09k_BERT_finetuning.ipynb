{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10fa50a",
   "metadata": {},
   "source": [
    "### Train BERT from Scratch using Transformers in Python\n",
    "* https://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python\n",
    "* https://huggingface.co/transformers/v3.2.0/training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "93e678dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "from datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import nltk\n",
    "from nltk.data import load\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42187cb",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9182b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"c09k_pretrained_bert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a48238",
   "metadata": {},
   "source": [
    "### model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "691b8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31613b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_pretrained_bert/checkpoint-7500/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert/checkpoint-4320\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_pretrained_bert/checkpoint-7500/pytorch_model.bin\n",
      "Some weights of the model checkpoint at c09k_pretrained_bert/checkpoint-7500 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at c09k_pretrained_bert/checkpoint-7500 and are newly initialized: ['classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model1 = BertForSequenceClassification.from_pretrained(os.path.join(model_path, \"checkpoint-7500\"), return_dict=True, num_labels=18)\n",
    "# load the tokenizer\n",
    "tokenizer1 = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)\n",
    "optimizer = AdamW(model1.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de9f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model1.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model1.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19c748",
   "metadata": {},
   "source": [
    "### finetuning data prepare\n",
    "* 성능 비교를 목적으로 KoBERT의 학습 데이터를 그대로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e46fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55c841c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9881\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5203\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = pd.read_csv('data/train_C09K11_220715.txt', sep='\\t')  # text와 라벨 파일\n",
    "test_dataset_df = pd.read_csv('data/test_C09K11_220715.txt', sep='\\t')\n",
    "train_dataset = Dataset.from_pandas(train_data_df)  # Dataset 객체 생성\n",
    "test_dataset = Dataset.from_pandas(test_dataset_df)\n",
    "finetune_dataset = DatasetDict()  # DatasetDict 객체 생성\n",
    "finetune_dataset['train'] = train_dataset\n",
    "finetune_dataset['test'] = test_dataset\n",
    "finetune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125307ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer1(train_dataset['text'][:16], return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "269944cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>광활성 형광체 프로브 및 이를 이용한 암세포 검출방법</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>본 발명은 광활성 형광체 검출방법에 관한 것으로서, 화학식 1로 표시되는 화합물...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>하기 [화학식 1]로 표시되는 OPA 또는 TPA 구조체를 포함하는 활용한 광활성 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                      광활성 형광체 프로브 및 이를 이용한 암세포 검출방법      1\n",
       "1    본 발명은 광활성 형광체 검출방법에 관한 것으로서, 화학식 1로 표시되는 화합물...      1\n",
       "2  하기 [화학식 1]로 표시되는 OPA 또는 TPA 구조체를 포함하는 활용한 광활성 ...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c90fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmup_steps = 2\n",
    "n_epochs = 5\n",
    "num_train_steps = n_epochs + 1\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4721859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 함수 객체 생성\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer1(examples[\"text\"], truncation=True, max_length=64, padding=True)\n",
    "def preprocess_function1(examples):\n",
    "    return examples['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e39a7174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12.28ba/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 15.14ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_finetune_dataset = finetune_dataset.map(preprocess_function, batched=True)  \n",
    "# DatasetDict의 'text'를 토크나이징, finetune_dataset에는 features: ['text', 'label']만 있었으나,\n",
    "# tokenized_finetune_dataset에는 'input_ids', 'token_type_ids', 'attention_mask'가 추가됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3717a0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9881\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5203\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_finetune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c1ca5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_finetune_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdacdf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdf56bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f656ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# training_args = TrainingArguments(output_dir=\"c09k_finetuned_bert\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"c09k_finetuned_bert\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eab3f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1 = Trainer(\n",
    "    model=model1,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_finetune_dataset['train'],\n",
    "    eval_dataset=tokenized_finetune_dataset['test'],\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     tokenizer=tokenizer1,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "792c4cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9881\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3090\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3090' max='3090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3090/3090 06:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.815100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.500900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.346000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to c09k_finetuned_bert/checkpoint-500\n",
      "Configuration saved in c09k_finetuned_bert/checkpoint-500/config.json\n",
      "Model weights saved in c09k_finetuned_bert/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to c09k_finetuned_bert/checkpoint-1000\n",
      "Configuration saved in c09k_finetuned_bert/checkpoint-1000/config.json\n",
      "Model weights saved in c09k_finetuned_bert/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to c09k_finetuned_bert/checkpoint-1500\n",
      "Configuration saved in c09k_finetuned_bert/checkpoint-1500/config.json\n",
      "Model weights saved in c09k_finetuned_bert/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to c09k_finetuned_bert/checkpoint-2000\n",
      "Configuration saved in c09k_finetuned_bert/checkpoint-2000/config.json\n",
      "Model weights saved in c09k_finetuned_bert/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to c09k_finetuned_bert/checkpoint-2500\n",
      "Configuration saved in c09k_finetuned_bert/checkpoint-2500/config.json\n",
      "Model weights saved in c09k_finetuned_bert/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to c09k_finetuned_bert/checkpoint-3000\n",
      "Configuration saved in c09k_finetuned_bert/checkpoint-3000/config.json\n",
      "Model weights saved in c09k_finetuned_bert/checkpoint-3000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3090, training_loss=1.6539026920849451, metrics={'train_runtime': 405.526, 'train_samples_per_second': 121.829, 'train_steps_per_second': 7.62, 'total_flos': 1625108636839680.0, 'train_loss': 1.6539026920849451, 'epoch': 5.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer1.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffd19b",
   "metadata": {},
   "source": [
    "### finetuning된 모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08833588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_finetuned_bert/checkpoint-3000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert/checkpoint-7500\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert/checkpoint-3000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert/checkpoint-3000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model2 = BertForSequenceClassification.from_pretrained(\n",
    "    os.path.join('c09k_finetuned_bert', \"checkpoint-3000\"),\n",
    "    return_dict=True, num_labels=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06f95e5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer\n",
    "tokenizer2 = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87eceea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc0ca02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model1.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "390b3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?pipeline\n",
    "text_classifier = pipeline('text-classification', model=model2, tokenizer=tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a68d480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5203\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_output = trainer1.predict(test_dataset=tokenized_finetune_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74aea569",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(test_output.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "884f6de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label = np.array(test_dataset['label'])\n",
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46a463ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.label_ids == test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d31567f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8bb985cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07630213338458582"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61d4db0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보안 인쇄물의 위변조 확인 방법\n",
      "pred:  [{'label': 'LABEL_9', 'score': 0.1434182971715927}] , label:  0 \n",
      "\n",
      "보안 인쇄물의 위변조 확인 방법\n",
      "pred:  [{'label': 'LABEL_9', 'score': 0.1434182971715927}] , label:  0 \n",
      "\n",
      "  본 발명은 보안잉크가 인쇄된 보안 인쇄물에 관한 것으로서, 보다 상세하게는, 발광색상, 지속시간 또는 여기파장이 다른 형광체 또는 인광체를 포함하는 보안잉크가 인쇄된 보안 인쇄물에 관한 것이다. 이를 위해 보안 인쇄물은 제1영역 및 제2영역으로 구분되는 것으로서, 제1영역은 청색 형광체를 포함하는 보안잉크로 인쇄되고, 제2영역은 청색 형광체, 녹색 인광체 및 적색 인광체를 포함하는 보안잉크로 인쇄되는 것을 특징으로 한다.  \n",
      "  본 발명은 보안잉크가 인쇄된 보안 인쇄물에 관한 것으로서, 보다 상세하게는, 발광색상, 지속시간 또는 여기파장이 다른 형광체 또는 인광체를 포함하는 보안잉크가 인쇄된 보안 인쇄물에 관한 것이다. 이를 위해 보안 인쇄물은 제1영역 및 제2영역으로 구분되는 것으로서, 제1영역은 청색 형광체를 포함하는 보안잉크로 인쇄되고, 제2영역은 청색 형광체, 녹색 인광체 및 적색 인광체를 포함하는 보안잉크로 인쇄되는 것을 특징으로 한다.  \n",
      "UV 광원을 사용한 보안 인쇄물의 위변조 확인 방법에 있어서,피인쇄물을 준비하는 단계;UV 광원을 상기 제1영역 및 제2영역에 조사하여, 상기 제1영역은 제1색으로 발광하고, 제2영역은 제2색으로 발광하는 다색 발광 단계;UV 광원의 조사를 중지하는 단계; 및UV 조사가 중지된 이후, 상기 제1영역의 발광은 사라지고, 동시에 제2영역은 소정 시간 동안 제3색을 발광하고, 이후 제3색이 사라지거나, 제4색으로 일정 시간 동안 발광하는 색변환 단계;를 포함하고, 상기 피인쇄물은, 형광체을 포함하는 제1 보안 잉크를 사용하여 상기 피인쇄물의 표면에 제1영역이 인쇄되고, 형광체와 인광체을 포함하는 제2 보안 잉크를 사용하여 상기 피인쇄물의 표면에 제2영역이 인쇄된 것인, UV 광원을 사용한 보안 인쇄물의 위변조 확인 방법.\n",
      "UV 광원을 사용한 보안 인쇄물의 위변조 확인 방법에 있어서,제1영역과 제2영역으로 구분되는 인쇄 영역을 포함하는 피인쇄물을 준비하는 단계;UV 광원을 상기 제1영역 및 제2영역에 조사하여, 상기 제1영역은 제1색으로 발광하고, 제2영역은 제2색으로 발광하는 다색 발광 단계;UV 광원의 조사를 중지하는 단계; 및UV 조사가 중지된 이후, 상기 제1영역의 발광은 사라지고, 동시에 제2영역은 소정 시간 동안 제3색을 발광하고, 이후 제3색이 사라지거나, 제4색으로 일정 시간 동안 발광하는 색변환 단계;를 포함하고,상기 피인쇄물은, 형광체를 포함하는 제1 보안 잉크를 사용하여 상기 피인쇄물의 제1영역이 인쇄되고, 형광체와 인광체를 포함하는 제2 보안 잉크를 사용하여 상기 피인쇄물의 제2영역이 인쇄된 것이며,상기 제1 보안 잉크에 포함되는 형광체는, UV에 의해 발광하는 청색 형광체이고,상기 제2 보안 잉크에 포함되는 인광체는, 녹색 인광체 및/또는 적색 인광체를 포함하며,UV 조사 시 제1영역의 발광색과 제2영역의 발광색은 상이한 것을 특징으로 하는, UV 광원을 사용한 보안 인쇄물의 위변조 확인 방법.\n",
      "1. 발명은 상기와 같은 문제점을 해결하기 위한 것으로서, 본 발명의 목적은 여기광원을 받고 있을 때와 여기광원이 사라진 후 발광효과가 남아 있으면서 발광색상이 변화하는 보안 인쇄물을 제공하는데 있다.\n",
      "pred:  [{'label': 'LABEL_1', 'score': 0.8897316455841064}] , label:  0 \n",
      "\n",
      "1. 발명은 상기와 같은 문제점을 해결하기 위한 것으로서, 본 발명의 목적은 여기광원을 받고 있을 때와 여기광원이 사라진 후 발광효과가 남아 있으면서 발광색상이 변화하는 보안 인쇄물을 제공하는데 있다.\n",
      "pred:  [{'label': 'LABEL_1', 'score': 0.8897316455841064}] , label:  0 \n",
      "\n",
      "1. 발명의 일 실시예에 따른 보안 인쇄물은 제1영역 및 제2영역으로 구분되는 것으로서, 제1영역은 청색 형광체를 포함하는 보안잉크로 인쇄되고, 제2영역은 청색 형광체, 녹색 인광체 및 적색 인광체를 포함하는 보안잉크로 인쇄되는 것을 특징으로 한다.\n",
      "pred:  [{'label': 'LABEL_9', 'score': 0.20660096406936646}] , label:  0 \n",
      "\n",
      "1. 발명의 일 실시예에 따른 보안 인쇄물은 제1영역 및 제2영역으로 구분되는 것으로서, 제1영역은 청색 형광체를 포함하는 보안잉크로 인쇄되고, 제2영역은 청색 형광체, 녹색 인광체 및 적색 인광체를 포함하는 보안잉크로 인쇄되는 것을 특징으로 한다.\n",
      "pred:  [{'label': 'LABEL_9', 'score': 0.20660096406936646}] , label:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    try:\n",
    "        print(test_dataset['text'][i])\n",
    "        print('pred: ', text_classifier(test_dataset['text'][i]), ', label: ', test_dataset['label'][i], '\\n')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e1b8f36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_loss = 0\n",
    "epoch_acc = 0\n",
    "for epoch in range(n_epochs):\n",
    "    scheduler.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "545aea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_finetuned_bert/checkpoint-3000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert/checkpoint-7500\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert/checkpoint-3000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert/checkpoint-3000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model2 = BertForSequenceClassification.from_pretrained(\n",
    "    os.path.join('c09k_finetuned_bert', \"checkpoint-3000\"),\n",
    "    return_dict=True, num_labels=18)\n",
    "# load the tokenizer\n",
    "tokenizer2 = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4510dfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# training_args = TrainingArguments(output_dir=\"c09k_finetuned_bert\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"c09k_finetuned_bert2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1257f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_finetune_dataset['train'],\n",
    "    eval_dataset=tokenized_finetune_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer2,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "504d5ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9881\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6180\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6180' max='6180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6180/6180 13:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.107100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-1000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-1000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-1500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-1500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-2000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-2000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-2500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-2500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-3000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-3000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-3500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-3500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-4000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-4000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-4500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-4500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-5000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-5000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-5500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-5500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-6000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-6000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-6000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6180, training_loss=1.2481813918425426, metrics={'train_runtime': 810.8951, 'train_samples_per_second': 121.853, 'train_steps_per_second': 7.621, 'total_flos': 3250217273679360.0, 'train_loss': 1.2481813918425426, 'epoch': 10.0})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e81e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a71f4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://bo-10000.tistory.com/154\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auroc': auc\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "763d9064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_finetuned_bert/checkpoint-3000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert/checkpoint-7500\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert/checkpoint-3000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert/checkpoint-3000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9881\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6180' max='6180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6180/6180 13:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.107100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-1000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-1000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-1500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-1500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-2000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-2000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-2500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-2500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-3000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-3000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-3500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-3500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-4000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-4000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-4500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-4500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-5000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-5000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-5500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-5500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-6000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-6000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-6000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6180, training_loss=1.2481813918425426, metrics={'train_runtime': 816.681, 'train_samples_per_second': 120.99, 'train_steps_per_second': 7.567, 'total_flos': 3250217273679360.0, 'train_loss': 1.2481813918425426, 'epoch': 10.0})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model3 = BertForSequenceClassification.from_pretrained(\n",
    "    os.path.join('c09k_finetuned_bert2', \"checkpoint-6000\"),\n",
    "    return_dict=True, num_labels=18)\n",
    "# load the tokenizer\n",
    "tokenizer3 = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"c09k_finetuned_bert2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01, eval_steps=1000, resume_from_checkpoint=os.path.join('c09k_finetuned_bert', \"checkpoint-3000\")\n",
    ")\n",
    "trainer3 = Trainer(\n",
    "    model=model3,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_finetune_dataset['train'],\n",
    "    eval_dataset=tokenized_finetune_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer3,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "699b7b72",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_label\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_label' is not defined"
     ]
    }
   ],
   "source": [
    "test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d54d0a",
   "metadata": {},
   "source": [
    "### 테스트 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc732085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://bo-10000.tistory.com/154\n",
    "# # https://stackoverflow.com/questions/59666138/sklearn-roc-auc-score-with-multi-class-ovr-should-have-none-average-available\n",
    "# def compute_metrics(pred):\n",
    "#     labels = pred.label_ids\n",
    "#     preds = pred.predictions.argmax(-1)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     auc = roc_auc_score(labels, preds, multi_class=\"ovo\",average='macro')\n",
    "#     return {\n",
    "#         'accuracy': acc,\n",
    "#         'f1': f1,\n",
    "#         'precision': precision,\n",
    "#         'recall': recall,\n",
    "#         'auroc': auc\n",
    "#     }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5973ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_finetuned_bert2/checkpoint-6000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_finetuned_bert/checkpoint-3000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert2/checkpoint-6000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert2/checkpoint-6000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model3 = BertForSequenceClassification.from_pretrained(\n",
    "    os.path.join('c09k_finetuned_bert2', \"checkpoint-6000\"),\n",
    "    return_dict=True, num_labels=18)\n",
    "# load the tokenizer\n",
    "tokenizer3 = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"c09k_finetuned_bert2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01, eval_steps=1000, resume_from_checkpoint=os.path.join('c09k_finetuned_bert', \"checkpoint-3000\")\n",
    ")\n",
    "trainer3 = Trainer(\n",
    "    model=model3,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_finetune_dataset['train'],\n",
    "    eval_dataset=tokenized_finetune_dataset['test'],\n",
    "#     compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer3,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51165dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5203\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = trainer3.evaluate(eval_dataset=tokenized_finetune_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2244078c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.973349094390869,\n",
       " 'eval_runtime': 10.3277,\n",
       " 'eval_samples_per_second': 503.791,\n",
       " 'eval_steps_per_second': 31.566}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "212244c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5203\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 9, 9, ..., 7, 9, 9])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = trainer3.predict(test_dataset=tokenized_finetune_dataset['test'])\n",
    "pred = np.argmax(test_output.predictions, axis=1)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2c9bd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_label = np.array(test_dataset['label'])\n",
    "test_label = np.array(test_dataset['label'])\n",
    "test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f555db81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 5203\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_finetune_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10ca1d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5203"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a4b83e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df['label'] = test_label\n",
    "result_df['pred'] = pred\n",
    "result_df['input'] = np.array(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1069c0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>화합물, 이를 포함하는 시스테인 탐지용 조성물, 및 시스테인 검출 방법</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>본 발명은 화합물, 이를 포함하는 시스테인 탐지용 조성물, 및 시스테인 검출 방...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1. 발명은 화합물, 이를 포함하는 시스테인 탐지용 조성물, 및 시스테인 검출 방법...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>화합물, 이를 포함하는 시스테인 탐지용 조성물, 및 시스테인 검출 방법   본 발명...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>화합물, 이를 포함하는 시스테인 탐지용 조성물, 및 시스테인 검출 방법 하기 화학식...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>유기 전계발광 재료 및 디바이스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1.    하기 화학식 I, 화학식 II, 화학식 III 또는 화학식 IV의 리간드...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>유기 전계발광 재료 및 디바이스   하기 화학식 I, 화학식 II, 화학식 III ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>유기 전계발광 재료 및 디바이스 1.    하기 화학식 I, 화학식 II, 화학식 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5202</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>유기 전계발광 재료 및 디바이스 1. 양태에서, 본 개시는 하기 화학식 I, 화학식...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1329 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  pred                                              input\n",
       "99        1     1            화합물, 이를 포함하는 시스테인 탐지용 조성물, 및 시스테인 검출 방법\n",
       "100       1     1    본 발명은 화합물, 이를 포함하는 시스테인 탐지용 조성물, 및 시스테인 검출 방...\n",
       "102       1     1  1. 발명은 화합물, 이를 포함하는 시스테인 탐지용 조성물, 및 시스테인 검출 방법...\n",
       "104       1     1  화합물, 이를 포함하는 시스테인 탐지용 조성물, 및 시스테인 검출 방법   본 발명...\n",
       "105       1     1  화합물, 이를 포함하는 시스테인 탐지용 조성물, 및 시스테인 검출 방법 하기 화학식...\n",
       "...     ...   ...                                                ...\n",
       "5194      9     9                                  유기 전계발광 재료 및 디바이스\n",
       "5197      9     9  1.    하기 화학식 I, 화학식 II, 화학식 III 또는 화학식 IV의 리간드...\n",
       "5199      9     9  유기 전계발광 재료 및 디바이스   하기 화학식 I, 화학식 II, 화학식 III ...\n",
       "5201      9     9  유기 전계발광 재료 및 디바이스 1.    하기 화학식 I, 화학식 II, 화학식 ...\n",
       "5202      9     9  유기 전계발광 재료 및 디바이스 1. 양태에서, 본 개시는 하기 화학식 I, 화학식...\n",
       "\n",
       "[1329 rows x 3 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.loc[result_df['label']==result_df['pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "974cdc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pred</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>보안 인쇄물의 위변조 확인 방법</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>보안 인쇄물의 위변조 확인 방법</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>본 발명은 보안잉크가 인쇄된 보안 인쇄물에 관한 것으로서, 보다 상세하게는, 발...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>본 발명은 보안잉크가 인쇄된 보안 인쇄물에 관한 것으로서, 보다 상세하게는, 발...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UV 광원을 사용한 보안 인쇄물의 위변조 확인 방법에 있어서,피인쇄물을 준비하는 단...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5193</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>유기금속 화합물 및 이를 포함한 유기 발광 소자 1. 하기 화학식 1로 표시되는 유...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>하기 화학식 I, 화학식 II, 화학식 III 또는 화학식 IV의 리간드 LA를...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>하기 화학식 I, 화학식 II, 화학식 III 또는 화학식 IV의 리간드 LA를 포...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1. 양태에서, 본 개시는 하기 화학식 I, 화학식 II, 화학식 III 또는 화학...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>유기 전계발광 재료 및 디바이스 하기 화학식 I, 화학식 II, 화학식 III 또는...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3874 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  pred                                              input\n",
       "0         0     9                                  보안 인쇄물의 위변조 확인 방법\n",
       "1         0     9                                  보안 인쇄물의 위변조 확인 방법\n",
       "2         0     9    본 발명은 보안잉크가 인쇄된 보안 인쇄물에 관한 것으로서, 보다 상세하게는, 발...\n",
       "3         0     9    본 발명은 보안잉크가 인쇄된 보안 인쇄물에 관한 것으로서, 보다 상세하게는, 발...\n",
       "4         0     1  UV 광원을 사용한 보안 인쇄물의 위변조 확인 방법에 있어서,피인쇄물을 준비하는 단...\n",
       "...     ...   ...                                                ...\n",
       "5193      3     9  유기금속 화합물 및 이를 포함한 유기 발광 소자 1. 하기 화학식 1로 표시되는 유...\n",
       "5195      9     7    하기 화학식 I, 화학식 II, 화학식 III 또는 화학식 IV의 리간드 LA를...\n",
       "5196      9     7  하기 화학식 I, 화학식 II, 화학식 III 또는 화학식 IV의 리간드 LA를 포...\n",
       "5198      9     7  1. 양태에서, 본 개시는 하기 화학식 I, 화학식 II, 화학식 III 또는 화학...\n",
       "5200      9     7  유기 전계발광 재료 및 디바이스 하기 화학식 I, 화학식 II, 화학식 III 또는...\n",
       "\n",
       "[3874 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.loc[result_df['label']!=result_df['pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ea17bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/c09k_label_ind.pickle', 'rb') as f:\n",
    "    label_ind = pickle.load(f)\n",
    "ind_label = {v:k for k, v in label_ind.items()}\n",
    "# ind_label = pickle.loads('data/c09k_ind_label.pickle', )\n",
    "# label_ind = pickle.loads('data/c09k_label_ind.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "02de9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df2 = result_df.replace({'label': ind_label, 'pred': ind_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "912afad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df2.to_csv('data/predict_result_c09k.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eba4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_pretrain",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
