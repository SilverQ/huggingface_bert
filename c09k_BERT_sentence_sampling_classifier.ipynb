{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10fa50a",
   "metadata": {},
   "source": [
    "### Sentence Sampling Classifier\n",
    "* c09k 분류 학습 모델을 사용하여 분류한 결과 분류 예측에 성공한 문장과 실패한 문장을 구분\n",
    "* 분류 예측에 실패한 문장의 라벨을 0, 성공한 문장의 라벨을 1로 하여 학습 데이터를 생성\n",
    "* 분류 예측에 활용될 수 있는 문장을 선별하는 분류기를 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e678dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "from datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import nltk\n",
    "from nltk.data import load\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42187cb",
   "metadata": {},
   "source": [
    "### Loading the c09k Classify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691b8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9182b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = 'c09k_pretrained_bert'\n",
    "model_load_path = 'c09k_finetuned_bert_512'\n",
    "chkpoint = 'checkpoint-5000'\n",
    "model_save_path = 'c09k_sampling_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31613b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_finetuned_bert_512/checkpoint-5000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert_512_2/checkpoint-9000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert_512/checkpoint-5000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert_512/checkpoint-5000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model1 = BertForSequenceClassification.from_pretrained(os.path.join(model_load_path, chkpoint), return_dict=True, num_labels=18)\n",
    "# load the tokenizer\n",
    "tokenizer1 = BertTokenizerFast.from_pretrained(tokenizer_path, vocab_size=8000, local_files_only=True)\n",
    "optimizer = AdamW(model1.parameters(), lr=1e-5)\n",
    "# optimizer = A AdamW(model1.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de9f260",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model1.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model1.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19c748",
   "metadata": {},
   "source": [
    "### finetuning data prepare\n",
    "* 성능 비교를 목적으로 KoBERT의 학습 데이터를 그대로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e46fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c55c841c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9881\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5203\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df = pd.read_csv('data/train_C09K11_220715.txt', sep='\\t')  # text와 라벨 파일\n",
    "test_dataset_df = pd.read_csv('data/test_C09K11_220715.txt', sep='\\t')\n",
    "train_dataset = Dataset.from_pandas(train_data_df)  # Dataset 객체 생성\n",
    "test_dataset = Dataset.from_pandas(test_dataset_df)\n",
    "finetune_dataset = DatasetDict()  # DatasetDict 객체 생성\n",
    "finetune_dataset['train'] = train_dataset\n",
    "finetune_dataset['test'] = test_dataset\n",
    "finetune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "125307ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer1(train_dataset['text'][:16], return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "269944cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>광활성 형광체 프로브 및 이를 이용한 암세포 검출방법</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>본 발명은 광활성 형광체 검출방법에 관한 것으로서, 화학식 1로 표시되는 화합물...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>하기 [화학식 1]로 표시되는 OPA 또는 TPA 구조체를 포함하는 활용한 광활성 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                      광활성 형광체 프로브 및 이를 이용한 암세포 검출방법      1\n",
       "1    본 발명은 광활성 형광체 검출방법에 관한 것으로서, 화학식 1로 표시되는 화합물...      1\n",
       "2  하기 [화학식 1]로 표시되는 OPA 또는 TPA 구조체를 포함하는 활용한 광활성 ...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c90fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_warmup_steps = 2\n",
    "n_epochs = 5\n",
    "num_train_steps = n_epochs + 1\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4721859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 함수 객체 생성\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer1(examples[\"text\"], truncation=True, max_length=512, padding=True)\n",
    "def preprocess_function1(examples):\n",
    "    return examples['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e39a7174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.91ba/s]\n",
      "100%|██████████| 6/6 [00:00<00:00,  6.99ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_finetune_dataset = finetune_dataset.map(preprocess_function, batched=True)  \n",
    "# DatasetDict의 'text'를 토크나이징, finetune_dataset에는 features: ['text', 'label']만 있었으나,\n",
    "# tokenized_finetune_dataset에는 'input_ids', 'token_type_ids', 'attention_mask'가 추가됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3717a0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9881\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 5203\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_finetune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c1ca5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_finetune_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdacdf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f656ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# training_args = TrainingArguments(output_dir=\"c09k_finetuned_bert\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"c09k_finetuned_bert_512\",\n",
    "    evaluation_strategy=\"epoch\",    # 'steps': evaluate each `logging_steps`, 'epoch'  : each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffd19b",
   "metadata": {},
   "source": [
    "### c09k 모델 evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "390b3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?pipeline\n",
    "text_classifier = pipeline('text-classification', model=model1, tokenizer=tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87eceea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc0ca02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model1,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_finetune_dataset['train'],\n",
    "    eval_dataset=tokenized_finetune_dataset['test'],\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     tokenizer=tokenizer1,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a68d480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5203\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_output = trainer.predict(test_dataset=tokenized_finetune_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74aea569",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(test_output.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "884f6de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array(test_dataset['label'])\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30724e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label = finetune_dataset['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46a463ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output.label_ids == test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38637dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df['label'] = y_true\n",
    "result_df['pred'] = y_pred\n",
    "result_df['input'] = np.array(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a5b21d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 9881\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_output1 = trainer1.predict(test_dataset=tokenized_finetune_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20e7b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = np.argmax(test_output1.predictions, axis=1)\n",
    "y_true1 = np.array(train_dataset['label'])\n",
    "result_df1 = pd.DataFrame()\n",
    "result_df1['label'] = y_true1\n",
    "result_df1['pred'] = y_pred1\n",
    "result_df1['input'] = np.array(train_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe31ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_total = pd.concat([result_df, result_df1])\n",
    "result_total.reset_index(inplace=True)\n",
    "result_total.pop('index')\n",
    "result_total['target'] = result_total.apply(lambda x : 0 if x['pred'] != x['label'] else 1, axis=1)\n",
    "result_total.pop('label')\n",
    "result_total.pop('pred')\n",
    "result_total.to_csv('data/c09k_sampling_data.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0d269",
   "metadata": {},
   "source": [
    "### split_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cde387c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a05c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input = train_test_split(result_total, random_state=15, stratify=result_total['target'], shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ca27030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12067, 2) (3017, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape, test_input.shape)  # (12067, 2) (3017, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f98e1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.to_csv('data/c09k_sampling_train_input.csv', sep='\\t', encoding='utf-8', index=False)\n",
    "test_input.to_csv('data/c09k_sampling_test_input.csv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3311298",
   "metadata": {},
   "source": [
    "### sampling model train set load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7277e592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12067\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 3017\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df1 = pd.read_csv('data/c09k_sampling_train_input.csv', sep='\\t')  # text와 라벨 파일\n",
    "test_dataset_df1 = pd.read_csv('data/c09k_sampling_test_input.csv', sep='\\t')\n",
    "train_data_df1.rename(columns={'input':'text', 'target':'label'}, inplace=True)\n",
    "test_dataset_df1.rename(columns={'input':'text', 'target':'label'}, inplace=True)\n",
    "train_dataset1 = Dataset.from_pandas(train_data_df1)  # Dataset 객체 생성\n",
    "test_dataset1 = Dataset.from_pandas(test_dataset_df1)\n",
    "finetune_dataset1 = DatasetDict()  # DatasetDict 객체 생성\n",
    "finetune_dataset1['train'] = train_dataset1\n",
    "finetune_dataset1['test'] = test_dataset1\n",
    "finetune_dataset1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25071b27",
   "metadata": {},
   "source": [
    "### Training Sampling Model\n",
    "* 9.6일의 Fine-tuning시 로드한 pre-trained model을 사용\n",
    "     * model_path = \"c09k_pretrained_bert_512_2\", \"checkpoint-9000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02fda7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_pretrained_bert_512_2/checkpoint-9000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert_512/checkpoint-15000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_pretrained_bert_512_2/checkpoint-9000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at c09k_pretrained_bert_512_2/checkpoint-9000 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at c09k_pretrained_bert_512_2/checkpoint-9000 and are newly initialized: ['bert.pooler.dense.bias', 'classifier.bias', 'classifier.weight', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model2 = BertForSequenceClassification.from_pretrained(\n",
    "    os.path.join('c09k_pretrained_bert_512_2', 'checkpoint-9000'),\n",
    "    return_dict=True, num_labels=2)\n",
    "# load the tokenizer\n",
    "tokenizer2 = BertTokenizerFast.from_pretrained(tokenizer_path, vocab_size=8000, local_files_only=True)\n",
    "optimizer = AdamW(model2.parameters(), lr=1e-5)\n",
    "# optimizer = A AdamW(model1.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ab22b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model2.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model2.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc3f349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:02<00:00,  6.31ba/s]\n",
      "100%|██████████| 4/4 [00:00<00:00,  8.14ba/s]\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 함수 객체 생성\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer2(examples[\"text\"], truncation=True, max_length=512, padding=True)\n",
    "# def preprocess_function1(examples):\n",
    "#     return examples['label']\n",
    "tokenized_finetune_dataset2 = finetune_dataset1.map(preprocess_function, batched=True)  \n",
    "data_collator2 = DataCollatorWithPadding(tokenizer=tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98bc101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 12067\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3017\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_finetune_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29309aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# training_args = TrainingArguments(output_dir=\"c09k_finetuned_bert\")\n",
    "training_args2 = TrainingArguments(\n",
    "    output_dir=model_save_path,\n",
    "    evaluation_strategy=\"steps\",    # 'steps': evaluate each `logging_steps`, 'epoch'  : each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=100,\n",
    "    logging_steps=500,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=500,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdf56bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b7f36c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args2,\n",
    "    train_dataset=tokenized_finetune_dataset2['train'],\n",
    "    eval_dataset=tokenized_finetune_dataset2['test'],\n",
    "#     compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer2,\n",
    "    data_collator=data_collator2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63e34e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 12067\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 37700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16501' max='37700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16501/37700 8:40:22 < 11:08:37, 0.53 it/s, Epoch 43.77/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.625467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.616343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.555200</td>\n",
       "      <td>0.653431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.529700</td>\n",
       "      <td>0.791353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.527500</td>\n",
       "      <td>0.740984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>0.767063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.506100</td>\n",
       "      <td>0.893558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.886150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>0.941815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.490700</td>\n",
       "      <td>0.953361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.489800</td>\n",
       "      <td>0.861793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.477700</td>\n",
       "      <td>1.019963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.482900</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.478600</td>\n",
       "      <td>1.262196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.478400</td>\n",
       "      <td>1.116668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.469900</td>\n",
       "      <td>1.171676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.470200</td>\n",
       "      <td>1.190876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.468600</td>\n",
       "      <td>1.436488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.466100</td>\n",
       "      <td>1.243666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.468900</td>\n",
       "      <td>1.418642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>1.358890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>1.390572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>1.389251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.461900</td>\n",
       "      <td>1.379360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>1.409828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.462100</td>\n",
       "      <td>1.380571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.455300</td>\n",
       "      <td>1.233226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.455700</td>\n",
       "      <td>1.565032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.449800</td>\n",
       "      <td>1.670675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.451400</td>\n",
       "      <td>1.452541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.449700</td>\n",
       "      <td>1.558865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.451100</td>\n",
       "      <td>1.597115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.444800</td>\n",
       "      <td>1.395992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-1000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-1000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-1500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-1500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-2000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-2000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-2500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-2500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-3000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-3000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-3500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-3500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-4000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-4000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-4500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-4500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-4500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-5000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-5000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-5500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-5500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-5500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-6000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-6000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-6500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-6500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-6500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-7000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-7000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-7000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-7500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-7500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-7500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-8000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-8000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-8000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-8500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-8500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-8500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-9000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-9000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-9000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-9500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-9500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-9500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-10000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-10000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-10000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-10500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-10500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-10500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-11000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-11000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-11000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-11500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-11500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-11500/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in c09k_sampling_model/checkpoint-11500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-12000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-12000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-12000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-12500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-12500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-12500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-13000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-13000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-13000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-13500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-13500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-13500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-14000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-14000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-14000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-14500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-14500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-14500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-15000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-15000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-15000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-15500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-15500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-15500/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-16000\n",
      "Configuration saved in c09k_sampling_model/checkpoint-16000/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-16000/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3017\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_sampling_model/checkpoint-16500\n",
      "Configuration saved in c09k_sampling_model/checkpoint-16500/config.json\n",
      "Model weights saved in c09k_sampling_model/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_sampling_model/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_sampling_model/checkpoint-16500/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:319] . unexpected pos 619618496 vs 619618384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/torch/serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 379\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/torch/serialization.py:604\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    603\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 604\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/trainer.py:1505\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1502\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1504\u001b[0m )\n\u001b[0;32m-> 1505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/trainer.py:1824\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   1822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1824\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1825\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1826\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/trainer.py:2049\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2049\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2050\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/trainer.py:2155\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2152\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, SCALER_NAME))\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[1;32m   2154\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 2155\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m caught_warnings:\n\u001b[1;32m   2157\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, SCHEDULER_NAME))\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/torch/serialization.py:380\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    379\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m--> 380\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    381\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m~/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/torch/serialization.py:259\u001b[0m, in \u001b[0;36m_open_zipfile_writer_buffer.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:319] . unexpected pos 619618496 vs 619618384"
     ]
    }
   ],
   "source": [
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a979e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d37fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1678ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a737a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df2 = result_df.replace({'label': ind_label, 'pred': ind_label})\n",
    "result_df2.to_csv('data/finetuned_predict_result_c09k.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d76a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "45c57e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "?accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef8d5506",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        99\n",
      "           1       0.00      0.00      0.00        72\n",
      "           2       0.00      0.00      0.00        70\n",
      "           3       0.05      0.01      0.01       691\n",
      "           4       0.10      0.08      0.09       469\n",
      "           5       0.10      0.09      0.09       435\n",
      "           6       0.11      0.11      0.11       463\n",
      "           7       0.00      0.00      0.00       589\n",
      "           8       0.03      0.02      0.02       356\n",
      "           9       0.14      0.01      0.03       568\n",
      "          10       0.04      0.04      0.04       296\n",
      "          11       0.07      0.14      0.10       202\n",
      "          12       0.00      0.00      0.00       133\n",
      "          13       0.00      0.00      0.00       196\n",
      "          14       0.03      0.07      0.04       122\n",
      "          15       0.05      0.19      0.08       219\n",
      "          16       0.00      0.00      0.00       205\n",
      "          17       0.00      0.33      0.01        18\n",
      "\n",
      "    accuracy                           0.05      5203\n",
      "   macro avg       0.04      0.06      0.03      5203\n",
      "weighted avg       0.06      0.05      0.04      5203\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_true, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea75909f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "accuracy_score() got an unexpected keyword argument 'average'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[1;32m      2\u001b[0m recall_score(y_true\u001b[38;5;241m=\u001b[39my_true, y_pred\u001b[38;5;241m=\u001b[39my_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m f1_score(y_true\u001b[38;5;241m=\u001b[39my_true, y_pred\u001b[38;5;241m=\u001b[39my_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "\u001b[0;31mTypeError\u001b[0m: accuracy_score() got an unexpected keyword argument 'average'"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_true=y_true, y_pred=y_pred, average=None),\n",
    "recall_score(y_true=y_true, y_pred=y_pred, average=None),\n",
    "f1_score(y_true=y_true, y_pred=y_pred, average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b63b90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   8,  11,  12,   8,   0,   2,   5,   0,  10,   0,\n",
       "          6,   0,   1,   1,  35],\n",
       "       [  0,   0,   0,   0,   1,   5,  16,   0,   2,   0,   3,  10,   0,\n",
       "          2,   0,   2,  14,  17],\n",
       "       [  0,   0,   0,   0,   4,   2,   6,   0,  25,   2,   0,   0,   0,\n",
       "          7,   9,   8,   0,   7],\n",
       "       [ 12,   3,   4,   5,  37,  36,  62,   1,  21,  10,  42,  36,   0,\n",
       "          5,  35, 117,   7, 258],\n",
       "       [  2,   1,   4,   1,  38,  24,  41,   0,  18,   5,  18,  36,   0,\n",
       "          0,  23,  38,   3, 217],\n",
       "       [ 10,   1,   0,   5,  40,  37,  37,   0,  13,   5,  27,  33,   0,\n",
       "          2,  17,  36,   0, 172],\n",
       "       [  6,   3,   0,  14,  22,  51,  50,   0,  22,   1,  33,  27,   0,\n",
       "          3,  19,  86,   0, 126],\n",
       "       [  9,   2,   0,  12,  27,  47,  57,   0,  29,   4,  34,  35,   0,\n",
       "          4,  27, 142,   0, 160],\n",
       "       [  1,   0,   0,   4,  50,  24,  26,   0,   6,   4,  20,  35,   0,\n",
       "          0,  14,  50,   0, 122],\n",
       "       [ 20,   5,   4,   9,  33,  22,  61,   1,  33,   8,  64,  54,   0,\n",
       "          5,  29,  99,   7, 114],\n",
       "       [  2,   0,   6,   0,  36,  21,  21,   0,   4,   2,  12,  24,   0,\n",
       "          2,  30,  65,   0,  71],\n",
       "       [  1,   0,   6,   0,  34,  11,  11,   0,   3,   2,   1,  29,   0,\n",
       "          3,  35,  21,   0,  45],\n",
       "       [  0,   1,   0,   1,  19,   5,  19,   0,   4,   7,  10,  11,   0,\n",
       "          1,   6,  28,   0,  21],\n",
       "       [  0,   2,   0,  13,  19,  23,  21,   0,   5,   2,  16,  15,   0,\n",
       "          0,  12,  20,   0,  48],\n",
       "       [  2,   7,   0,   6,  11,  13,   0,   0,   2,   0,   7,   9,   0,\n",
       "          2,   9,  19,   0,  35],\n",
       "       [  2,   6,   0,  16,  13,  14,  10,   0,   5,   1,  10,  16,   0,\n",
       "          3,  16,  42,   0,  65],\n",
       "       [  5,   0,   0,   0,   2,  22,  25,   0,  13,   0,  20,  14,   0,\n",
       "          2,   4,  31,   0,  67],\n",
       "       [  4,   0,   0,   0,   0,   1,   0,   0,   3,   0,   2,   0,   0,\n",
       "          0,   0,   2,   0,   6]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_label, pred\n",
    "#                  , labels=['K0', 'K1', 'K21', 'K211', 'K212', 'K2121', 'K2122', 'K2123', 'K213', 'K2131', 'K2132', 'K2133', 'K214', 'K22', 'K23', 'K24', 'K241', 'K242']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "04e4799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_c09k(path, chkpt):\n",
    "    model = BertForSequenceClassification.from_pretrained(os.path.join(path, chkpt), return_dict=True, num_labels=18)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('c09k_pretrained_bert', vocab_size=8000, local_files_only=True)\n",
    "    optimizer = AdamW(model1.parameters(), lr=1e-5)\n",
    "    text_classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "    # training_args = TrainingArguments(output_dir=\"c09k_finetuned_bert\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"c09k_finetuned_bert_512\",\n",
    "        evaluation_strategy=\"epoch\",    # 'steps': evaluate each `logging_steps`, 'epoch'  : each epoch\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=500,             # evaluate, log and save model checkpoints every 1000 step\n",
    "        save_steps=1000,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        eval_dataset=tokenized_finetune_dataset['test'],\n",
    "#         compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    test_output = trainer.predict(test_dataset=tokenized_finetune_dataset['test'])\n",
    "    y_pred = np.argmax(test_output.predictions, axis=1)\n",
    "    y_true = np.array(test_dataset['label'])\n",
    "    return [classification_report(y_true, y_pred), confusion_matrix(y_true, y_pred), y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d2056aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_finetuned_bert_512/checkpoint-3000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert_512_2/checkpoint-9000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert_512/checkpoint-3000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert_512/checkpoint-3000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5203\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "loading configuration file c09k_finetuned_bert_512/checkpoint-6000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert_512_2/checkpoint-9000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert_512/checkpoint-6000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert_512/checkpoint-6000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5203\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "loading configuration file c09k_finetuned_bert_512/checkpoint-4000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert_512_2/checkpoint-9000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert_512/checkpoint-4000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert_512/checkpoint-4000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5203\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "loading configuration file c09k_finetuned_bert_512/checkpoint-5000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert_512_2/checkpoint-9000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert_512/checkpoint-5000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert_512/checkpoint-5000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5203\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "chks = [['c09k_finetuned_bert_512', 'checkpoint-3000'],\n",
    "['c09k_finetuned_bert_512', 'checkpoint-6000'],\n",
    "['c09k_finetuned_bert_512', 'checkpoint-9000'],\n",
    "['c09k_finetuned_bert_512', 'checkpoint-12000'],\n",
    "['c09k_finetuned_bert_512', 'checkpoint-15000'],\n",
    "['c09k_finetuned_bert_512', 'checkpoint-4000'],\n",
    "['c09k_finetuned_bert_512', 'checkpoint-5000'],\n",
    "['c09k_finetuned_bert_512', 'checkpoint-7000']]\n",
    "result = []\n",
    "for chk in chks:\n",
    "    try:\n",
    "        report, conf_mat, y_pred = eval_c09k(chk[0], chk[1])\n",
    "        result.append([chk, report, conf_mat, y_pred])\n",
    "    except:\n",
    "        pass\n",
    "#     finally:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "99e4f3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c09k_finetuned_bert_512', 'checkpoint-3000']               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        99\n",
      "           1       0.62      0.92      0.74        72\n",
      "           2       0.89      0.70      0.78        70\n",
      "           3       0.18      0.05      0.08       691\n",
      "           4       0.25      0.41      0.32       469\n",
      "           5       0.20      0.17      0.19       435\n",
      "           6       0.15      0.27      0.19       463\n",
      "           7       0.19      0.17      0.18       589\n",
      "           8       0.17      0.06      0.09       356\n",
      "           9       0.38      0.55      0.45       568\n",
      "          10       0.26      0.24      0.25       296\n",
      "          11       0.17      0.24      0.20       202\n",
      "          12       0.00      0.00      0.00       133\n",
      "          13       0.04      0.03      0.03       196\n",
      "          14       0.21      0.25      0.22       122\n",
      "          15       0.18      0.03      0.05       219\n",
      "          16       0.29      0.41      0.34       205\n",
      "          17       0.07      0.28      0.12        18\n",
      "\n",
      "    accuracy                           0.24      5203\n",
      "   macro avg       0.24      0.27      0.23      5203\n",
      "weighted avg       0.22      0.24      0.21      5203\n",
      " \n",
      " [[  0  25   0   6   3  29   2   2   3  14   1   0   6   0   0   0   1   7]\n",
      " [  0  66   0   2   0   0   0   0   0   2   0   0   0   2   0   0   0   0]\n",
      " [  0   0  49   2   2   0   0   1   0   8   0   0   4   1   0   1   0   2]\n",
      " [  0   0   6  36 148  42 108  81  18 150   8  24  14  29   5   1  15   6]\n",
      " [  0   0   0  17 194  66   5  47   8  10  33  28   8   2  11   0  37   3]\n",
      " [  0   0   0  12 119  74  40  37  20  56  13  33   4   9   5   1   7   5]\n",
      " [  0   2   0  14  75  10 127  51  10  65  16  19  19  15   9   9  12  10]\n",
      " [  0   0   0  33  68   9 146  98  13  63  36  25  27  25   9   6  21  10]\n",
      " [  0   0   0   7  39  61  65  42  21  54   2  24   2   7   1   5  21   5]\n",
      " [  0   0   0   4   8  12 142  12  17 311   6   2   7  34   5   1   4   3]\n",
      " [  0   0   0  19  16   8  53  14   1   4  72  28   7  14  13   0  46   1]\n",
      " [  0   0   0   0   8  15  25  10   1   4  46  48   6  14  12   0  10   3]\n",
      " [  0   0   0   6   1   4  40  16   3   6  14  17   0   1   0   0  23   2]\n",
      " [  0   2   0  12  21  15  47   8   2  54   2  11   6   6   5   1   4   0]\n",
      " [  0   5   0   1   4  10  13  29   3   1   5  11   5   2  30   2   1   0]\n",
      " [  0   7   0   3  13   9  37  47   4   9  12  12  15   4  39   6   2   0]\n",
      " [  0   0   0  28  29   0   7  24   0   0  13   0  12   1   2   0  84   5]\n",
      " [  0   0   0   0  13   0   0   0   0   0   0   0   0   0   0   0   0   5]] \n",
      " --------------------------------------------------------\n",
      "['c09k_finetuned_bert_512', 'checkpoint-6000']               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        99\n",
      "           1       0.64      0.92      0.75        72\n",
      "           2       0.89      0.73      0.80        70\n",
      "           3       0.16      0.04      0.07       691\n",
      "           4       0.25      0.50      0.34       469\n",
      "           5       0.23      0.13      0.16       435\n",
      "           6       0.15      0.18      0.17       463\n",
      "           7       0.19      0.35      0.24       589\n",
      "           8       0.20      0.04      0.07       356\n",
      "           9       0.38      0.52      0.44       568\n",
      "          10       0.28      0.22      0.24       296\n",
      "          11       0.22      0.32      0.26       202\n",
      "          12       0.00      0.00      0.00       133\n",
      "          13       0.03      0.01      0.01       196\n",
      "          14       0.20      0.25      0.22       122\n",
      "          15       0.29      0.02      0.04       219\n",
      "          16       0.34      0.40      0.37       205\n",
      "          17       0.10      0.56      0.17        18\n",
      "\n",
      "    accuracy                           0.25      5203\n",
      "   macro avg       0.25      0.29      0.24      5203\n",
      "weighted avg       0.23      0.25      0.22      5203\n",
      " \n",
      " [[  0  26   0  10   1  16   0   2   2  15   2   2   9   0   1   4   2   7]\n",
      " [  0  66   0   1   1   0   0   0   0   2   0   0   0   2   0   0   0   0]\n",
      " [  0   0  51   1   9   0   4   0   0   0   1   0   2   0   0   0   0   2]\n",
      " [  0   0   6  29 167  27  89 158   4 141   8  16   9   9   6   0   7  15]\n",
      " [  0   0   0  30 234  40   4  50   8  11  17  35   4   0   8   0  24   4]\n",
      " [  0   0   0  18 140  55  19  80   7  60   7  26   6   1   4   1   0  11]\n",
      " [  0   1   0   7  90   5  85 127   1  57  19  11  10   3  16   3  12  16]\n",
      " [  0   0   0  10  81   4 102 207   8  60  37  17   7   4  15   1  27   9]\n",
      " [  0   0   0  28  49  39  25  92  16  59   4  28   3   1   1   1   7   3]\n",
      " [  0   0   0   6  12  12  84 112   6 297   4   3   5  11   3   1   3   9]\n",
      " [  0   0   0   4  19  10  52  34   8   4  64  38   0   2  10   0  50   1]\n",
      " [  0   0   0   0  19  11  22  23   7   4  28  65   5   2   8   0   7   1]\n",
      " [  0   0   0   7   2   2  18  41   3   5   6  20   0   1   3   0  23   2]\n",
      " [  0   2   0  12  31  12  20  40   7  54   1   9   2   1   4   1   0   0]\n",
      " [  0   3   0   5  10   6   3  41   2   4   0  12   6   0  30   0   0   0]\n",
      " [  0   5   0   5  28   3  18  73   3   8   3  16   9   0  39   5   2   2]\n",
      " [  0   0   0   6  25   2  18  23   0   2  28   0   5   1   2   0  83  10]\n",
      " [  0   0   0   0   8   0   0   0   0   0   0   0   0   0   0   0   0  10]] \n",
      " --------------------------------------------------------\n",
      "['c09k_finetuned_bert_512', 'checkpoint-4000']               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        99\n",
      "           1       0.66      0.92      0.77        72\n",
      "           2       0.89      0.70      0.78        70\n",
      "           3       0.22      0.13      0.16       691\n",
      "           4       0.21      0.38      0.27       469\n",
      "           5       0.24      0.14      0.18       435\n",
      "           6       0.15      0.11      0.12       463\n",
      "           7       0.18      0.37      0.24       589\n",
      "           8       0.08      0.01      0.02       356\n",
      "           9       0.39      0.51      0.45       568\n",
      "          10       0.42      0.07      0.12       296\n",
      "          11       0.21      0.48      0.29       202\n",
      "          12       0.00      0.00      0.00       133\n",
      "          13       0.00      0.00      0.00       196\n",
      "          14       0.14      0.30      0.19       122\n",
      "          15       0.21      0.02      0.04       219\n",
      "          16       0.34      0.40      0.37       205\n",
      "          17       0.00      0.00      0.00        18\n",
      "\n",
      "    accuracy                           0.24      5203\n",
      "   macro avg       0.24      0.25      0.22      5203\n",
      "weighted avg       0.23      0.24      0.21      5203\n",
      " \n",
      " [[  0  21   0  19   3  13   0   2   3  12   0   4   9   0   9   2   0   2]\n",
      " [  0  66   0   0   1   0   0   0   0   0   0   0   0   0   4   1   0   0]\n",
      " [  0   0  49   1  12   0   0   0   0   0   0   2   4   0   2   0   0   0]\n",
      " [  0   0   6  90 136  22  47 166   6 135   0  33   8   4  17   4   4  13]\n",
      " [  0   0   0  90 178  49   3  46   3   9   0  54   3   0   8   0  25   1]\n",
      " [  0   0   0  69 111  63   9  91   2  55   0  26   0   0   9   0   0   0]\n",
      " [  0   1   0  14 101   4  49 142   6  53   5  24  10   0  24   4  15  11]\n",
      " [  0   0   0  26  91   5  53 216  13  55  13  37  11   0  28   0  34   7]\n",
      " [  0   0   0  47  40  35  16 109   5  54   0  33   2   0   3   3   8   1]\n",
      " [  0   0   0   5  11   9  52 151   5 290   0   9   6   4  14   1   1  10]\n",
      " [  0   0   0  10  15  15  38  34   3   5  20  76  18   0  12   0  48   2]\n",
      " [  0   0   0   4  24  19  20  18   2   4   0  97   3   0   8   0   2   1]\n",
      " [  0   0   0   0   1   4  18  42   4   6   3  22   0   0   9   4  19   1]\n",
      " [  0   2   0  12  29  14  12  47   5  52   0   8   4   0  11   0   0   0]\n",
      " [  0   4   0  12  12   6   3  34   4   0   0   9   2   0  36   0   0   0]\n",
      " [  0   6   0   6  26   6  14  63   5   5   0  18   9   0  53   5   0   3]\n",
      " [  0   0   0   5  30   2   2  39   0   0   7   4  23   1  11   0  81   0]\n",
      " [  0   0   0   0  18   0   0   0   0   0   0   0   0   0   0   0   0   0]] \n",
      " --------------------------------------------------------\n",
      "['c09k_finetuned_bert_512', 'checkpoint-5000']               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        99\n",
      "           1       0.71      0.89      0.79        72\n",
      "           2       0.86      0.73      0.79        70\n",
      "           3       0.18      0.05      0.08       691\n",
      "           4       0.25      0.50      0.34       469\n",
      "           5       0.19      0.15      0.17       435\n",
      "           6       0.16      0.09      0.12       463\n",
      "           7       0.20      0.35      0.25       589\n",
      "           8       0.10      0.01      0.02       356\n",
      "           9       0.38      0.65      0.48       568\n",
      "          10       0.32      0.21      0.25       296\n",
      "          11       0.21      0.34      0.26       202\n",
      "          12       0.00      0.00      0.00       133\n",
      "          13       0.08      0.01      0.01       196\n",
      "          14       0.18      0.32      0.23       122\n",
      "          15       0.24      0.03      0.06       219\n",
      "          16       0.29      0.49      0.37       205\n",
      "          17       0.10      0.44      0.16        18\n",
      "\n",
      "    accuracy                           0.26      5203\n",
      "   macro avg       0.25      0.29      0.24      5203\n",
      "weighted avg       0.22      0.26      0.22      5203\n",
      " \n",
      " [[  0  20   0  12   2  29   0   2   4   8   2   2   4   0   1   2   2   9]\n",
      " [  0  64   2   1   1   0   0   0   0   0   0   0   2   0   0   2   0   0]\n",
      " [  0   0  51   2   7   0   2   0   0   0   0   4   2   0   0   0   0   2]\n",
      " [  0   0   6  33 163  44  40 165   2 169   1  21   5   1   6   2  23  10]\n",
      " [  0   0   0  21 236  52   3  53   4  11  12  38   2   0  13   0  21   3]\n",
      " [  0   0   0  12 146  65   7  78   2  76   2  25   4   1   8   1   0   8]\n",
      " [  0   0   0  10  86   6  43 121   1  81  16  15   6   2  26   5  33  12]\n",
      " [  0   0   0  14  72   4  52 209   8  80  35  17   5   1  30   2  51   9]\n",
      " [  0   0   0  17  52  53   9  97   4  75   1  27   2   2   2   2  10   3]\n",
      " [  0   0   0  17  12  26  37  75   3 367   2   4   4   3   4   2   6   6]\n",
      " [  0   0   0   5  30   3  34  36   1   7  62  42   4   0  16   0  56   0]\n",
      " [  0   0   0   0  32   7  20  25   2   3  21  69   4   0  14   0   5   0]\n",
      " [  0   0   0  10   1   6   6  44   1   5  14  19   0   1   3   1  22   0]\n",
      " [  0   0   0  11  23  26   9  29   3  68   2   9   2   1   8   1   2   2]\n",
      " [  0   3   0   4   9   8   0  33   2   2   1  11   5   0  39   2   3   0]\n",
      " [  0   3   0   5  24   6   9  58   3  11   6  20   8   0  47   7   9   3]\n",
      " [  0   0   0   6  26   4   2  26   0   2  19   0   7   1   3   0 101   8]\n",
      " [  0   0   0   0  10   0   0   0   0   0   0   0   0   0   0   0   0   8]] \n",
      " --------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for chk, report, confusion_mat, y_pred in result:\n",
    "    print(chk, report, '\\n', confusion_mat, '\\n', '--------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e9ef9ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 10,  5, ...,  7,  7,  6])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best: ['c09k_finetuned_bert_512', 'checkpoint-5000']\n",
    "y_pred = result[3][3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ba533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b75dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "545aea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_finetuned_bert/checkpoint-3000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert/checkpoint-7500\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert/checkpoint-3000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert/checkpoint-3000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model2 = BertForSequenceClassification.from_pretrained(\n",
    "    os.path.join('c09k_finetuned_bert', \"checkpoint-3000\"),\n",
    "    return_dict=True, num_labels=18)\n",
    "# load the tokenizer\n",
    "tokenizer2 = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4510dfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# training_args = TrainingArguments(output_dir=\"c09k_finetuned_bert\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"c09k_finetuned_bert2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1257f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = Trainer(\n",
    "    model=model2,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_finetune_dataset['train'],\n",
    "    eval_dataset=tokenized_finetune_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer2,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e81e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "763d9064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_finetuned_bert/checkpoint-3000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert/checkpoint-7500\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_finetuned_bert/checkpoint-3000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at c09k_finetuned_bert/checkpoint-3000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9881\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6180' max='6180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6180/6180 13:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.305800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.107100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-1000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-1000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-1500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-1500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-2000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-2000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-2500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-2500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-3000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-3000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-3500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-3500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-4000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-4000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-4500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-4500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-5000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-5000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-5500\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-5500/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to c09k_finetuned_bert2/checkpoint-6000\n",
      "Configuration saved in c09k_finetuned_bert2/checkpoint-6000/config.json\n",
      "Model weights saved in c09k_finetuned_bert2/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in c09k_finetuned_bert2/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in c09k_finetuned_bert2/checkpoint-6000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6180, training_loss=1.2481813918425426, metrics={'train_runtime': 816.681, 'train_samples_per_second': 120.99, 'train_steps_per_second': 7.567, 'total_flos': 3250217273679360.0, 'train_loss': 1.2481813918425426, 'epoch': 10.0})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model3 = BertForSequenceClassification.from_pretrained(\n",
    "    os.path.join('c09k_finetuned_bert2', \"checkpoint-6000\"),\n",
    "    return_dict=True, num_labels=18)\n",
    "# load the tokenizer\n",
    "tokenizer3 = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"c09k_finetuned_bert2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01, eval_steps=1000, resume_from_checkpoint=os.path.join('c09k_finetuned_bert', \"checkpoint-3000\")\n",
    ")\n",
    "trainer3 = Trainer(\n",
    "    model=model3,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_finetune_dataset['train'],\n",
    "    eval_dataset=tokenized_finetune_dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer3,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer3.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "10ca1d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5203"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eba4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_pretrain",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
