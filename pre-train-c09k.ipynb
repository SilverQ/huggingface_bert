{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10fa50a",
   "metadata": {},
   "source": [
    "### Train BERT from Scratch using Transformers in Python\n",
    "* https://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers==4.18.0 sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "93e678dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "from datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.data import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3ddb6",
   "metadata": {},
   "source": [
    "### Picking a Dataset\n",
    "* sentence tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41eda328",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = load(\"tokenizers/punkt/english.pickle\")\n",
    "extra_abbreviations = [\n",
    "    'RE','re','pat', 'no', 'nos','vol','jan','feb','mar','apr','jun',\n",
    "    'jul','aug','sep','oct','nov','dec','eng','ser','ind','ed','pp',\n",
    "    'e.g','al','T.E.N.S', 'E.M.S','F.E','U.H.T.S.T','degree',\n",
    "    '/gm','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "    'P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "sent_tokenizer._params.abbrev_types.update(extra_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e384c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 김명선 책임의 문장 분리 코드\n",
    "REG_SENT_KO=r'([ㄱ-ㅣ가-힣]+[.]|[\\n]|[:;!?])'\n",
    "REG_SENT_EN=r'([a-zA-Z]+[.]\\s|[\\n]|[:;!?])'\n",
    "\n",
    "def split_sentence(doc, regex):\n",
    "    s = 0\n",
    "    for m in re.finditer(regex, doc):\n",
    "        sent = doc[s:m.end()].strip()\n",
    "        s = m.end()\n",
    "        if not sent:\n",
    "            continue\n",
    "        yield sent\n",
    "\n",
    "    if s < len(doc):\n",
    "        sent = doc[s:].strip()\n",
    "        if sent:\n",
    "            yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "99289490",
   "metadata": {},
   "outputs": [],
   "source": [
    "?unicodedata.normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fbe758c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "kiwee_files = ['data/c09k_0001-1000.xlsx', 'data/c09k_1001-2000.xlsx', 'data/c09k_2001-3000.xlsx', 'data/c09k_3001-3935.xlsx']\n",
    "with open('data/c09k_corpus.txt', 'a') as f:\n",
    "    f.truncate(0)\n",
    "    for i, fn in enumerate(kiwee_files):\n",
    "        tmp = pd.read_excel(fn).fillna('')\n",
    "        # pandas는 비어있는 컬럼의 dtype을 float로 바꿔서 인식한다. 그로 인해 토크나이징 할 데이터가 없으면 오류가 발생되어 fillna를 사용해 모두 텍스트로 인식시키도록 한다\n",
    "        # https://stackoverflow.com/questions/53953286/pandas-read-excel-blanks-in-string-columns-convert-to-floats-converting-via-st\n",
    "        col_text = ['발행번호', '발명의명칭', '요약', '대표청구항', '과제', '해결방안']\n",
    "        tmp = tmp[col_text]\n",
    "        for index, row in tmp.iterrows():\n",
    "    #         print(index, '\\n', row['발명의명칭'], row['요약'], row['대표청구항'], row['과제'], row['해결방안'], '\\n')\n",
    "            for col in col_text[1:]:\n",
    "    #             print('처리중인 데이터:', col, row[col], '\\n')\n",
    "                if row[col].strip() == \"\":\n",
    "                    pass\n",
    "                else:\n",
    "    #                 print(row[col].strip())\n",
    "    #                     row[col] = unicodedata.normalize('NFKC', row[col])\n",
    "                    row[col] = unicodedata.normalize('NFC', row[col])  # 자음과 모음이 깨질 때는 NFC로 변환\n",
    "    #                     row[col] = unicodedata.normalize('NFKD', row[col])\n",
    "                    #     https://blog.naver.com/PostView.nhn?blogId=duswl0319&logNo=221516880642&from=search&redirect=Log&widgetTypeCall=true&directAccess=false\n",
    "                    row[col] = row[col].replace('\\n\\t',' ')\n",
    "                    row[col] = row[col].replace('\\n',' ')\n",
    "    #                 row[col] = row[col].replace('&lt;',' ')\n",
    "                    row[col] = row[col].replace('\\t\\t',' ')\n",
    "                    row[col] = row[col].replace('.  .','.')\n",
    "                    row[col] = row[col].replace('. .','.')\n",
    "                    row[col] = row[col].replace('..','.')\n",
    "                    row[col] = row[col].replace(' . ','.')\n",
    "                    row[col] = row[col].replace(' ． ','.')\n",
    "                    row[col] = row[col].replace('． ','.')\n",
    "                    row[col] = row[col].replace('. ','.')\n",
    "                    row[col] = row[col].replace('  ',' ')\n",
    "                    row[col] = row[col].replace('  ',' ')\n",
    "                    row[col] = row[col].replace('【과제】',' ')\n",
    "                    row[col] = row[col].replace('【해결 수단】',' ')\n",
    "                    str_tmp = sent_tokenizer.tokenize(row[col].strip())\n",
    "    #                 print('문장 분리: ', str_tmp, '\\n'*3)\n",
    "    #                 result  = [f\"{line}\\n\" for line in str_tmp]\n",
    "                    for line in str_tmp:\n",
    "                        f.write(f\"{line}\\n\")\n",
    "#                 data.extend(result)\n",
    "#                 print('누적된 문장수: ', len(data), '\\n')\n",
    "# data1 = np.array(data)\n",
    "# np.savetxt('data/c09k_corpus.txt', data1, fmt='<U1176 %.18e', header='', footer='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a2a5b92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-f402c51ede181d32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/hdh/.cache/huggingface/datasets/text/default-f402c51ede181d32/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 783.54it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1317.72it/s]\n",
      "                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/hdh/.cache/huggingface/datasets/text/default-f402c51ede181d32/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_text('data/c09k_corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6560ac51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이산화티탄-염화리튬 전기변색물질, 이를 이용한전기변색장치용 파우더 및 필름전극의 제조방법', '본 발명은 이산화티탄-염화리튬 전기변색물질 및 이를 이용한 전기변색장치용 파우더 및 필름전극의 제조방법에 관한 것으로, 이산화티탄에 염화리튬이 균일하게 혼합된 물질을 합성하여 제조되며 상기 이산화티탄-염화리튬에서 티탄 대 리튬의 몰 함량 비율이 1 : x (0.5 &lt; x &lt; 2) 범위를 갖는 이산화티탄-염화리튬을 졸-겔 방법으로 합성함으로써 표면적이 극대화되는 다공성화 또는 그 물질 내에 존재하는 높은 함량의 리튬 이온으로 인하여 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 매우 뛰어난 발명인 것이다.', '이산화티탄-염화리튬 파우더에 아세틸아세톤이 포함된 증류수를 첨가하여 끈적거리는 상태가 될 때까지 교반하는 단계; 상기 용액에 증류수와 트리톤 X-100을 첨가하여 완전히 균일한 용액이 될 때까지 다시 교반하는 단계; 및 상기 용액을 전도성 유리인 ITO 위에 얇게 입힌 후, 상온에서 건조한 다음 열처리하는 단계를 포함함을 특징으로 하는 전기변색장치용 이산화티탄-염화리튬 필름전극의 제조방법.', '1@@발명은 상기의 제문제점을 감안하여 안출한 것으로, 본 발명에서는 이산화티탄-염화리튬을 새롭게 합성하여 전기변색 물질로서 실용화함에 그 목적이 있다.', '2@@발명의 다른 목적은 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 이산화티탄-염화리튬 전기변색물질을 제공하는데 있다.']\n",
      "['전기 화학 디바이스 및 그것을 형성하는 방법', '전기 화학 디바이스에 대해서, 개시한다.전기 화학 디바이스는, 제 1의 투명 도전층, 제 1의 투명 도전층 위에 있는 에렉트로 크로믹층, 에렉트로 크로믹층 위에 있는 대향 전극층, 제 2의 투명 도전층을 갖추어 23℃으로 0.68초/mm이하의 스위칭 속도 파라미터를 가진다.', '제 1의 투명 도전층과 전기 제 1의 투명 도전층 위에 있는 음극 전기 화학층과 전기 음극 전기 화학층 위에 있는 양극 전기 화학층과 제 2의 투명 도전층과 (을)를 갖추는 전기 화학 디바이스이며, 전기 전기 화학 디바이스는, 23℃으로 0.68초/mm이하인 제 1의 스위칭 속도 파라미터, －20℃으로 1.0초/mm이하인 제 2의 스위칭 속도 파라미터, 1.5/log(오옴) 미만인 log｜Z｜의 impedance파라미터, 또는 －20℃으로 8%미만인 착색 투과률 파라미터 중 적어도 1개를 가지는, 전기 화학 디바이스.', '이산화 바나듐 함유 입자, 사모 크로믹 필름 및 이산화 바나듐 함유 입자의 제조 방법', '사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.６５〜２.05의 범위내인 원소 A와 폴링의 전기 음성도가 2.１０〜２.54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['text'][0:5])\n",
    "print(dataset['text'][-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5ee72fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n['사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.',\\n '６５〜２． 05의 범위내인 원소 A와 폴링의 전기 음성도가 2.',\\n '１０〜２． 54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\\n 위와 같이 소수점 앞 뒤로 공백이 존재하는데, 이로 인해 소수점에서 문장 분리가 되버린다. 오류 조치가 필요\\n \""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "['사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.',\n",
    " '６５〜２． 05의 범위내인 원소 A와 폴링의 전기 음성도가 2.',\n",
    " '１０〜２． 54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\n",
    " 위와 같이 소수점 앞 뒤로 공백이 존재하는데, 이로 인해 소수점에서 문장 분리가 되버린다. 오류 조치가 필요\n",
    " \"\"\"\n",
    "# 아래와 같이 구두점 앞뒤 블랭크를 제거하여 일단 구분\n",
    "# row[col] = unicodedata.normalize('NFKC', row[col])\n",
    "# https://blog.naver.com/PostView.nhn?blogId=duswl0319&logNo=221516880642&from=search&redirect=Log&widgetTypeCall=true&directAccess=false\n",
    "# row[col] = row[col].replace(' . ','.')\n",
    "# row[col] = row[col].replace(' ． ','.')\n",
    "# row[col] = row[col].replace('． ','.')\n",
    "# row[col] = row[col].replace('. ','.')\n",
    "# row[col] = row[col].replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d13f3e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4636dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과거 데이터 처리\n",
    "data = pd.read_excel('data/c09k_biblio(861건).xlsx')\n",
    "data = data[['발행번호', '발명의명칭', '요약', '대표청구항', '과제', '해결방안']]\n",
    "# data_train, data_test = train_test_split(data, test_size=0.1, random_state=15, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c07b1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_train.shape, data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af338b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_process(data):\n",
    "    data['text'] = data[['발명의명칭', '요약', '대표청구항', '과제', '해결방안']].apply(\". \".join, axis=1)\n",
    "    data['text'] = data['text'].str.replace('\\n\\t',' ')\n",
    "    data['text'] = data['text'].str.replace('\\t\\t',' ')\n",
    "    data['text'] = data['text'].str.replace('.  .','.')\n",
    "    data['text'] = data['text'].str.replace('. .','.')\n",
    "    data['text'] = data['text'].str.replace('..','.')\n",
    "    data['text'] = data['text'].str.replace('  ',' ')\n",
    "    data['text'] = data['text'].str.strip()\n",
    "    data1 = data[['발행번호', 'text']].copy()\n",
    "#     data1.set_index('발행번호', inplace=True)\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89979a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_file=open('./input.txt','r')\n",
    "save_file=open('./output.txt','w')\n",
    "no_blank = False\n",
    "while True:\n",
    "    line = load_file.readline()\n",
    "    if line == \"\":\n",
    "        break\n",
    "    if line.strip() == \"\":\n",
    "        if no_blank:\n",
    "            continue\n",
    "        save_file.write(f\"{line}\")\n",
    "    else:\n",
    "        print(line)\n",
    "        result_ = tokenizer.tokenize(line)\n",
    "        print(result_)\n",
    "        result  = [ f\"{cur_line}\\n\" for cur_line in result_ ]\n",
    "        for save_line in result:\n",
    "            save_file.write(save_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c773cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1638/1848036980.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace('.  .','.')\n",
      "/tmp/ipykernel_1638/1848036980.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace('. .','.')\n",
      "/tmp/ipykernel_1638/1848036980.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace('..','.')\n"
     ]
    }
   ],
   "source": [
    "data_text = data_pre_process(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddcd622c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1638/1848036980.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace('.  .','.')\n",
      "/tmp/ipykernel_1638/1848036980.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace('. .','.')\n",
      "/tmp/ipykernel_1638/1848036980.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['text'] = data['text'].str.replace('..','.')\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(data_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b2b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2272b29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "복합 재료, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치. 유기 화합물과 무기 화합물을 복합한 복합 재료로서, 캐리어 수소성이 높은 복합 재료를 제공한다. 또한, 유기 화합물로의 캐리어 주입성이 높은 복합 재료를 제공한다. 또한, 전하 이동 상호 작용에 의한 흡수가 발생하기 어려운 복합 재료를 제공한다. 또한, 가시광에 대한 투광성이 높은 복합 재료를 제공한다. \n",
      "나프탈렌 골격, 페난트렌 골격, 또는 트리페닐렌 골격에 치환기가 결합한, 분자량이 350 이상 2000 이하인 탄화수소 화합물과 당해 탄화수소 화합물에 대해 전자 수용성을 나타내는 무기 화합물을 함유하고, 당해 치환기가 갖는 환은, 벤젠 환, 나프탈렌 환, 페난트렌 환, 트리페닐렌 환으로부터 선택되는 1종 또는 복수종인 복합 재료를 제공한다.  . 비페닐기를 갖는 탄화수소 화합물; 및억셉터성 물질 (acceptor substance)을 포함하는, 복합 재료로서,상기 비페닐기는 나프탈렌 골격, 페난트렌 골격, 및 트리페닐렌 골격으로부터 선택되는 복수의 치환기에 결합되고,상기 탄화수소 화합물의 분자량은 350 이상 2000 이하인, 복합 재료.. 1. 발명의 일 형태의 복합 재료인 경우에는, 결정화를 억제할 목적으로 무기 화합물의 비율을 높게 할 필요가 없으며, 전하 이동 상호 작용에 유래하는 흡수 피크가 가시광 영역에 관측되는 것을 방지할 수 있다.. 1. 비페닐기를 갖는 탄화수소 화합물; 및억셉터성 물질 (acceptor substance)을 포함하는, 복합 재료로서,상기 비페닐기는 나프탈렌 골격, 페난트렌 골격, 및 트리페닐렌 골격으로부터 선택되는 복수의 치환기에 결합되고,상기 탄화수소 화합물의 분자량은 350 이상 2000 이하인, 복합 재료.\n",
      "==================================================\n",
      "다환 화합물 및 이를 포함하는 유기 발광 소자. 본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다. . 하기 화학식 1로 표시되는 화합물:[화학식 1] 상기 화학식 1에 있어서,Ar1 내지 Ar4는 서로 같거나 상이하고, 각각 독립적으로 수소; 중수소; 탄소수 1 내지 20의 알킬기; 또는 탄소수 6 내지 30의 아릴기이거나, 인접한 기는 서로 결합하여 지방족 탄화수소 고리 또는 방향족 탄화수소 고리가 축합되거나 비축합된 6원의 지방족 탄화수소 고리를 형성하고,A1 및 A2는 서로 같거나 상이하고, 각각 독립적으로 수소; 중수소; 탄소수 1 내지 20의 알킬기; 또는 탄소수 1 내지 20의 알킬기로 치환 또는 비치환된 탄소수 6 내지 30의 아릴기이거나, 서로 결합하여 중수소, 탄소수 1 내지 20의 트리알킬실릴기, 탄소수 6 내지 30의 트리아릴실릴기, 탄소수 1 내지 20의 알킬기 및 탄소수 6 내지 30의 아릴기로 이루어진 군으로부터 선택되는 1 이상의 치환기로 치환 또는 비치환되고, 지방족 탄화수소 고리 또는 방향족 탄화수소 고리가 축합 또는 비축합된 5원의 고리를 형성하고,R1 및 R2는 서로 같거나 상이하고, 각각 독립적으로 수소; 중수소; 탄소수 1 내지 20의 트리알킬실릴기; 탄소수 6 내지 30의 트리아릴실릴기; 탄소수 1 내지 10의 알킬기; 또는 탄소수 6 내지 30의 아릴기이고,R3는 탄소수 1 내지 10의 알킬기 또는 탄소수 6 내지 30의 아릴기로 치환 또는 비치환된 디하이드로아크리딘기; 탄소수 1 내지 10의 알킬기 또는 탄소수 6 내지 30의 아릴기로 치환 또는 비치환된 디하이드로디벤조아자실린기; 스피로(디벤조실롤-디벤조아자실린)기; 스피로(아크리딘-플루오렌)기; 또는 탄소수 1 내지 10의 알킬기 또는 탄소수 6 내지 30의 아릴기로 치환 또는 비치환되고, 벤젠고리가 축합 또는 비축합된 헥사하이드로카바졸기이고,n3는 1 이고,n1 및 n2는 각각 0 내지 3의 정수이고, n1 및 n2가 각각 2 이상인 경우 복수 개의 괄호 내의 치환기는 서로 같거나 상이하다.. 1. \t본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다. . 1. 발명에 따른 유기 발광 소자는 제1 전극; 상기 제1 전극과 대향하여 구비되는 제2 전극; 및 상기 제1 전극과 상기 제2 전극 사이에 구비되는 1층 이상의 유기물층을 포함하는 유기 발광 소자로서, 상기 유기물층 중 1층 이상은 상기 전술한 화합물을 포함하는 것을 특징으로 한다.\n",
      "==================================================\n",
      "유기 전계 발광 소자 및 유기 전계 발광 소자용 다환 화합물. 일 실시예의 유기 전계 발광 소자는 제1 전극, 제2 전극, 및 제1 전극과 제2 전극 사이에 배치되고 다환 화합물을 포함하는 발광층을 포함하고, 다환 화합물은 보란아민 유도체, 보란아민 유도체의 붕소(Boron) 원자에 직접 결합된 제1 고리와 제2 고리, 보란아민 유도체의 질소(Nitrogen) 원자에 직접 결합된 제3 고리와 제4 고리, 및 제1 내지 제4 고리 중 적어도 하나의 고리에 결합된 적어도 하나의 치환 또는 비치환된 보릴기를 포함하여 장수명 특성 및 우수한 색재현성을 나타낼 수 있다. . 제1 전극;상기 제1 전극 상에 배치된 제2 전극; 및상기 제1 전극과 상기 제2 전극 사이에 배치되고, 다환 화합물을 포함하는 발광층; 을 포함하고,상기 다환 화합물은 보란아민(boranamine) 유도체; 상기 보란아민 유도체의 붕소(Boron) 원자에 직접 결합된 제1 고리와 제2 고리; 상기 보란아민 유도체의 질소(Nitrogen) 원자에 직접 결합된 제3 고리와 제4 고리; 및 상기 제1 내지 제4 고리 중 적어도 하나의 고리에 결합된 적어도 하나의 치환 또는 비치환된 보릴기; 를 포함하는 유기 전계 발광 소자.. 1. 발명의 목적은 양호한 수명 특성과 우수한 발광 효율을 나타내는 유기 전계 발광 소자를 제공하는 것이다.\n",
      "2. 발명의 다른 목적은 높은 색순도 및 장수명 특성을 갖는 유기 전계 발광 소자용 재료인 다환 화합물을 제공하는 것이다.. 1. 제1 전극;상기 제1 전극 상에 배치된 제2 전극; 및상기 제1 전극과 상기 제2 전극 사이에 배치되고, 다환 화합물을 포함하는 발광층; 을 포함하고,상기 다환 화합물은 보란아민(boranamine) 유도체; 상기 보란아민 유도체의 붕소(Boron) 원자에 직접 결합된 제1 고리와 제2 고리; 상기 보란아민 유도체의 질소(Nitrogen) 원자에 직접 결합된 제3 고리와 제4 고리; 및 상기 제1 내지 제4 고리 중 적어도 하나의 고리에 결합된 적어도 하나의 치환 또는 비치환된 보릴기; 를 포함하는 유기 전계 발광 소자.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "d = dataset.train_test_split(test_size=0.1)\n",
    "d[\"train\"], d[\"test\"]\n",
    "for t in d[\"train\"][\"text\"][:3]:\n",
    "    print(t)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a79640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train1 = data_pre_process(data_train)\n",
    "# data_test1 = data_pre_process(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a04f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download and prepare cc_news dataset\n",
    "# dataset = load_dataset(\"cc_news\", split=\"train\")\n",
    "# dataset[0]\n",
    "# # split the dataset into training (90%) and testing (10%)\n",
    "# d = dataset.train_test_split(test_size=0.1)\n",
    "# d[\"train\"], d[\"test\"]\n",
    "# for t in d[\"train\"][\"text\"][:3]:\n",
    "#     print(t)\n",
    "#     print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f583b25",
   "metadata": {},
   "source": [
    "### using personal dataset\n",
    "* https://huggingface.co/docs/datasets/dataset_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you have huge custom dataset separated into files\n",
    "# # load the splitted files\n",
    "# files = [\"train1.txt\", \"train2.txt\"] # train3.txt, etc.\n",
    "# dataset = load_dataset(\"text\", data_files=files, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221e15e",
   "metadata": {},
   "source": [
    "### Training the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06e75208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train the tokenizer from scratch (especially if you have custom\n",
    "# dataset loaded as datasets object), then run this cell to save it as files\n",
    "# but if you already have your custom data as text files, there is no point using this\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    \"\"\"Utility function to save dataset text to disk,\n",
    "    useful for using the texts to train the tokenizer \n",
    "    (as the tokenizer accepts files)\"\"\"\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2e2453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_to_text(ds, \"c09k_pre_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b91d375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_to_text(d[\"train\"], \"c09k_pre_train.txt\")\n",
    "dataset_to_text(d[\"test\"], \"c09k_pre_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "712352f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the training set to train.txt\n",
    "# dataset_to_text(d[\"train\"], \"train.txt\")\n",
    "# # save the testing set to test.txt\n",
    "# dataset_to_text(d[\"test\"], \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cc64d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# if you want to train the tokenizer on both sets\n",
    "# files = [\"train.txt\", \"test.txt\"]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"c09k_pre_train.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 15072\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37aa7caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the WordPiece tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "# train the tokenizer\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "# enable truncation up to the maximum 512 tokens\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef9182b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"c09k_pretrained_bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5d3f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the directory if not already there\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "# save the tokenizer  \n",
    "tokenizer.save_model(model_path)\n",
    "tokenizer.save('c09k_pretrained_bert/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b4658e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping some of the tokenizer config to config file, \n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "    tokenizer_cfg = {\"do_lower_case\": True,\n",
    "                     \"unk_token\": \"[UNK]\",\n",
    "                     \"sep_token\": \"[SEP]\",\n",
    "                     \"pad_token\": \"[PAD]\",\n",
    "                     \"cls_token\": \"[CLS]\",\n",
    "                     \"mask_token\": \"[MASK]\",\n",
    "                     \"model_max_length\": max_length,\n",
    "                     \"max_len\": max_length,\n",
    "                    }\n",
    "    json.dump(tokenizer_cfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81bd4e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# when the tokenizer is trained and configured, load it as BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af37bfc",
   "metadata": {},
   "source": [
    "### Tokenizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "820274b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                                     max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9526a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "635c6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizing the train dataset\n",
    "# train_dataset = data_train1['text'].map(encode, batched=True)\n",
    "# test_dataset = data_test1['text'].map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1791637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.31ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 24.07ba/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the train dataset\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_dataset = d[\"test\"].map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ecd7d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'발행번호': ['KR10002269040B1', 'KR10002318136B1'], 'text': ['복합 재료, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치. 유기 화합물과 무기 화합물을 복합한 복합 재료로서, 캐리어 수소성이 높은 복합 재료를 제공한다. 또한, 유기 화합물로의 캐리어 주입성이 높은 복합 재료를 제공한다. 또한, 전하 이동 상호 작용에 의한 흡수가 발생하기 어려운 복합 재료를 제공한다. 또한, 가시광에 대한 투광성이 높은 복합 재료를 제공한다. \\n나프탈렌 골격, 페난트렌 골격, 또는 트리페닐렌 골격에 치환기가 결합한, 분자량이 350 이상 2000 이하인 탄화수소 화합물과 당해 탄화수소 화합물에 대해 전자 수용성을 나타내는 무기 화합물을 함유하고, 당해 치환기가 갖는 환은, 벤젠 환, 나프탈렌 환, 페난트렌 환, 트리페닐렌 환으로부터 선택되는 1종 또는 복수종인 복합 재료를 제공한다.  . 비페닐기를 갖는 탄화수소 화합물; 및억셉터성 물질 (acceptor substance)을 포함하는, 복합 재료로서,상기 비페닐기는 나프탈렌 골격, 페난트렌 골격, 및 트리페닐렌 골격으로부터 선택되는 복수의 치환기에 결합되고,상기 탄화수소 화합물의 분자량은 350 이상 2000 이하인, 복합 재료.. 1. 발명의 일 형태의 복합 재료인 경우에는, 결정화를 억제할 목적으로 무기 화합물의 비율을 높게 할 필요가 없으며, 전하 이동 상호 작용에 유래하는 흡수 피크가 가시광 영역에 관측되는 것을 방지할 수 있다.. 1. 비페닐기를 갖는 탄화수소 화합물; 및억셉터성 물질 (acceptor substance)을 포함하는, 복합 재료로서,상기 비페닐기는 나프탈렌 골격, 페난트렌 골격, 및 트리페닐렌 골격으로부터 선택되는 복수의 치환기에 결합되고,상기 탄화수소 화합물의 분자량은 350 이상 2000 이하인, 복합 재료.', '다환 화합물 및 이를 포함하는 유기 발광 소자. 본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다. . 하기 화학식 1로 표시되는 화합물:[화학식 1] 상기 화학식 1에 있어서,Ar1 내지 Ar4는 서로 같거나 상이하고, 각각 독립적으로 수소; 중수소; 탄소수 1 내지 20의 알킬기; 또는 탄소수 6 내지 30의 아릴기이거나, 인접한 기는 서로 결합하여 지방족 탄화수소 고리 또는 방향족 탄화수소 고리가 축합되거나 비축합된 6원의 지방족 탄화수소 고리를 형성하고,A1 및 A2는 서로 같거나 상이하고, 각각 독립적으로 수소; 중수소; 탄소수 1 내지 20의 알킬기; 또는 탄소수 1 내지 20의 알킬기로 치환 또는 비치환된 탄소수 6 내지 30의 아릴기이거나, 서로 결합하여 중수소, 탄소수 1 내지 20의 트리알킬실릴기, 탄소수 6 내지 30의 트리아릴실릴기, 탄소수 1 내지 20의 알킬기 및 탄소수 6 내지 30의 아릴기로 이루어진 군으로부터 선택되는 1 이상의 치환기로 치환 또는 비치환되고, 지방족 탄화수소 고리 또는 방향족 탄화수소 고리가 축합 또는 비축합된 5원의 고리를 형성하고,R1 및 R2는 서로 같거나 상이하고, 각각 독립적으로 수소; 중수소; 탄소수 1 내지 20의 트리알킬실릴기; 탄소수 6 내지 30의 트리아릴실릴기; 탄소수 1 내지 10의 알킬기; 또는 탄소수 6 내지 30의 아릴기이고,R3는 탄소수 1 내지 10의 알킬기 또는 탄소수 6 내지 30의 아릴기로 치환 또는 비치환된 디하이드로아크리딘기; 탄소수 1 내지 10의 알킬기 또는 탄소수 6 내지 30의 아릴기로 치환 또는 비치환된 디하이드로디벤조아자실린기; 스피로(디벤조실롤-디벤조아자실린)기; 스피로(아크리딘-플루오렌)기; 또는 탄소수 1 내지 10의 알킬기 또는 탄소수 6 내지 30의 아릴기로 치환 또는 비치환되고, 벤젠고리가 축합 또는 비축합된 헥사하이드로카바졸기이고,n3는 1 이고,n1 및 n2는 각각 0 내지 3의 정수이고, n1 및 n2가 각각 2 이상인 경우 복수 개의 괄호 내의 치환기는 서로 같거나 상이하다.. 1. \\t본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다. . 1. 발명에 따른 유기 발광 소자는 제1 전극; 상기 제1 전극과 대향하여 구비되는 제2 전극; 및 상기 제1 전극과 상기 제2 전극 사이에 구비되는 1층 이상의 유기물층을 포함하는 유기 발광 소자로서, 상기 유기물층 중 1층 이상은 상기 전술한 화합물을 포함하는 것을 특징으로 한다.']} \n",
      " {'발행번호': ['KR10002269040B1', 'KR10002318136B1'], 'text': ['복합 재료, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치. 유기 화합물과 무기 화합물을 복합한 복합 재료로서, 캐리어 수소성이 높은 복합 재료를 제공한다. 또한, 유기 화합물로의 캐리어 주입성이 높은 복합 재료를 제공한다. 또한, 전하 이동 상호 작용에 의한 흡수가 발생하기 어려운 복합 재료를 제공한다. 또한, 가시광에 대한 투광성이 높은 복합 재료를 제공한다. \\n나프탈렌 골격, 페난트렌 골격, 또는 트리페닐렌 골격에 치환기가 결합한, 분자량이 350 이상 2000 이하인 탄화수소 화합물과 당해 탄화수소 화합물에 대해 전자 수용성을 나타내는 무기 화합물을 함유하고, 당해 치환기가 갖는 환은, 벤젠 환, 나프탈렌 환, 페난트렌 환, 트리페닐렌 환으로부터 선택되는 1종 또는 복수종인 복합 재료를 제공한다.  . 비페닐기를 갖는 탄화수소 화합물; 및억셉터성 물질 (acceptor substance)을 포함하는, 복합 재료로서,상기 비페닐기는 나프탈렌 골격, 페난트렌 골격, 및 트리페닐렌 골격으로부터 선택되는 복수의 치환기에 결합되고,상기 탄화수소 화합물의 분자량은 350 이상 2000 이하인, 복합 재료.. 1. 발명의 일 형태의 복합 재료인 경우에는, 결정화를 억제할 목적으로 무기 화합물의 비율을 높게 할 필요가 없으며, 전하 이동 상호 작용에 유래하는 흡수 피크가 가시광 영역에 관측되는 것을 방지할 수 있다.. 1. 비페닐기를 갖는 탄화수소 화합물; 및억셉터성 물질 (acceptor substance)을 포함하는, 복합 재료로서,상기 비페닐기는 나프탈렌 골격, 페난트렌 골격, 및 트리페닐렌 골격으로부터 선택되는 복수의 치환기에 결합되고,상기 탄화수소 화합물의 분자량은 350 이상 2000 이하인, 복합 재료.', '다환 화합물 및 이를 포함하는 유기 발광 소자. 본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다. . 하기 화학식 1로 표시되는 화합물:[화학식 1] 상기 화학식 1에 있어서,Ar1 내지 Ar4는 서로 같거나 상이하고, 각각 독립적으로 수소; 중수소; 탄소수 1 내지 20의 알킬기; 또는 탄소수 6 내지 30의 아릴기이거나, 인접한 기는 서로 결합하여 지방족 탄화수소 고리 또는 방향족 탄화수소 고리가 축합되거나 비축합된 6원의 지방족 탄화수소 고리를 형성하고,A1 및 A2는 서로 같거나 상이하고, 각각 독립적으로 수소; 중수소; 탄소수 1 내지 20의 알킬기; 또는 탄소수 1 내지 20의 알킬기로 치환 또는 비치환된 탄소수 6 내지 30의 아릴기이거나, 서로 결합하여 중수소, 탄소수 1 내지 20의 트리알킬실릴기, 탄소수 6 내지 30의 트리아릴실릴기, 탄소수 1 내지 20의 알킬기 및 탄소수 6 내지 30의 아릴기로 이루어진 군으로부터 선택되는 1 이상의 치환기로 치환 또는 비치환되고, 지방족 탄화수소 고리 또는 방향족 탄화수소 고리가 축합 또는 비축합된 5원의 고리를 형성하고,R1 및 R2는 서로 같거나 상이하고, 각각 독립적으로 수소; 중수소; 탄소수 1 내지 20의 트리알킬실릴기; 탄소수 6 내지 30의 트리아릴실릴기; 탄소수 1 내지 10의 알킬기; 또는 탄소수 6 내지 30의 아릴기이고,R3는 탄소수 1 내지 10의 알킬기 또는 탄소수 6 내지 30의 아릴기로 치환 또는 비치환된 디하이드로아크리딘기; 탄소수 1 내지 10의 알킬기 또는 탄소수 6 내지 30의 아릴기로 치환 또는 비치환된 디하이드로디벤조아자실린기; 스피로(디벤조실롤-디벤조아자실린)기; 스피로(아크리딘-플루오렌)기; 또는 탄소수 1 내지 10의 알킬기 또는 탄소수 6 내지 30의 아릴기로 치환 또는 비치환되고, 벤젠고리가 축합 또는 비축합된 헥사하이드로카바졸기이고,n3는 1 이고,n1 및 n2는 각각 0 내지 3의 정수이고, n1 및 n2가 각각 2 이상인 경우 복수 개의 괄호 내의 치환기는 서로 같거나 상이하다.. 1. \\t본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다. . 1. 발명에 따른 유기 발광 소자는 제1 전극; 상기 제1 전극과 대향하여 구비되는 제2 전극; 및 상기 제1 전극과 상기 제2 전극 사이에 구비되는 1층 이상의 유기물층을 포함하는 유기 발광 소자로서, 상기 유기물층 중 1층 이상은 상기 전술한 화합물을 포함하는 것을 특징으로 한다.'], 'input_ids': [[2579, 702, 16, 396, 499, 16, 396, 940, 16, 670, 1882, 16, 335, 1604, 940, 18, 379, 1721, 5162, 565, 13268, 2579, 2098, 16, 4359, 11885, 1122, 2579, 1491, 685, 18, 928, 16, 379, 13043, 4359, 13717, 1122, 2579, 1491, 685, 18, 928, 16, 1924, 3610, 3930, 8981, 2175, 12912, 9146, 7569, 2579, 1491, 685, 18, 928, 16, 13372, 1207, 13076, 1122, 2579, 1491, 685, 18, 1872, 1537, 16, 3390, 1537, 16, 312, 1718, 7338, 1612, 4648, 16, 6370, 5276, 437, 7183, 3455, 813, 1721, 2617, 813, 1587, 2580, 670, 13266, 1467, 5162, 565, 4090, 16, 2617, 1612, 566, 12536, 16, 1429, 1168, 16, 1872, 1168, 16, 3390, 1168, 16, 1718, 8919, 701, 1208, 312, 14505, 2579, 1491, 685, 18, 18, 8984, 566, 813, 387, 31, 9786, 852, 12, 7736, 9770, 13, 1204, 445, 16, 2579, 2098, 16, 366, 12714, 1872, 1537, 16, 3390, 1537, 16, 335, 1718, 9015, 701, 1268, 6629, 2573, 16, 366, 813, 1241, 9383, 5276, 437, 7183, 3455, 16, 2579, 702, 18, 18, 21, 18, 629, 615, 4646, 2579, 5399, 3558, 16, 13933, 5854, 1378, 5162, 1241, 13639, 7969, 538, 10826, 4268, 16, 1924, 3610, 3930, 8981, 14644, 1745, 7457, 2883, 4698, 14491, 651, 7087, 383, 717, 18, 18, 21, 18, 8984, 566, 813, 387, 31, 9786, 852, 12, 7736, 9770, 13, 1204, 445, 16, 2579, 2098, 16, 366, 12714, 1872, 1537, 16, 3390, 1537, 16, 335, 1718, 9015, 701, 1268, 6629, 2573, 16, 366, 813, 1241, 9383, 5276, 437, 7183, 3455, 16, 2579, 702, 18], [746, 387, 335, 556, 445, 379, 396, 499, 18, 541, 1143, 369, 628, 512, 387, 335, 556, 445, 379, 396, 840, 623, 593, 18, 18, 484, 369, 628, 512, 387, 30, 36, 369, 21, 38, 366, 369, 1054, 692, 16, 795, 349, 1940, 435, 676, 582, 16, 411, 421, 474, 31, 483, 31, 426, 21, 349, 1163, 539, 31, 312, 426, 26, 349, 518, 2863, 16, 845, 1158, 435, 727, 969, 813, 448, 312, 467, 813, 3099, 888, 1377, 11027, 10455, 969, 813, 620, 982, 16, 848, 335, 1798, 435, 676, 582, 16, 411, 421, 474, 31, 483, 31, 426, 21, 349, 1163, 539, 31, 312, 426, 21, 349, 1163, 2005, 317, 312, 336, 426, 26, 349, 518, 2863, 16, 435, 727, 483, 16, 426, 21, 349, 1163, 6424, 16, 426, 26, 349, 518, 6425, 16, 426, 21, 349, 1163, 539, 335, 426, 26, 349, 518, 1566, 584, 647, 701, 21, 523, 1072, 317, 312, 1855, 16, 969, 813, 448, 312, 467, 813, 3099, 888, 312, 11027, 1504, 309, 620, 982, 16, 429, 335, 925, 435, 676, 582, 16, 411, 421, 474, 31, 483, 31, 426, 21, 349, 1163, 6424, 31, 426, 26, 349, 518, 6425, 31, 426, 21, 349, 1270, 539, 31, 312, 426, 26, 349, 518, 1767, 16, 1623, 426, 21, 349, 1270, 539, 312, 426, 26, 349, 518, 1566, 317, 312, 336, 10744, 31, 426, 21, 349, 1270, 539, 312, 426, 26, 349, 518, 1566, 317, 312, 336, 3110, 5383, 8619, 300, 31, 5454, 12, 816, 4176, 2088, 17, 8777, 13, 502, 31, 5454, 12, 5135, 17, 1357, 13, 502, 31, 312, 426, 21, 349, 1270, 539, 312, 426, 26, 349, 518, 1566, 317, 312, 1855, 16, 5467, 431, 888, 312, 11027, 8189, 5826, 403, 16, 4422, 21, 934, 16, 1217, 335, 1963, 411, 20, 349, 884, 657, 16, 1217, 335, 2933, 411, 22, 917, 542, 859, 695, 1678, 1435, 1060, 435, 676, 1463, 18, 18, 21, 18, 541, 1143, 369, 628, 512, 387, 335, 556, 445, 379, 396, 840, 623, 593, 18, 18, 21, 18, 1003, 929, 379, 396, 1731, 588, 698, 31, 366, 588, 1351, 2347, 2344, 663, 698, 31, 335, 366, 588, 1351, 366, 663, 698, 806, 2344, 1383, 523, 1472, 445, 379, 396, 2155, 16, 366, 1372, 376, 1383, 1113, 366, 1988, 565, 445, 651, 828, 775, 18]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'special_tokens_mask': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(d[\"train\"][:2], '\\n', train_dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e884ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "if truncate_longer_samples:\n",
    "    # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    # remove other columns, and remain them as Python lists\n",
    "    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb19d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ba5fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a0a6c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# https://pytorch.org/get-started/locally/#windows-package-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d21bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 512: 100%|██████████| 1/1 [00:00<00:00,  2.13ba/s]\n",
      "Grouping texts in chunks of 512: 100%|██████████| 1/1 [00:00<00:00, 16.24ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "if not truncate_longer_samples:\n",
    "    train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "                                                                        desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    # convert them from lists to torch tensors\n",
    "    train_dataset.set_format(\"torch\")\n",
    "    test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38b171d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1049, 135)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42187cb",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcc3326b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15072"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c302756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model with the config\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd61aca",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e84fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "091a0983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 10\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=10,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=10, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=8,  # evaluation batch size\n",
    "    logging_steps=10,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=10,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb0c3d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c09k_pretrained_bert'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fec125d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the trainer and pass everything to it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34b95bbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1049\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 80\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 130\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [130/130 11:37, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>8.463800</td>\n",
       "      <td>7.923255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.833800</td>\n",
       "      <td>7.537686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.440900</td>\n",
       "      <td>7.227184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.128600</td>\n",
       "      <td>6.954088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.805100</td>\n",
       "      <td>6.723242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.616200</td>\n",
       "      <td>6.485281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>6.443300</td>\n",
       "      <td>6.311793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>6.321800</td>\n",
       "      <td>6.222284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6.129300</td>\n",
       "      <td>6.170388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.135000</td>\n",
       "      <td>6.089207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>6.078100</td>\n",
       "      <td>6.077502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.032700</td>\n",
       "      <td>6.055802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>5.959000</td>\n",
       "      <td>6.038351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-10\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-10/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-10/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-20\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-20/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-20/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-30\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-30/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-30/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-40\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-40/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-40/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-50\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-50/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-50/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-60\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-60/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-60/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-70\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-70/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-70/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-80\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-80/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-80/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-90\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-90/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-90/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-100\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-100/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-100/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-110\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-110/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-110/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-120\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-120/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-120/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-130\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-130/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-130/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=130, training_loss=6.722118172278771, metrics={'train_runtime': 702.5491, 'train_samples_per_second': 14.931, 'train_steps_per_second': 0.185, 'total_flos': 2758152189542400.0, 'train_loss': 6.722118172278771, 'epoch': 9.99})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c426bb",
   "metadata": {},
   "source": [
    "### Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "491d7a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_pretrained_bert/checkpoint-130/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 15072\n",
      "}\n",
      "\n",
      "loading weights file c09k_pretrained_bert/checkpoint-130/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at c09k_pretrained_bert/checkpoint-130.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-130\"))\n",
    "# load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d2e5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f44b9985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인광성 유기 금속 이리듐 착체,, 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.29707568883895874\n",
      "인광성 유기 금속 이리듐 착체, - 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.016033636406064034\n",
      "인광성 유기 금속 이리듐 착체,. 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.015503378584980965\n",
      "인광성 유기 금속 이리듐 착체, 또는 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.01475199218839407\n",
      "인광성 유기 금속 이리듐 착체, 및 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.012600366026163101\n",
      "==================================================\n",
      "본 명세서는 화학식 1로 표시되는. 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.03568422794342041\n",
      "본 명세서는 화학식 1로 표시되는, 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.03509819507598877\n",
      "본 명세서는 화학식 1로 표시되는 ; 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.031512174755334854\n",
      "본 명세서는 화학식 1로 표시되는 또는 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.019853292033076286\n",
      "본 명세서는 화학식 1로 표시되는 및 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.012950831092894077\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "# 인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치\n",
    "# 본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다\n",
    "examples = [\n",
    "    \"인광성 유기 금속 이리듐 착체, [MASK] 소자, 발광 장치, 전자 기기, 및 조명 장치\",\n",
    "    \"본 명세서는 화학식 1로 표시되는 [MASK] 및 이를 포함하는 유기 발광 소자에 관한 것이다\",\n",
    "]\n",
    "for example in examples:\n",
    "    for prediction in fill_mask(example):\n",
    "        print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c3a7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_pretrain",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
