{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8562982",
   "metadata": {},
   "source": [
    "### Train BERT from Scratch using Transformers in Python\n",
    "* https://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df59dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers==4.18.0 sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d26f496",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/huggingface_bert/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "from tokenizers import *\n",
    "import os\n",
    "import json\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb19a0b3",
   "metadata": {},
   "source": [
    "### Picking a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "514fac54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cc_news (/home/hdh/.cache/huggingface/datasets/cc_news/plain_text/1.0.0/ae469e556251e6e7e20a789f93803c7de19d0c4311b6854ab072fecb4e401bd6)\n"
     ]
    }
   ],
   "source": [
    "# download and prepare cc_news dataset\n",
    "dataset = load_dataset(\"cc_news\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76af3b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url'],\n",
       "     num_rows: 637416\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url'],\n",
       "     num_rows: 70825\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the dataset into training (90%) and testing (10%)\n",
    "d = dataset.train_test_split(test_size=0.1)\n",
    "d[\"train\"], d[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabe197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kings XI Punjab looked all set to stage a dramatic comeback against defending champions Sunrisers Hyderabadâ€™s total of 159/6, before the hosts reeled them with their display of superior bowling might that relied on Bhuvneshwar Kumar (5/19) to edge out a nerve-wracking five run victory with just two balls to spare on Monday.\n",
      "Earlier in the day, both Sunrisers Hyderabad and Kings XI Punjab set out with a point to prove, having lost their two preceding games, as they took on each other. However, with their respective defeats coinciding with them playing away matches, it was KXIP, who were slightly under more pressure to set aside their disappointing run.\n",
      "Stinginess of Punjabâ€™s bowlers\n",
      "Image Credit: Ron Gaunt/ Sportzpics/ IPL\n",
      "KXIPâ€™s skipper Glenn Maxwell counted on his bowlers to substantiate his decision to bowl first after winning the toss. His three-man pace bowling tandem of Sharmaâ€™s â€“ Sandeep, Ishant and Mohit â€“ then more than lived up to their task of choking the Sunrisersâ€™ run-rate by keeping their openers, David Warner and Shikhar Dhawan on the backfoot.\n",
      "Punjab conceded just 29 runs in the powerplay, with Mohit Sharma picking the vital wicket of Dhawan, who frustrated by the drying up of runs was caught behind by the wicketkeeper Wriddhiman Saha trying to outsmart a bouncer.\n",
      "The lack of runs during the powerplay shifted the balance of the match in favour of Punjab, who continued to keep a leash on the batsmen giving them no room whatsoever to even take the slightest of risks.\n",
      "The introduction of Axar Patel, then compounded the Sunrisersâ€™ batting woes. Not only with the deficiency of runs continuing, but also with him striking through the batting order â€“ picking Moises Henriques and Yuvraj Singhâ€™s wickets in consecutive balls â€“ leaving the Sunrisers floundering with three wickets down.\n",
      "Captain David Warner to the rescue\n",
      "Our skipper loves it against KXIP, given his track record. A great way to rise to the occasion and deliver. Kaboom! #OrangeArmy #SRHvKXIP pic.twitter.com/sRXjONHKcW â€” SunRisers Hyderabad (@SunRisers) April 17, 2017\n",
      "Stranded as they were at 58/3 after the 10th over, it was the Sunrisersâ€™ skipper Warner, who then stepped in to try and save the day for the team. Unlike his former partners, who had made their way back into the dugout after trying too hard, Warner opted to go back to the basics in rotating the strike, weaving the boundaries and occasional sixes around them.\n",
      "Warner then found a steady partner in keeper-batsman Naman Ojha at the other end. The two posted a 50-run stand for the fourth wicket even as they took the teamâ€™s total past the 100-run mark and helped push the run-rate to over seven runs per over.\n",
      "Ojhaâ€™s wicket in the 16th over snapped the partnership, but Warner continued to maintain his grip on the match, bringing up his fifth half-century on the trot against Punjab. Leading from the front, the 30-year-old left the field with an unbeaten 70 off 54 balls that included just two sixes and seven fours, pulling up the Sunrisers to a respectable 159/6 in their assigned 20 overs.\n",
      "Taking mentor VVS Laxmanâ€™s words to heart\n",
      "5 for 19. What an outstanding spell by our swing king #Bhuvi. You changed the game. ðŸ‘Š#OrangeArmy #RiseOfOrange #SRHvKXIP â€” SunRisers Hyderabad (@SunRisers) April 17, 2017\n",
      "Looking at the way Warner guided the Sunrisers along, team mentor VVS Laxman seemed to be in a relaxed and confident mood as he quipped about the hosts posting a winning total, during the innings break.\n",
      "As purple cap owner Bhuvneshwar Kumar scalped Hashim Amlaâ€™s wicket in the first ball of the first over, before bringing down curtains on Maxwellâ€™s outing, Laxmanâ€™s words resonated like a prediction.\n",
      "Punjab, however, looked to have gained the upperhand as two expensive overs off IPL debutant Mohammad Nabi, and fan favourite Rashid Khan, in the powerplay, allowed the team to cross the 50-run mark. It wasnâ€™t, however, long before the Afghani duo stormed back into the match in their second spell, compensating for their first over costliness.\n",
      "Nabi struck first, by castling Eoin Morgan in the ninth over, before Khan followed up on the act by replicating the effort against David Miller and Wriddhiman Saha in the next over. Needless to say, as the wickets fell, Punjabâ€™s run-rate too trickled down, with the team reeling at 62/5 midway through their innings.\n",
      "Manan Vohra tried to marshal Punjabâ€™s run-chase in a semblance of Warnerâ€™s heroics in the first innings, hitting a finely paced half-century. But, the Sunrisersâ€™ bowlersâ€™ frequent chipping of his partnersâ€™ wickets at the other side, meant that he was left fighting a lone battle.\n",
      "Just like it seemed Vohra would run away with the match, Sunrisersâ€™ in-form bowler, Kumar bowled a beauty of a spell to dismiss the opener, who top-scored in the match with 95 runs from 50 balls, 66 of which came in fours and sixes.\n",
      "==================================================\n",
      "Why French globalist Macron is befriending nationalist Trump\n",
      "window._taboola = window._taboola || []; _taboola.push({ mode: 'thumbnails-c', container: 'taboola-interstitial-gallery-thumbnails-5', placement: 'Interstitial Gallery Thumbnails 5', target_type: 'mix' }); _taboola.push({flush: true});\n",
      "window._taboola = window._taboola || []; _taboola.push({ mode: 'thumbnails-c', container: 'taboola-interstitial-gallery-thumbnails-7', placement: 'Interstitial Gallery Thumbnails 7', target_type: 'mix' }); _taboola.push({flush: true});\n",
      "Photo: Evan Vucci, AP Image 1 of / 7 Caption Close Image 1 of 7 FILE - In this May 25, 2017 file photo, US President Donald Trump, shakes hands with French President Emmanuel Macron, right, during a meeting at the U.S. Embassy in Brussels. Macron arrives Monday April 23, 2018 in Washington for the first state visit of Trumpâ€™s presidency. The two men have an unlikely friendship, despite strong differences on areas such as climate change. less FILE - In this May 25, 2017 file photo, US President Donald Trump, shakes hands with French President Emmanuel Macron, right, during a meeting at the U.S. Embassy in Brussels. Macron arrives Monday April 23, ... more Photo: Evan Vucci, AP Image 2 of 7 FILE - In this Thursday, July 13, 2017 file photo, French President Emmanuel Macron and US President Donald Trump , right, walk in the courtyard of the Invalides as part of an official welcoming ceremony in Paris. Macron arrives Monday April 23, 2018 in Washington for the first state visit of Trumpâ€™s presidency. The two men have an unlikely friendship, despite strong differences on areas such as climate change. less FILE - In this Thursday, July 13, 2017 file photo, French President Emmanuel Macron and US President Donald Trump , right, walk in the courtyard of the Invalides as part of an official welcoming ceremony in ... more Photo: Matthieu Alexandre, AP Image 3 of 7 The U.S. and French flags are displayed on the Eisenhower Executive Office Building, Friday, April 20, 2018, in Washington. President Donald Trump plans to celebrate nearly 250 years of U.S.-French relations by hosting President Emmanuel Macron at a glitzy White House state dinner on Tuesday. Itâ€™s the first state visit and the first big soiree of the Trump era in Washington. less The U.S. and French flags are displayed on the Eisenhower Executive Office Building, Friday, April 20, 2018, in Washington. President Donald Trump plans to celebrate nearly 250 years of U.S.-French relations by ... more Photo: Alex Brandon, AP Image 4 of 7 FILE - In this Thursday, July 13, 2017 file photo, US President Donald Trump, first lady Melania Trump, French President Emmanuel Macron, right, and his wife Brigitte Macron, left, sit for dinner at the Jules Verne Restaurant at the Eiffel Tower in Paris. Macron arrives Monday April 23, 2018 in Washington for the first state visit of Trumpâ€™s presidency. The two men have an unlikely friendship, despite strong differences on areas such as climate change. less FILE - In this Thursday, July 13, 2017 file photo, US President Donald Trump, first lady Melania Trump, French President Emmanuel Macron, right, and his wife Brigitte Macron, left, sit for dinner at the Jules ... more Photo: Carolyn Kaster, AP Image 5 of 7 Image 6 of 7 The U.S., and French flags are displayed on the Eisenhower Executive Office Building as seen through the portico of the West Wing of the White House, Friday, April 20, 2018, in Washington. President Donald Trump plans to celebrate nearly 250 years of U.S.-French relations by hosting President Emmanuel Macron at a glitzy White House state dinner on Tuesday. Itâ€™s the first state visit and the first big soiree of the Trump era in Washington. less The U.S., and French flags are displayed on the Eisenhower Executive Office Building as seen through the portico of the West Wing of the White House, Friday, April 20, 2018, in Washington. President Donald ... more Photo: Alex Brandon, AP Image 7 of 7 Why French globalist Macron is befriending nationalist Trump 1 / 7 Back to Gallery\n",
      "PARIS (AP) â€” Of everything Emmanuel Macron has accomplished in nearly a year as France's president, the most important may be his tough-love friendship with Donald Trump.\n",
      "From their first bone-squeezing handshake to Macron's recent claim that he persuaded Trump to bomb Syria, it's been an improbable relationship. And it will be on pomp-filled display starting Monday as Macron goes on a state visit to Washington, the first by any leader since Trump took office.\n",
      "Macron calls Trump all the time. With other world leaders too wary or weak to woo the impulsive U.S. president, Macron calculates that it's smarter and safer to talk to Trump than isolate him.\n",
      "The 40-year-old moderate progressive, who had never held elected office before he won France's presidential election, defended his overtures to the 71-year-old conservative Trump in an interview on the broadcast \"Fox News Sunday.\"\n",
      "\"I am not going to judge ... what should be your president, or to consider that because of these controversies or because of these investigations, your president is less credible,\" he said.\n",
      "The French president has the most to gain from the three-day state visit. He wants to fortify his image as the face of today's Europe and the No. 1 defender of a liberal world order, as well as prove that France is essential to solving world problems such as Iran's nuclear ambitions and international trade wars.\n",
      "His aims may sound like French hubris or wishful thinking, but they are consistent with the \"France is back\" global strategy Macron has set for his tenure.\n",
      "He talks regularly to Russian President Vladimir Putin and other controversial leaders, too. He also has tried his own diplomatic maneuvering in the Middle East with the goals of defending French interests and making sure Europe has a say in the region's future.\n",
      "For all their camaraderie, Macron and Trump disagree on some fundamental issues.\n",
      "Take global warming. Macron mocked Trump's campaign slogan by promising in a Twitter video he recorded in English to \"Make our planet great again!\" The video was posted moments after Trump announced he wanted to pull out of the U.N.-sponsored Paris climate accord last year.\n",
      "Policy toward Iran is another point of discord. France is the most vigorous defender of the 2015 deal curbing Iran's nuclear ambitions. Trump is threatening to abandon the agreement next month. Macron hopes to make progress this week on convincing Trump to stay onboard.\n",
      "And then there's trade. Macron and German Chancellor Angela Merkel â€” who planned to visit Washington on Friday â€” have pushed back hard on Trump's steel tariffs and his America First vision, which threaten Europe's powerful single market.\n",
      "By design, Macron's state visit will be more about symbolism than substance, and no big breakthroughs are expected.\n",
      "But over the long term, Macron hopes his rapport with Trump will help mitigate some of their policy differences. His office holds up the U.S.-French cooperation on missile strikes on Syria this month as a model for future joint actions.\n",
      "So how has Macron managed to avoid annoying Trump, famously sensitive to slights?\n",
      "\"He has played Trump very well,\" said Nicolas Dungan, senior fellow at the Atlantic Council, a Washington-based think tank.\n",
      "While other world leaders and veterans of Beltway politics have made Trump feel like an outsider, Macron \"accepts him and respects him rather than disdaining him,\" Dungan said. \"It's a very effective strategy of influence ... through respect and treating him legitimately.\"\n",
      "At least so far. Macron still runs the risk of getting \"tagged with Trump's vulgarity, which so far he's been immune to,\" Dungan said.\n",
      "Macron's ease in speaking English probably helped in establishing a rapport â€” as did his ability to play the tough-guy game that Trump relishes.\n",
      "Despite being smaller and younger than Trump, Macron has been able to spar in a way that begets admiration instead of resentment, starting with their first meeting at a NATO summit in May, where Macron took Trump's hand and wouldn't let go.\n",
      "In Trump's confrontational world, \"Macron comes across as straight arrow, ready for a fight,\" said Francois Heisbourg, a former French government adviser and current chair of the International Institute for Strategic Studies. At their NATO meeting, \"Macron twists his arm. And this is the beginning of a great friendship,\" Heisbourg said.\n",
      "The French president's U.S. visit will highlight \"Macron's rather uncanny ability\" to get along with hard-line leaders, Heisbourg said. \"He is apparently the only one who can actually talk with all of them substantively, while at the same time not appearing to be jettisoning his own values.\"\n",
      "Macron also hopes the Washington trip will also raise his profile everywhere else â€” from the EU to Syria and Israel-Palestinian peacemaking.\n",
      "The trip will be a welcome distraction from Macron's troubles at home: He takes off the same day labor strikes are expected to halt a majority of French trains and a quarter of Air France flights.\n",
      "As Macron prepares to be feted at two fancy dinners and give a special speech to Congress, the White House and Elysee Palace are stressing their likenesses instead of points of discord.\n",
      "Both Macron and Trump are newcomers to politics with a background in business. And France is considered the first American ally, for helping the colonists win the Revolutionary War.\n",
      "Macron will celebrate the long-running alliance by giving Trump an oak tree sapling from the site of one of the first World War I battles involving American troops, the Battle of Belleau Wood.\n",
      "It's a sign of appreciation for the sacrifices America made for Franceâ€” and an apparent nod to Macron's concern for the environment.\n",
      "He wants it planted in the White House gardens.\n",
      "___\n",
      "Darlene Superville in Washington contributed.\n",
      "==================================================\n",
      "The Shelter for Abused Women & Children\n",
      "Kirsten Ferrara, Linda Oberhaus\n",
      "Wynnell Schrenk, Teri Kuhn, Linda Malone\n",
      "Simone Lutgert, Heather Dockweiler\n",
      "Jennifer Hyman\n",
      "Barbara Jordan, Shelly Stayer, Patty Baker\n",
      "Juliana Meek, Barbara Meek\n",
      "Lisa Arundale, Jamie Anderson, Nancy White, Jennifer Hyman, Pat Wheeler, Colleen Murphy\n",
      "Kaleigh Grover, Melissa Kaplan, Brenda Melton, Carly Stewart\n",
      "Lisa Spiller, Gillian Campbell, Sandy Cotter\n",
      "What: Old Bags Luncheon and Handbag Auction with guest speaker Jennifer Hyman, Rent The Runway co-founder and CEO\n",
      "Who: Guests and Supporters of the shelter\n",
      "Where: The Ritz-Carlton Golf Resort, Naples\n",
      "The Event Was Sponsored by Naples Illustrated\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for t in d[\"train\"][\"text\"][:3]:\n",
    "    print(t)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e0377",
   "metadata": {},
   "source": [
    "### using personal dataset\n",
    "* https://huggingface.co/docs/datasets/dataset_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eefe659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you have huge custom dataset separated into files\n",
    "# # load the splitted files\n",
    "# files = [\"train1.txt\", \"train2.txt\"] # train3.txt, etc.\n",
    "# dataset = load_dataset(\"text\", data_files=files, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05736a8a",
   "metadata": {},
   "source": [
    "### Training the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04d6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train the tokenizer from scratch (especially if you have custom\n",
    "# dataset loaded as datasets object), then run this cell to save it as files\n",
    "# but if you already have your custom data as text files, there is no point using this\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    \"\"\"Utility function to save dataset text to disk,\n",
    "    useful for using the texts to train the tokenizer \n",
    "    (as the tokenizer accepts files)\"\"\"\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)\n",
    "\n",
    "# save the training set to train.txt\n",
    "dataset_to_text(d[\"train\"], \"train.txt\")\n",
    "# save the testing set to test.txt\n",
    "dataset_to_text(d[\"test\"], \"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0080d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# if you want to train the tokenizer on both sets\n",
    "# files = [\"train.txt\", \"test.txt\"]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"train.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 30_522\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fba11d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize the WordPiece tokenizer\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "# train the tokenizer\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "# enable truncation up to the maximum 512 tokens\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e997ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"pretrained_bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f6a3687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pretrained_bert/vocab.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the directory if not already there\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "# save the tokenizer  \n",
    "tokenizer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e1ef64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping some of the tokenizer config to config file, \n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "    tokenizer_cfg = {\"do_lower_case\": True,\n",
    "                     \"unk_token\": \"[UNK]\",\n",
    "                     \"sep_token\": \"[SEP]\",\n",
    "                     \"pad_token\": \"[PAD]\",\n",
    "                     \"cls_token\": \"[CLS]\",\n",
    "                     \"mask_token\": \"[MASK]\",\n",
    "                     \"model_max_length\": max_length,\n",
    "                     \"max_len\": max_length,\n",
    "                    }\n",
    "    json.dump(tokenizer_cfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ae599c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file pretrained_bert/tokenizer.json. We won't load it.\n",
      "Didn't find file pretrained_bert/added_tokens.json. We won't load it.\n",
      "Didn't find file pretrained_bert/special_tokens_map.json. We won't load it.\n",
      "Didn't find file pretrained_bert/tokenizer_config.json. We won't load it.\n",
      "loading file pretrained_bert/vocab.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading configuration file pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when the tokenizer is trained and configured, load it as BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c948342c",
   "metadata": {},
   "source": [
    "### Tokenizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f4d61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                                     max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d69ef9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49315ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 638/638 [02:59<00:00,  3.55ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:19<00:00,  3.56ba/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the train dataset\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_dataset = d[\"test\"].map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a221ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if truncate_longer_samples:\n",
    "    # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    # remove other columns, and remain them as Python lists\n",
    "    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69f57d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "108773bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b05b4ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# https://pytorch.org/get-started/locally/#windows-package-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f18758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 638/638 [04:55<00:00,  2.16ba/s]\n",
      "Grouping texts in chunks of 512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:32<00:00,  2.18ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "if not truncate_longer_samples:\n",
    "    train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "                                                                        desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    # convert them from lists to torch tensors\n",
    "    train_dataset.set_format(\"torch\")\n",
    "    test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "172a1569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644108, 71094)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce758966",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fbf1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model with the config\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd00dc",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "095d5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fe9336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 1000\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # evaluate each `logging_steps` steps\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=10,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=10, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=8,  # evaluation batch size\n",
    "    logging_steps=1000,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1000,\n",
    "    # load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7df02f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the trainer and pass everything to it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41ffb4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/hdh/PycharmProjects/huggingface_bert/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 644108\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 80\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 80510\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='631' max='80510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  631/80510 57:10 < 121:00:12, 0.18 it/s, Epoch 0.08/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/huggingface_bert/venv/lib/python3.8/site-packages/transformers/trainer.py:1422\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1422\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1425\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1428\u001b[0m ):\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/PycharmProjects/huggingface_bert/venv/lib/python3.8/site-packages/transformers/trainer.py:2029\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2027\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2029\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/PycharmProjects/huggingface_bert/venv/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/huggingface_bert/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ebafb",
   "metadata": {},
   "source": [
    "### Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model checkpoint\n",
    "model = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-10000\"))\n",
    "# load the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaee74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b09e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform predictions\n",
    "examples = [\n",
    "    \"Today's most trending hashtags on [MASK] is Donald Trump\",\n",
    "    \"The [MASK] was cloudy yesterday, but today it's rainy.\",\n",
    "]\n",
    "for example in examples:\n",
    "    for prediction in fill_mask(example):\n",
    "        print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
