{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10fa50a",
   "metadata": {},
   "source": [
    "### Train BERT Tokenizer\n",
    "* https://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python\n",
    "* https://huggingface.co/transformers/v3.2.0/training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e678dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/huggingface_bert/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "from datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.data import load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3ddb6",
   "metadata": {},
   "source": [
    "### Tokenizer train data 생성\n",
    "* 약어 이후에 등장하는 마침표를 사용해 문장이 분리되지 않도록 조치를 해야 한다.\n",
    "* NLTK의 tokenizer를 사용해 문장 분리하기 위해 extra_abbreviations에 예외조건을 추가하여 준다.\n",
    "    * https://cryptosalamander.tistory.com/140?category=1218889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8cce6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = load(\"tokenizers/punkt/english.pickle\")\n",
    "extra_abbreviations = [\n",
    "    'RE','re','pat', 'no', 'nos','vol','jan','feb','mar','apr','jun',\n",
    "    'jul','aug','sep','oct','nov','dec','eng','ser','ind','ed','pp',\n",
    "    'e.g','al','T.E.N.S', 'E.M.S','F.E','U.H.T.S.T','degree',\n",
    "    '/gm','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "    'P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "sent_tokenizer._params.abbrev_types.update(extra_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbed7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK의 tokenizer를 사용해 문장 분리(미사용)\n",
    "# https://cryptosalamander.tistory.com/140?category=1218889\n",
    "def sent_tokenize(input='./input.txt', output='./output.txt'):\n",
    "    sent_tokenizer = load(\"tokenizers/punkt/english.pickle\")\n",
    "    extra_abbreviations = [\n",
    "        'RE','re','pat', 'no', 'nos','vol','jan','feb','mar','apr','jun',\n",
    "        'jul','aug','sep','oct','nov','dec','eng','ser','ind','ed','pp',\n",
    "        'e.g','al','T.E.N.S', 'E.M.S','F.E','U.H.T.S.T','degree',\n",
    "        '/gm','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "        'P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "    sent_tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "\n",
    "    load_file=open(input,'r')\n",
    "    save_file=open(output,'w')\n",
    "    no_blank = False\n",
    "    while True:\n",
    "        line = load_file.readline()\n",
    "        if line == \"\":\n",
    "            break\n",
    "        if line.strip() == \"\":\n",
    "            if no_blank:\n",
    "                continue\n",
    "            save_file.write(f\"{line}\")\n",
    "        else:\n",
    "            print(line)\n",
    "            result_ = tokenizer.tokenize(line)\n",
    "            print(result_)\n",
    "            result  = [ f\"{cur_line}\\n\" for cur_line in result_ ]\n",
    "            for save_line in result:\n",
    "                save_file.write(save_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99df9ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"data/h04w4/train_H04W4_220511.txt\", delimiter='\\t', dtype=str, header=0, names=['text', 'label'])\n",
    "df_test = pd.read_csv(\"data/h04w4/test_H04W4_220511.txt\", delimiter='\\t', dtype=str, header=0, names=['text', 'label'])\n",
    "# dataset_train = Dataset.from_text('data/h04w4/train_H04W4_220511.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38d074bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_test], axis=0)\n",
    "df['text'].to_csv(\"data/h04w4/tok_train_H04W4.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d86bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-84170d85970a0abc\n",
      "WARNING:datasets.builder:Reusing dataset text (/home/hdh/.cache/huggingface/datasets/text/default-84170d85970a0abc/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     dataset = Dataset.from_text('data/c09k_corpus.txt')\n",
    "# except:\n",
    "#     kiwee_files = ['data/c09k_0001-1000.xlsx', 'data/c09k_1001-2000.xlsx', 'data/c09k_2001-3000.xlsx', 'data/c09k_3001-3935.xlsx']\n",
    "#     with open('data/c09k_corpus.txt', 'a') as f:\n",
    "#         f.truncate(0)\n",
    "#         for i, fn in enumerate(kiwee_files):\n",
    "#             tmp = pd.read_excel(fn).fillna('')\n",
    "#             # pandas는 비어있는 컬럼의 dtype을 float로 바꿔서 인식한다. 그로 인해 토크나이징 할 데이터가 없으면 오류가 발생되어 fillna를 사용해 모두 텍스트로 인식시키도록 한다\n",
    "#             # https://stackoverflow.com/questions/53953286/pandas-read-excel-blanks-in-string-columns-convert-to-floats-converting-via-st\n",
    "#             col_text = ['발행번호', '발명의명칭', '요약', '대표청구항', '과제', '해결방안']\n",
    "#             tmp = tmp[col_text]\n",
    "#             for index, row in tmp.iterrows():\n",
    "#         #         print(index, '\\n', row['발명의명칭'], row['요약'], row['대표청구항'], row['과제'], row['해결방안'], '\\n')\n",
    "#                 for col in col_text[1:]:\n",
    "#         #             print('처리중인 데이터:', col, row[col], '\\n')\n",
    "#                     if row[col].strip() == \"\":\n",
    "#                         pass\n",
    "#                     else:\n",
    "#         #                 print(row[col].strip())\n",
    "#                         row[col] = unicodedata.normalize('NFKC', row[col])\n",
    "#                         # row[col] = unicodedata.normalize('NFC', row[col])  # 자음과 모음이 깨질 때는 NFC로 변환\n",
    "#                         # NFD(Normalization Form Decomposition) : 자음과 모음이 분리\n",
    "#                         # row[col] = unicodedata.normalize('NFKD', row[col])\n",
    "#                         #     https://blog.naver.com/PostView.nhn?blogId=duswl0319&logNo=221516880642&from=search&redirect=Log&widgetTypeCall=true&directAccess=false\n",
    "#                         row[col] = row[col].replace('\\n\\t',' ')\n",
    "#                         row[col] = row[col].replace('\\n',' ')\n",
    "#                         row[col] = row[col].replace('&lt;',' ')\n",
    "#                         row[col] = row[col].replace('_x000d_',' ')\n",
    "#                         row[col] = row[col].replace('\\t\\t',' ')\n",
    "#                         row[col] = row[col].replace('@@',' ')\n",
    "#                         row[col] = row[col].replace('.  .','.')\n",
    "#                         row[col] = row[col].replace('. .','.')\n",
    "#                         row[col] = row[col].replace('..','.')\n",
    "#                         row[col] = row[col].replace('〜','~')\n",
    "#                         row[col] = row[col].replace(' . ','.')\n",
    "#                         row[col] = row[col].replace(' ． ','.')\n",
    "#                         row[col] = row[col].replace('． ','.')\n",
    "#                         row[col] = row[col].replace('. ','.')\n",
    "#                         row[col] = row[col].replace('  ',' ')\n",
    "#                         row[col] = row[col].replace('  ',' ')\n",
    "#                         row[col] = row[col].replace('【과제】',' ')\n",
    "#                         row[col] = row[col].replace('【요약】',' ')\n",
    "#                         row[col] = row[col].replace('【해결 수단】',' ')\n",
    "#                         str_tmp = sent_tokenizer.tokenize(row[col].strip())\n",
    "#         #                 print('문장 분리: ', str_tmp, '\\n'*3)\n",
    "#         #                 result  = [f\"{line}\\n\" for line in str_tmp]\n",
    "#                         for line in str_tmp:\n",
    "#                             f.write(f\"{line}\\n\")\n",
    "#     dataset = Dataset.from_text('data/c09k_corpus.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221e15e",
   "metadata": {},
   "source": [
    "### Training the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cc64d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\n",
    "  \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "# if you want to train the tokenizer on both sets\n",
    "# files = [\"train.txt\", \"test.txt\"]\n",
    "# training the tokenizer on the training set\n",
    "files = [\"data/h04w4/tok_train_H04W4.txt\"]\n",
    "# 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "vocab_size = 3000\n",
    "# maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "max_length = 512\n",
    "# whether to truncate\n",
    "truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37aa7caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertWordPieceTokenizer(handle_chinese_chars=False, lowercase=False, strip_accents=False)\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens, show_progress=True)\n",
    "tokenizer.enable_truncation(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef9182b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"h04w4_trained_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5d3f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the directory if not already there\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "# save the tokenizer  \n",
    "tokenizer.save_model(model_path)\n",
    "tokenizer.save(os.path.join(model_path, 'tokenizer.json'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface_bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
