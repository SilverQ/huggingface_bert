{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0416e849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdh/PycharmProjects/huggingface_bert/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "# pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org tokenizers\n",
    "\n",
    "from datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.data import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68582e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = load(\"tokenizers/punkt/english.pickle\")\n",
    "extra_abbreviations = [\n",
    "    'RE', 're', 'pat', 'no', 'nos', 'vol', 'jan', 'feb', 'mar', 'apr', 'jun',\n",
    "    'jul', 'aug', 'sep', 'oct', 'nov', 'dec', 'eng', 'ser', 'ind', 'ed', 'pp',\n",
    "    'e.g', 'al', 'T.E.N.S', 'E.M.S', 'F.E', 'U.H.T.S.T', 'degree',\n",
    "    '/gm', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O',\n",
    "    'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1bd1fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer._params.abbrev_types.update(extra_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "091ef15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h04w4_03.xlsx', 'h04w4_05.xlsx', 'h04w4_01.xlsx', 'h04w4_04.xlsx', 'h04w4_02.xlsx']\n"
     ]
    }
   ],
   "source": [
    "data_path = 'data/h04w4/'\n",
    "kiwee_data_list = os.listdir(data_path)\n",
    "kiwee_data_list = [fn for fn in kiwee_data_list if fn[-4:]=='xlsx']\n",
    "print(kiwee_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad9dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kiwee_data():\n",
    "    kiwee_files = kiwee_data_list\n",
    "    with open('data/c09k_corpus.txt', 'a', encoding='utf-8') as f:\n",
    "        f.truncate(0)\n",
    "        for i, fn in enumerate(kiwee_files):\n",
    "            print(fn)\n",
    "            tmp = pd.read_excel(os.path.join(data_path, fn), engine=\"openpyxl\").fillna('')\n",
    "            # pandas는 비어있는 컬럼의 dtype을 float로 바꿔서 인식한다. 그로 인해 토크나이징 할 데이터가 없으면 오류가 발생되어 fillna를 사용해 모두 텍스트로 인식시키도록 한다\n",
    "            # https://stackoverflow.com/questions/53953286/pandas-read-excel-blanks-in-string-columns-convert-to-floats-converting-via-st\n",
    "            col_text = ['발행번호', '발명의명칭', '요약', '대표청구항', '과제', '해결방안']\n",
    "            tmp = tmp[col_text]\n",
    "            for index, row in tmp.iterrows():\n",
    "                # print(index, '\\n', row['발명의명칭'], row['요약'], row['대표청구항'], row['과제'], row['해결방안'], '\\n')\n",
    "                for col in col_text[1:]:\n",
    "                    # print('처리중인 데이터:', col, row[col], '\\n')\n",
    "                    if row[col].strip() == \"\":\n",
    "                        pass\n",
    "                    else:\n",
    "                        # print(row[col].strip())\n",
    "                        row[col] = unicodedata.normalize('NFKC', row[col])\n",
    "                        # row[col] = unicodedata.normalize('NFC', row[col])  # 자음과 모음이 깨질 때는 NFC로 변환\n",
    "                        # NFD(Normalization Form Decomposition) : 자음과 모음이 분리\n",
    "                        # row[col] = unicodedata.normalize('NFKD', row[col])\n",
    "                        #     https://blog.naver.com/PostView.nhn?blogId=duswl0319&logNo=221516880642&from=search&redirect=Log&widgetTypeCall=true&directAccess=false\n",
    "                        row[col] = row[col].replace('\\n\\t', ' ')\n",
    "                        row[col] = row[col].replace('\\n', ' ')\n",
    "                        row[col] = row[col].replace('&lt;', ' ')\n",
    "                        row[col] = row[col].replace('_x000d_', ' ')\n",
    "                        row[col] = row[col].replace('\\t\\t', ' ')\n",
    "                        row[col] = row[col].replace('@@', ' ')\n",
    "                        row[col] = row[col].replace('.  .', '.')\n",
    "                        row[col] = row[col].replace('. .', '.')\n",
    "                        row[col] = row[col].replace('..', '.')\n",
    "                        row[col] = row[col].replace('〜', '~')\n",
    "                        row[col] = row[col].replace(' . ', '.')\n",
    "                        row[col] = row[col].replace(' ． ', '.')\n",
    "                        row[col] = row[col].replace('． ', '.')\n",
    "                        row[col] = row[col].replace('. ', '.')\n",
    "                        row[col] = row[col].replace('  ', ' ')\n",
    "                        row[col] = row[col].replace('  ', ' ')\n",
    "                        row[col] = row[col].replace('【과제】', ' ')\n",
    "                        row[col] = row[col].replace('【요약】', ' ')\n",
    "                        row[col] = row[col].replace('【해결 수단】', ' ')\n",
    "                        str_tmp = sent_tokenizer.tokenize(row[col].strip())\n",
    "                        # print('문장 분리: ', str_tmp, '\\n'*3)\n",
    "                        # result  = [f\"{line}\\n\" for line in str_tmp]\n",
    "                        for line in str_tmp:\n",
    "                            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_kiwee_data()\n",
    "\n",
    "dataset = Dataset.from_text('data/c09k_170k_corpus.txt')\n",
    "# print(dataset['text'][0:5])\n",
    "# print(dataset['text'][-5:])\n",
    "\n",
    "# d = dataset.train_test_split(test_size=0.15)\n",
    "\n",
    "try:\n",
    "    with open('data/c09k_170k_dataset.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    print('dataset loading completed')\n",
    "except:\n",
    "    d = dataset.train_test_split(test_size=0.15)\n",
    "    with open('data/c09k_170k_dataset.pkl', 'wb') as f:\n",
    "        pickle.dump(d, f)\n",
    "    print('dataset split/saving completed')\n",
    "\n",
    "print(d)\n",
    "print(d['train']['text'][0:5])\n",
    "print(d['test']['text'][-5:])\n",
    "\n",
    "# for t in d[\"train\"][\"text\"][:3]:\n",
    "#     print(t)\n",
    "#     print(\"=\"*50)\n",
    "#\n",
    "# for t in d[\"test\"][\"text\"][:3]:\n",
    "#     print(t)\n",
    "#     print(\"=\"*50)\n",
    "\n",
    "special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"]\n",
    "files = [\"data/c09k_corpus.txt\"]\n",
    "vocab_size = 8000\n",
    "max_length = 512\n",
    "truncate_longer_samples = False\n",
    "\n",
    "#\n",
    "# def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "#     \"\"\"Utility function to save dataset text to disk, useful for using the texts to train the tokenizer\n",
    "#      (as the tokenizer accepts files)\"\"\"\n",
    "#     with open(output_filename, \"w\") as f:\n",
    "#         for t in dataset[\"text\"]:\n",
    "#             print(t, file=f)\n",
    "#\n",
    "#\n",
    "# model_path = 'c09k_pretrained_bert'\n",
    "#\n",
    "# dataset_to_text(d[\"train\"], \"data/c09k_pre_train.txt\")\n",
    "# dataset_to_text(d[\"test\"], \"data/c09k_pre_test.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface_bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
