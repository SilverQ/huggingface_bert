{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10fa50a",
   "metadata": {},
   "source": [
    "### Pre-train BERT\n",
    "* https://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python\n",
    "* https://huggingface.co/transformers/v3.2.0/training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93e678dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import *\n",
    "from tokenizers import *\n",
    "from datasets import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.data import load\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3ddb6",
   "metadata": {},
   "source": [
    "### Tokenizer train data 생성\n",
    "* 약어 이후에 등장하는 마침표를 사용해 문장이 분리되지 않도록 조치를 해야 한다.\n",
    "* NLTK의 tokenizer를 사용해 문장 분리하기 위해 extra_abbreviations에 예외조건을 추가하여 준다.\n",
    "* 클린징이 끝난 데이터는 토크나이저 학습에 바로 사용하고, 이 데이터에서 테스트셋을 분리하여 pre-train 데이터로 사용예정\n",
    "    * https://cryptosalamander.tistory.com/140?category=1218889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8cce6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenizer = load(\"tokenizers/punkt/english.pickle\")\n",
    "extra_abbreviations = [\n",
    "    'RE','re','pat', 'no', 'nos','vol','jan','feb','mar','apr','jun',\n",
    "    'jul','aug','sep','oct','nov','dec','eng','ser','ind','ed','pp',\n",
    "    'e.g','al','T.E.N.S', 'E.M.S','F.E','U.H.T.S.T','degree',\n",
    "    '/gm','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "    'P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "sent_tokenizer._params.abbrev_types.update(extra_abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbed7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK의 tokenizer를 사용해 문장 분리(미사용)\n",
    "# https://cryptosalamander.tistory.com/140?category=1218889\n",
    "def sent_tokenize(input='./input.txt', output='./output.txt'):\n",
    "    sent_tokenizer = load(\"tokenizers/punkt/english.pickle\")\n",
    "    extra_abbreviations = [\n",
    "        'RE','re','pat', 'no', 'nos','vol','jan','feb','mar','apr','jun',\n",
    "        'jul','aug','sep','oct','nov','dec','eng','ser','ind','ed','pp',\n",
    "        'e.g','al','T.E.N.S', 'E.M.S','F.E','U.H.T.S.T','degree',\n",
    "        '/gm','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O',\n",
    "        'P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "    sent_tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "\n",
    "    load_file=open(input,'r')\n",
    "    save_file=open(output,'w')\n",
    "    no_blank = False\n",
    "    while True:\n",
    "        line = load_file.readline()\n",
    "        if line == \"\":\n",
    "            break\n",
    "        if line.strip() == \"\":\n",
    "            if no_blank:\n",
    "                continue\n",
    "            save_file.write(f\"{line}\")\n",
    "        else:\n",
    "            print(line)\n",
    "            result_ = tokenizer.tokenize(line)\n",
    "            print(result_)\n",
    "            result  = [ f\"{cur_line}\\n\" for cur_line in result_ ]\n",
    "            for save_line in result:\n",
    "                save_file.write(save_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d86bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-84170d85970a0abc\n",
      "WARNING:datasets.builder:Reusing dataset text (/home/hdh/.cache/huggingface/datasets/text/default-84170d85970a0abc/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset = Dataset.from_text('data/c09k_corpus.txt')\n",
    "except:\n",
    "    kiwee_files = ['data/c09k_0001-1000.xlsx', 'data/c09k_1001-2000.xlsx', 'data/c09k_2001-3000.xlsx', 'data/c09k_3001-3935.xlsx']\n",
    "    with open('data/c09k_corpus.txt', 'a') as f:\n",
    "        f.truncate(0)\n",
    "        for i, fn in enumerate(kiwee_files):\n",
    "            tmp = pd.read_excel(fn).fillna('')\n",
    "            # pandas는 비어있는 컬럼의 dtype을 float로 바꿔서 인식한다. 그로 인해 토크나이징 할 데이터가 없으면 오류가 발생되어 fillna를 사용해 모두 텍스트로 인식시키도록 한다\n",
    "            # https://stackoverflow.com/questions/53953286/pandas-read-excel-blanks-in-string-columns-convert-to-floats-converting-via-st\n",
    "            col_text = ['발행번호', '발명의명칭', '요약', '대표청구항', '과제', '해결방안']\n",
    "            tmp = tmp[col_text]\n",
    "            for index, row in tmp.iterrows():\n",
    "        #         print(index, '\\n', row['발명의명칭'], row['요약'], row['대표청구항'], row['과제'], row['해결방안'], '\\n')\n",
    "                for col in col_text[1:]:\n",
    "        #             print('처리중인 데이터:', col, row[col], '\\n')\n",
    "                    if row[col].strip() == \"\":\n",
    "                        pass\n",
    "                    else:\n",
    "        #                 print(row[col].strip())\n",
    "                        row[col] = unicodedata.normalize('NFKC', row[col])\n",
    "                        # row[col] = unicodedata.normalize('NFC', row[col])  # 자음과 모음이 깨질 때는 NFC로 변환\n",
    "                        # NFD(Normalization Form Decomposition) : 자음과 모음이 분리\n",
    "                        # row[col] = unicodedata.normalize('NFKD', row[col])\n",
    "                        #     https://blog.naver.com/PostView.nhn?blogId=duswl0319&logNo=221516880642&from=search&redirect=Log&widgetTypeCall=true&directAccess=false\n",
    "                        row[col] = row[col].replace('\\n\\t',' ')\n",
    "                        row[col] = row[col].replace('\\n',' ')\n",
    "                        row[col] = row[col].replace('&lt;',' ')\n",
    "                        row[col] = row[col].replace('_x000d_',' ')\n",
    "                        row[col] = row[col].replace('\\t\\t',' ')\n",
    "                        row[col] = row[col].replace('@@',' ')\n",
    "                        row[col] = row[col].replace('.  .','.')\n",
    "                        row[col] = row[col].replace('. .','.')\n",
    "                        row[col] = row[col].replace('..','.')\n",
    "                        row[col] = row[col].replace('〜','~')\n",
    "                        row[col] = row[col].replace(' . ','.')\n",
    "                        row[col] = row[col].replace(' ． ','.')\n",
    "                        row[col] = row[col].replace('． ','.')\n",
    "                        row[col] = row[col].replace('. ','.')\n",
    "                        row[col] = row[col].replace('  ',' ')\n",
    "                        row[col] = row[col].replace('  ',' ')\n",
    "                        row[col] = row[col].replace('【과제】',' ')\n",
    "                        row[col] = row[col].replace('【요약】',' ')\n",
    "                        row[col] = row[col].replace('【해결 수단】',' ')\n",
    "                        str_tmp = sent_tokenizer.tokenize(row[col].strip())\n",
    "        #                 print('문장 분리: ', str_tmp, '\\n'*3)\n",
    "        #                 result  = [f\"{line}\\n\" for line in str_tmp]\n",
    "                        for line in str_tmp:\n",
    "                            f.write(f\"{line}\\n\")\n",
    "    dataset = Dataset.from_text('data/c09k_corpus.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5131dc6",
   "metadata": {},
   "source": [
    "### 학습 데이터 분리/로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2272b29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading completed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('data/c09k_dataset.pkl','rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    print('dataset loading completed')\n",
    "except:\n",
    "    d = dataset.train_test_split(test_size=0.1)\n",
    "    with open('data/c09k_dataset.pkl','wb') as f:\n",
    "        pickle.dump(d, f)\n",
    "    print('dataset split/saving completed')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58a042e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # d[\"train\"], d[\"test\"]\n",
    "# for t in d[\"train\"][\"text\"][:3]:\n",
    "#     print(t)\n",
    "#     print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cc64d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_tokens = [\n",
    "#   \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "# ]\n",
    "# # if you want to train the tokenizer on both sets\n",
    "# # files = [\"train.txt\", \"test.txt\"]\n",
    "# # training the tokenizer on the training set\n",
    "# files = [\"data/c09k_corpus.txt\"]\n",
    "# # 30,522 vocab is BERT's default vocab size, feel free to tweak\n",
    "# vocab_size = 8000\n",
    "# # maximum sequence length, lowering will result to faster training (when increasing batch size)\n",
    "# max_length = 512\n",
    "# # whether to truncate\n",
    "# truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef9182b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"c09k_pretrained_bert\"\n",
    "vocab_size = 8000\n",
    "max_length = 512\n",
    "truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90bf4df",
   "metadata": {},
   "source": [
    "### Pre-train data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e75208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b4658e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping some of the tokenizer config to config file, \n",
    "# including special tokens, whether to lower case and the maximum sequence length\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "    tokenizer_cfg = {\"do_lower_case\": True,\n",
    "                     \"unk_token\": \"[UNK]\",\n",
    "                     \"sep_token\": \"[SEP]\",\n",
    "                     \"pad_token\": \"[PAD]\",\n",
    "                     \"cls_token\": \"[CLS]\",\n",
    "                     \"mask_token\": \"[MASK]\",\n",
    "                     \"model_max_length\": max_length,\n",
    "                     \"max_len\": max_length,\n",
    "                    }\n",
    "    json.dump(tokenizer_cfg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af37bfc",
   "metadata": {},
   "source": [
    "### Tokenizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81bd4e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# when the tokenizer is trained and configured, load it as BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_path, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "820274b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\",\n",
    "                                     max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "    \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9526a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the encode function will depend on the truncate_longer_samples variable\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1791637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 18.37ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 21.79ba/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the train dataset\n",
    "train_dataset = d[\"train\"].map(encode, batched=True)\n",
    "# tokenizing the testing dataset\n",
    "test_dataset = d[\"test\"].map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4789f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ecd7d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14769"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(train_dataset[:2])\n",
    "len(train_dataset)  # 14769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e884ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "if truncate_longer_samples:\n",
    "    # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    # remove other columns, and remain them as Python lists\n",
    "    test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "    train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb19d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e6f35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ba5fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d21bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 64: 100%|██████████| 15/15 [00:01<00:00, 11.27ba/s]\n",
      "Grouping texts in chunks of 64: 100%|██████████| 2/2 [00:00<00:00, 16.79ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "# remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "# might be slower to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "if not truncate_longer_samples:\n",
    "    train_dataset1 = train_dataset.map(group_texts, batched=True,\n",
    "                                                                        desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    test_dataset1 = test_dataset.map(group_texts, batched=True,\n",
    "                                                                    desc=f\"Grouping texts in chunks of {max_length}\")\n",
    "    # convert them from lists to torch tensors\n",
    "    train_dataset1.set_format(\"torch\")\n",
    "    test_dataset1.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38b171d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19434, 2011)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(train_dataset), len(test_dataset)  # (14769, 1642)\n",
    "len(train_dataset1), len(test_dataset1)  # (2171, 225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d42a6a42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  37, 1075, 1198,   39,  891, 7003,   16, 6244, 2185, 3629, 3826, 1623,\n",
      "         447,  300,  428,  750,  398, 2129, 1185, 7661, 2781,   16, 2969,  547,\n",
      "         414, 5272,  401, 2338, 7140, 2159,  645,  595,  616,  551,   16,  863,\n",
      "        1527,  389,  960,  676,   18,   21,   18, 1399, 1200, 1558, 1300, 1875,\n",
      "          16,   37, 4342,  283,   39,   12,   41,   13,   22,  611,   24, 1288,\n",
      "           9,  518,  578, 1401])\n",
      "[ claim 15 ] 기판 표면이, 콜로이드 현탁액에 대한 젖어 성을 한층 높이기 위해서, 희산 또는 염기를 이용해 사전 처리 되는 것을 특징으로 하는, 청구항 14에 기재의 방법. 1. 발명에 따른 감온 안료 조성물은, [ 0036 ] ( a ) 2 내지 4중량 % 의 적어도 1종의\n",
      "tensor([ 784,   17, 5718,  678, 1149,  457,   16,   37, 4342,  272,   39,   12,\n",
      "          42,   13,   24,  611, 4296,    9,  518,  578, 1401,  784,   17, 4488,\n",
      "         457,   16,  412,   37, 4342,  278,   39,   12,   43,   13,   28,  283,\n",
      "         611,   29,  265, 1288,    9,  518,  746,   12,   49,   13,  498, 5165,\n",
      "         578, 1401,  758, 1059,   18,   22,   18, 1399, 1200, 4682, 3097,   24,\n",
      "         611, 3359,  383,  539])\n",
      "전자 - 공여체 유기 염료 화합물, [ 0037 ] ( b ) 4 내지 10중량 % 의 적어도 1종의 전자 - 수용체 화합물, 및 [ 0038 ] ( c ) 86 내지 94중량 % 의 화학식 ( i ) 에 대응하는 적어도 1종의 화합물을 포함한다. 2. 발명에 따른 아릴기는 바람직하게는 4 내지 12개의 탄소\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(train_dataset1['input_ids'][i])\n",
    "    print(tokenizer.decode(train_dataset1['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42187cb",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fcc3326b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c302756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model with the config\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd61aca",
   "metadata": {},
   "source": [
    "### Pre-Training - 최초"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e84fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data collator, randomly masking 20% (default is 15%) of the tokens for the Masked Language\n",
    "# Modeling (MLM) task\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "091a0983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 100\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # 'steps': evaluate each `logging_steps`, 'epoch'  : each epoch\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=50.,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=32, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=8,  # evaluation batch size\n",
    "    logging_steps=100,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    # save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb0c3d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c09k_pretrained_bert'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fec125d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the trainer and pass everything to it\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset1,\n",
    "    eval_dataset=test_dataset1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34b95bbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/hdh/PycharmProjects/bert_pretrain/venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 19400\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 3750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 1:46:42, Epoch 49/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.295600</td>\n",
       "      <td>6.732242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.603300</td>\n",
       "      <td>6.342851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>6.378600</td>\n",
       "      <td>6.237882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6.275000</td>\n",
       "      <td>6.045237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.102300</td>\n",
       "      <td>5.956049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.024500</td>\n",
       "      <td>5.918322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>5.994600</td>\n",
       "      <td>5.867238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>5.883700</td>\n",
       "      <td>5.784225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>5.825500</td>\n",
       "      <td>5.757144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.819400</td>\n",
       "      <td>5.694105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>5.728500</td>\n",
       "      <td>5.668516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>5.691100</td>\n",
       "      <td>5.638960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>5.687200</td>\n",
       "      <td>5.578866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>5.619400</td>\n",
       "      <td>5.558354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.582000</td>\n",
       "      <td>5.563128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>5.597600</td>\n",
       "      <td>5.528009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>5.518400</td>\n",
       "      <td>5.488570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>5.498000</td>\n",
       "      <td>5.512976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>5.503800</td>\n",
       "      <td>5.431159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.374900</td>\n",
       "      <td>5.309493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>5.276200</td>\n",
       "      <td>5.241166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>5.267300</td>\n",
       "      <td>5.139965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>5.171000</td>\n",
       "      <td>5.100589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>5.114700</td>\n",
       "      <td>5.052434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.118000</td>\n",
       "      <td>5.023229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>5.009800</td>\n",
       "      <td>4.934514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>4.978600</td>\n",
       "      <td>4.957025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>4.977100</td>\n",
       "      <td>4.885470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>4.901400</td>\n",
       "      <td>4.841716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.859000</td>\n",
       "      <td>4.804327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>4.865800</td>\n",
       "      <td>4.772527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>4.791300</td>\n",
       "      <td>4.738567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>4.776500</td>\n",
       "      <td>4.705458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>4.787400</td>\n",
       "      <td>4.691324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.727700</td>\n",
       "      <td>4.645294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>4.727700</td>\n",
       "      <td>4.653579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>4.763200</td>\n",
       "      <td>4.658380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-1000\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-1000/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-2000\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-2000/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-2000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-3000\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-3000/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-3000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from c09k_pretrained_bert/checkpoint-3000 (score: 4.804327011108398).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=5.451891813151041, metrics={'train_runtime': 6405.2907, 'train_samples_per_second': 151.437, 'train_steps_per_second': 0.585, 'total_flos': 3.18986161668096e+16, 'train_loss': 5.451891813151041, 'epoch': 49.99})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68505fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in c09k_pretrained_bert/checkpoint-4320/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-4320/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(os.path.join(model_path, 'checkpoint-4320'))\n",
    "# 마지막 iter의 모델을 수동 저장 - 뭔가 편한 방법이 있을듯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c426bb",
   "metadata": {},
   "source": [
    "### Pre-Training - Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "491d7a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_pretrained_bert/checkpoint-7500/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert/checkpoint-4320\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_pretrained_bert/checkpoint-7500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at c09k_pretrained_bert/checkpoint-7500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model1 = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-7500\"))\n",
    "# load the tokenizer\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "tokenizer1 = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d2e5f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=model1, tokenizer=tokenizer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f44b9985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.46306195855140686, 'token': 1193, 'token_str': '발광', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.13777363300323486, 'token': 737, 'token_str': '표시', 'sequence': '인광성 유기 금속 이리듐 착체, 표시 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.05654741823673248, 'token': 1125, 'token_str': '일렉트로크로믹', 'sequence': '인광성 유기 금속 이리듐 착체, 일렉트로크로믹 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.03776992857456207, 'token': 3316, 'token_str': '광기능', 'sequence': '인광성 유기 금속 이리듐 착체, 광기능 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.03028818406164646, 'token': 663, 'token_str': '전기변색', 'sequence': '인광성 유기 금속 이리듐 착체, 전기변색 소자, 발광 장치, 전자 기기, 및 조명 장치'}]\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.46306195855140686\n",
      "인광성 유기 금속 이리듐 착체, 표시 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.13777363300323486\n",
      "인광성 유기 금속 이리듐 착체, 일렉트로크로믹 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.05654741823673248\n",
      "인광성 유기 금속 이리듐 착체, 광기능 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.03776992857456207\n",
      "인광성 유기 금속 이리듐 착체, 전기변색 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.03028818406164646\n",
      "==================================================\n",
      "[{'score': 0.7140260934829712, 'token': 457, 'token_str': '화합물', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.11365755647420883, 'token': 700, 'token_str': '물질', 'sequence': '본 명세서는 화학식 1로 표시되는 물질 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.02802387624979019, 'token': 557, 'token_str': '조성물', 'sequence': '본 명세서는 화학식 1로 표시되는 조성물 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.008971570990979671, 'token': 887, 'token_str': '유도체', 'sequence': '본 명세서는 화학식 1로 표시되는 유도체 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.007706067059189081, 'token': 1169, 'token_str': '색소', 'sequence': '본 명세서는 화학식 1로 표시되는 색소 및 이를 포함하는 유기 발광 소자에 관한 것이다'}]\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.7140260934829712\n",
      "본 명세서는 화학식 1로 표시되는 물질 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.11365755647420883\n",
      "본 명세서는 화학식 1로 표시되는 조성물 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.02802387624979019\n",
      "본 명세서는 화학식 1로 표시되는 유도체 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.008971570990979671\n",
      "본 명세서는 화학식 1로 표시되는 색소 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.007706067059189081\n",
      "==================================================\n",
      "[{'score': 0.31265953183174133, 'token': 1193, 'token_str': '발광', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 발광 기기, 및 조명 장치'}, {'score': 0.04402359575033188, 'token': 3771, 'token_str': '형광체', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 형광체 기기, 및 조명 장치'}, {'score': 0.02140088379383087, 'token': 4361, 'token_str': '조명', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 조명 기기, 및 조명 장치'}, {'score': 0.019009079784154892, 'token': 4326, 'token_str': '전계', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전계 기기, 및 조명 장치'}, {'score': 0.009488631971180439, 'token': 678, 'token_str': '유기', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 유기 기기, 및 조명 장치'}]\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 발광 기기, 및 조명 장치, confidence: 0.31265953183174133\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 형광체 기기, 및 조명 장치, confidence: 0.04402359575033188\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 조명 기기, 및 조명 장치, confidence: 0.02140088379383087\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전계 기기, 및 조명 장치, confidence: 0.019009079784154892\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 유기 기기, 및 조명 장치, confidence: 0.009488631971180439\n",
      "==================================================\n",
      "[{'score': 0.6072555184364319, 'token': 678, 'token_str': '유기', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.06164100021123886, 'token': 16, 'token_str': ',', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는, 발광 소자에 관한 것이다'}, {'score': 0.05242976173758507, 'token': 450, 'token_str': '전기', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전기 발광 소자에 관한 것이다'}, {'score': 0.04774380475282669, 'token': 4326, 'token_str': '전계', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전계 발광 소자에 관한 것이다'}, {'score': 0.015567452646791935, 'token': 1193, 'token_str': '발광', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 발광 발광 소자에 관한 것이다'}]\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.6072555184364319\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는, 발광 소자에 관한 것이다, confidence: 0.06164100021123886\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전기 발광 소자에 관한 것이다, confidence: 0.05242976173758507\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전계 발광 소자에 관한 것이다, confidence: 0.04774380475282669\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 발광 발광 소자에 관한 것이다, confidence: 0.015567452646791935\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# perform predictions\n",
    "# 인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치\n",
    "# 본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다\n",
    "examples = [\n",
    "    \"인광성 유기 금속 이리듐 착체, [MASK] 소자, 발광 장치, 전자 기기, 및 조명 장치\",\n",
    "    \"본 명세서는 화학식 1로 표시되는 [MASK] 및 이를 포함하는 유기 발광 소자에 관한 것이다\",\n",
    "    \"인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, [MASK] 기기, 및 조명 장치\",\n",
    "    \"본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 [MASK] 발광 소자에 관한 것이다\",\n",
    "]\n",
    "for example in examples:\n",
    "    print(fill_mask(example))\n",
    "    for prediction in fill_mask(example):\n",
    "        print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01c3a7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 1500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 19400\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 7500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 3:28:10, Epoch 99/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.143100</td>\n",
       "      <td>3.477852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.073100</td>\n",
       "      <td>2.746345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.481400</td>\n",
       "      <td>2.393077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.170100</td>\n",
       "      <td>2.215955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.018300</td>\n",
       "      <td>2.143648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-1500\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-1500/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [c09k_pretrained_bert/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-3000\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-3000/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-3000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-4500\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-4500/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [c09k_pretrained_bert/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-6000\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-6000/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [c09k_pretrained_bert/checkpoint-4320] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2047\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to c09k_pretrained_bert/checkpoint-7500\n",
      "Configuration saved in c09k_pretrained_bert/checkpoint-7500/config.json\n",
      "Model weights saved in c09k_pretrained_bert/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [c09k_pretrained_bert/checkpoint-1440] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from c09k_pretrained_bert/checkpoint-7500 (score: 2.143648147583008).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=2.7772110026041665, metrics={'train_runtime': 12492.1984, 'train_samples_per_second': 155.297, 'train_steps_per_second': 0.6, 'total_flos': 6.38038107242496e+16, 'train_loss': 2.7772110026041665, 'epoch': 99.99})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Num examples = 19400\n",
    "# Num Epochs = 50\n",
    "# Total optimization steps = 3750 = 750*50 = \n",
    "# 로드한 모델 추가학습\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,          # output directory to where save model checkpoint\n",
    "    evaluation_strategy=\"steps\",    # 'steps': evaluate each `logging_steps`, 'epoch'  : each epoch\n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=100.,            # number of training epochs, feel free to tweak\n",
    "    per_device_train_batch_size=32, # the training batch size, put it as high as your GPU memory fits\n",
    "    gradient_accumulation_steps=8,  # accumulating the gradients before updating the weights\n",
    "    per_device_eval_batch_size=8,  # evaluation batch size\n",
    "    logging_steps=1500,             # evaluate, log and save model checkpoints every 1000 step\n",
    "    save_steps=1500,\n",
    "    load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training\n",
    "    save_total_limit=5,           # whether you don't have much space so you let only 3 model weights saved in the disk\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model1,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset1,\n",
    "    eval_dataset=test_dataset1,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3724f",
   "metadata": {},
   "source": [
    "### Masked-LM 성능 확인\n",
    "* 모델과 토크나이저를 한번은 로드 해줘야 하네. 그렇지 않으면 토크나이저가 CPU에 로드되어 있는 것으로 인식하여 에러 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ee29baad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file c09k_pretrained_bert/checkpoint-7500/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert/checkpoint-4320\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8000\n",
      "}\n",
      "\n",
      "loading weights file c09k_pretrained_bert/checkpoint-7500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForMaskedLM.\n",
      "\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at c09k_pretrained_bert/checkpoint-7500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file c09k_pretrained_bert/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"c09k_pretrained_bert\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"cls_token\": \"[CLS]\",\n",
      "  \"do_lower_case\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"mask_token\": \"[MASK]\",\n",
      "  \"max_len\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_max_length\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token\": \"[PAD]\",\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token\": \"[SEP]\",\n",
      "  \"transformers_version\": \"4.22.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"unk_token\": \"[UNK]\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_file vocab.txt\n",
      "tokenizer_file tokenizer.json\n",
      "added_tokens_file added_tokens.json\n",
      "special_tokens_map_file special_tokens_map.json\n",
      "tokenizer_config_file tokenizer_config.json\n",
      "[{'score': 0.46306195855140686, 'token': 1193, 'token_str': '발광', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.13777363300323486, 'token': 737, 'token_str': '표시', 'sequence': '인광성 유기 금속 이리듐 착체, 표시 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.05654741823673248, 'token': 1125, 'token_str': '일렉트로크로믹', 'sequence': '인광성 유기 금속 이리듐 착체, 일렉트로크로믹 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.03776992857456207, 'token': 3316, 'token_str': '광기능', 'sequence': '인광성 유기 금속 이리듐 착체, 광기능 소자, 발광 장치, 전자 기기, 및 조명 장치'}, {'score': 0.03028818406164646, 'token': 663, 'token_str': '전기변색', 'sequence': '인광성 유기 금속 이리듐 착체, 전기변색 소자, 발광 장치, 전자 기기, 및 조명 장치'}]\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.46306195855140686\n",
      "인광성 유기 금속 이리듐 착체, 표시 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.13777363300323486\n",
      "인광성 유기 금속 이리듐 착체, 일렉트로크로믹 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.05654741823673248\n",
      "인광성 유기 금속 이리듐 착체, 광기능 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.03776992857456207\n",
      "인광성 유기 금속 이리듐 착체, 전기변색 소자, 발광 장치, 전자 기기, 및 조명 장치, confidence: 0.03028818406164646\n",
      "==================================================\n",
      "[{'score': 0.7140260934829712, 'token': 457, 'token_str': '화합물', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.11365755647420883, 'token': 700, 'token_str': '물질', 'sequence': '본 명세서는 화학식 1로 표시되는 물질 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.02802387624979019, 'token': 557, 'token_str': '조성물', 'sequence': '본 명세서는 화학식 1로 표시되는 조성물 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.008971570990979671, 'token': 887, 'token_str': '유도체', 'sequence': '본 명세서는 화학식 1로 표시되는 유도체 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.007706067059189081, 'token': 1169, 'token_str': '색소', 'sequence': '본 명세서는 화학식 1로 표시되는 색소 및 이를 포함하는 유기 발광 소자에 관한 것이다'}]\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.7140260934829712\n",
      "본 명세서는 화학식 1로 표시되는 물질 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.11365755647420883\n",
      "본 명세서는 화학식 1로 표시되는 조성물 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.02802387624979019\n",
      "본 명세서는 화학식 1로 표시되는 유도체 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.008971570990979671\n",
      "본 명세서는 화학식 1로 표시되는 색소 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.007706067059189081\n",
      "==================================================\n",
      "[{'score': 0.31265953183174133, 'token': 1193, 'token_str': '발광', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 발광 기기, 및 조명 장치'}, {'score': 0.04402359575033188, 'token': 3771, 'token_str': '형광체', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 형광체 기기, 및 조명 장치'}, {'score': 0.02140088379383087, 'token': 4361, 'token_str': '조명', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 조명 기기, 및 조명 장치'}, {'score': 0.019009079784154892, 'token': 4326, 'token_str': '전계', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전계 기기, 및 조명 장치'}, {'score': 0.009488631971180439, 'token': 678, 'token_str': '유기', 'sequence': '인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 유기 기기, 및 조명 장치'}]\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 발광 기기, 및 조명 장치, confidence: 0.31265953183174133\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 형광체 기기, 및 조명 장치, confidence: 0.04402359575033188\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 조명 기기, 및 조명 장치, confidence: 0.02140088379383087\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전계 기기, 및 조명 장치, confidence: 0.019009079784154892\n",
      "인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 유기 기기, 및 조명 장치, confidence: 0.009488631971180439\n",
      "==================================================\n",
      "[{'score': 0.6072555184364319, 'token': 678, 'token_str': '유기', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다'}, {'score': 0.06164100021123886, 'token': 16, 'token_str': ',', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는, 발광 소자에 관한 것이다'}, {'score': 0.05242976173758507, 'token': 450, 'token_str': '전기', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전기 발광 소자에 관한 것이다'}, {'score': 0.04774380475282669, 'token': 4326, 'token_str': '전계', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전계 발광 소자에 관한 것이다'}, {'score': 0.015567452646791935, 'token': 1193, 'token_str': '발광', 'sequence': '본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 발광 발광 소자에 관한 것이다'}]\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다, confidence: 0.6072555184364319\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는, 발광 소자에 관한 것이다, confidence: 0.06164100021123886\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전기 발광 소자에 관한 것이다, confidence: 0.05242976173758507\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 전계 발광 소자에 관한 것이다, confidence: 0.04774380475282669\n",
      "본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 발광 발광 소자에 관한 것이다, confidence: 0.015567452646791935\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# load the model checkpoint\n",
    "model1 = BertForMaskedLM.from_pretrained(os.path.join(model_path, \"checkpoint-7500\"))\n",
    "# load the tokenizer\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "tokenizer1 = BertTokenizerFast.from_pretrained(model_path, vocab_size=8000, local_files_only=True)\n",
    "fill_mask = pipeline(\"fill-mask\", model=model1, tokenizer=tokenizer1)\n",
    "# perform predictions\n",
    "# 인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, 전자 기기, 및 조명 장치\n",
    "# 본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 유기 발광 소자에 관한 것이다\n",
    "examples = [\n",
    "    \"인광성 유기 금속 이리듐 착체, [MASK] 소자, 발광 장치, 전자 기기, 및 조명 장치\",\n",
    "    \"본 명세서는 화학식 1로 표시되는 [MASK] 및 이를 포함하는 유기 발광 소자에 관한 것이다\",\n",
    "    \"인광성 유기 금속 이리듐 착체, 발광 소자, 발광 장치, [MASK] 기기, 및 조명 장치\",\n",
    "    \"본 명세서는 화학식 1로 표시되는 화합물 및 이를 포함하는 [MASK] 발광 소자에 관한 것이다\",\n",
    "]\n",
    "for example in examples:\n",
    "    print(fill_mask(example))\n",
    "    for prediction in fill_mask(example):\n",
    "        print(f\"{prediction['sequence']}, confidence: {prediction['score']}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e010c90",
   "metadata": {},
   "source": [
    "### 그 외 참고용 자료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명선 책임의 문장 분리 코드(3글자 이후 마침표가 등장하는 경우에 간혹 분리되는 경우가 있다. 미사용)\n",
    "# REG_SENT_KO=r'([ㄱ-ㅣ가-힣]+[.]|[\\n]|[:;!?])'\n",
    "# REG_SENT_EN=r'([a-zA-Z]+[.]\\s|[\\n]|[:;!?])'\n",
    "\n",
    "# def split_sentence(doc, regex):\n",
    "#     s = 0\n",
    "#     for m in re.finditer(regex, doc):\n",
    "#         sent = doc[s:m.end()].strip()\n",
    "#         s = m.end()\n",
    "#         if not sent:\n",
    "#             continue\n",
    "#         yield sent\n",
    "\n",
    "#     if s < len(doc):\n",
    "#         sent = doc[s:].strip()\n",
    "#         if sent:\n",
    "#             yield sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5420d",
   "metadata": {},
   "source": [
    "### using personal dataset\n",
    "* https://huggingface.co/docs/datasets/dataset_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you have huge custom dataset separated into files\n",
    "# # load the splitted files\n",
    "# files = [\"train1.txt\", \"train2.txt\"] # train3.txt, etc.\n",
    "# dataset = load_dataset(\"text\", data_files=files, split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4154edda",
   "metadata": {},
   "source": [
    "### 문장 클린징 처리 전/후 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc03d8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이산화티탄-염화리튬 전기변색물질, 이를 이용한전기변색장치용 파우더 및 필름전극의 제조방법', '본 발명은 이산화티탄-염화리튬 전기변색물질 및 이를 이용한 전기변색장치용 파우더 및 필름전극의 제조방법에 관한 것으로, 이산화티탄에 염화리튬이 균일하게 혼합된 물질을 합성하여 제조되며 상기 이산화티탄-염화리튬에서 티탄 대 리튬의 몰 함량 비율이 1 : x (0.5 x 2) 범위를 갖는 이산화티탄-염화리튬을 졸-겔 방법으로 합성함으로써 표면적이 극대화되는 다공성화 또는 그 물질 내에 존재하는 높은 함량의 리튬 이온으로 인하여 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 매우 뛰어난 발명인 것이다.', '이산화티탄-염화리튬 파우더에 아세틸아세톤이 포함된 증류수를 첨가하여 끈적거리는 상태가 될 때까지 교반하는 단계; 상기 용액에 증류수와 트리톤 X-100을 첨가하여 완전히 균일한 용액이 될 때까지 다시 교반하는 단계; 및 상기 용액을 전도성 유리인 ITO 위에 얇게 입힌 후, 상온에서 건조한 다음 열처리하는 단계를 포함함을 특징으로 하는 전기변색장치용 이산화티탄-염화리튬 필름전극의 제조방법.', '1 발명은 상기의 제문제점을 감안하여 안출한 것으로, 본 발명에서는 이산화티탄-염화리튬을 새롭게 합성하여 전기변색 물질로서 실용화함에 그 목적이 있다.', '2 발명의 다른 목적은 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 이산화티탄-염화리튬 전기변색물질을 제공하는데 있다.']\n",
      "['전기 화학 디바이스 및 그것을 형성하는 방법', '전기 화학 디바이스에 대해서, 개시한다.전기 화학 디바이스는, 제 1의 투명 도전층, 제 1의 투명 도전층 위에 있는 에렉트로 크로믹층, 에렉트로 크로믹층 위에 있는 대향 전극층, 제 2의 투명 도전층을 갖추어 23°C으로 0.68초/mm이하의 스위칭 속도 파라미터를 가진다.', '제 1의 투명 도전층과 전기 제 1의 투명 도전층 위에 있는 음극 전기 화학층과 전기 음극 전기 화학층 위에 있는 양극 전기 화학층과 제 2의 투명 도전층과 (을)를 갖추는 전기 화학 디바이스이며, 전기 전기 화학 디바이스는, 23°C으로 0.68초/mm이하인 제 1의 스위칭 속도 파라미터, -20°C으로 1.0초/mm이하인 제 2의 스위칭 속도 파라미터, 1.5/log(오옴) 미만인 log|Z|의 impedance파라미터, 또는 -20°C으로 8%미만인 착색 투과률 파라미터 중 적어도 1개를 가지는, 전기 화학 디바이스.', '이산화 바나듐 함유 입자, 사모 크로믹 필름 및 이산화 바나듐 함유 입자의 제조 방법', '사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.65~2.05의 범위내인 원소 A와 폴링의 전기 음성도가 2.10~2.54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['text'][0:5])\n",
    "print(dataset['text'][-5:])\n",
    "# 문장 클린징 처리 전\n",
    "# ['이산화티탄-염화리튬 전기변색물질, 이를 이용한전기변색장치용 파우더 및 필름전극의 제조방법', '본 발명은 이산화티탄-염화리튬 전기변색물질 및 이를 이용한 전기변색장치용 파우더 및 필름전극의 제조방법에 관한 것으로, 이산화티탄에 염화리튬이 균일하게 혼합된 물질을 합성하여 제조되며 상기 이산화티탄-염화리튬에서 티탄 대 리튬의 몰 함량 비율이 1 : x (0.5 &lt; x &lt; 2) 범위를 갖는 이산화티탄-염화리튬을 졸-겔 방법으로 합성함으로써 표면적이 극대화되는 다공성화 또는 그 물질 내에 존재하는 높은 함량의 리튬 이온으로 인하여 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 매우 뛰어난 발명인 것이다.', '이산화티탄-염화리튬 파우더에 아세틸아세톤이 포함된 증류수를 첨가하여 끈적거리는 상태가 될 때까지 교반하는 단계; 상기 용액에 증류수와 트리톤 X-100을 첨가하여 완전히 균일한 용액이 될 때까지 다시 교반하는 단계; 및 상기 용액을 전도성 유리인 ITO 위에 얇게 입힌 후, 상온에서 건조한 다음 열처리하는 단계를 포함함을 특징으로 하는 전기변색장치용 이산화티탄-염화리튬 필름전극의 제조방법.', '1@@발명은 상기의 제문제점을 감안하여 안출한 것으로, 본 발명에서는 이산화티탄-염화리튬을 새롭게 합성하여 전기변색 물질로서 실용화함에 그 목적이 있다.', '2@@발명의 다른 목적은 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 이산화티탄-염화리튬 전기변색물질을 제공하는데 있다.']\n",
    "# ['사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며,', '산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.', '６５〜２． 05의 범위내인 원소 A와 폴링의 전기 음성도가 2.', '１０〜２． 54의 범위내인 원소 B만이 함유 되어', '바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\n",
    "# 처리 후에는 1문장으로 처리될 데이터가 무려 5문장으로 나눠진다!\n",
    "\n",
    "# 문장 클린징 처리 후\n",
    "# ['이산화티탄-염화리튬 전기변색물질, 이를 이용한전기변색장치용 파우더 및 필름전극의 제조방법', '본 발명은 이산화티탄-염화리튬 전기변색물질 및 이를 이용한 전기변색장치용 파우더 및 필름전극의 제조방법에 관한 것으로, 이산화티탄에 염화리튬이 균일하게 혼합된 물질을 합성하여 제조되며 상기 이산화티탄-염화리튬에서 티탄 대 리튬의 몰 함량 비율이 1 : x (0.5 x 2) 범위를 갖는 이산화티탄-염화리튬을 졸-겔 방법으로 합성함으로써 표면적이 극대화되는 다공성화 또는 그 물질 내에 존재하는 높은 함량의 리튬 이온으로 인하여 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 매우 뛰어난 발명인 것이다.', '이산화티탄-염화리튬 파우더에 아세틸아세톤이 포함된 증류수를 첨가하여 끈적거리는 상태가 될 때까지 교반하는 단계; 상기 용액에 증류수와 트리톤 X-100을 첨가하여 완전히 균일한 용액이 될 때까지 다시 교반하는 단계; 및 상기 용액을 전도성 유리인 ITO 위에 얇게 입힌 후, 상온에서 건조한 다음 열처리하는 단계를 포함함을 특징으로 하는 전기변색장치용 이산화티탄-염화리튬 필름전극의 제조방법.', '1 발명은 상기의 제문제점을 감안하여 안출한 것으로, 본 발명에서는 이산화티탄-염화리튬을 새롭게 합성하여 전기변색 물질로서 실용화함에 그 목적이 있다.', '2 발명의 다른 목적은 전기변색 감응시간을 획기적으로 줄이고 전기변색 효율을 크게 향상시킬 수 있는 이산화티탄-염화리튬 전기변색물질을 제공하는데 있다.']\n",
    "# ['전기 화학 디바이스 및 그것을 형성하는 방법', '전기 화학 디바이스에 대해서, 개시한다.전기 화학 디바이스는, 제 1의 투명 도전층, 제 1의 투명 도전층 위에 있는 에렉트로 크로믹층, 에렉트로 크로믹층 위에 있는 대향 전극층, 제 2의 투명 도전층을 갖추어 23°C으로 0.68초/mm이하의 스위칭 속도 파라미터를 가진다.', '제 1의 투명 도전층과 전기 제 1의 투명 도전층 위에 있는 음극 전기 화학층과 전기 음극 전기 화학층 위에 있는 양극 전기 화학층과 제 2의 투명 도전층과 (을)를 갖추는 전기 화학 디바이스이며, 전기 전기 화학 디바이스는, 23°C으로 0.68초/mm이하인 제 1의 스위칭 속도 파라미터, -20°C으로 1.0초/mm이하인 제 2의 스위칭 속도 파라미터, 1.5/log(오옴) 미만인 log|Z|의 impedance파라미터, 또는 -20°C으로 8%미만인 착색 투과률 파라미터 중 적어도 1개를 가지는, 전기 화학 디바이스.', '이산화 바나듐 함유 입자, 사모 크로믹 필름 및 이산화 바나듐 함유 입자의 제조 방법', \n",
    "#  '사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.65〜2.05의 범위내인 원소 A와 폴링의 전기 음성도가 2.10〜2.54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f00a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# ['사모 크로믹성을 가지는 이산화 바나듐 함유 입자이며, 산소 및 바나듐 이외의 원소로서, 폴링의 전기 음성도가 1.',\n",
    "#  '６５〜２． 05의 범위내인 원소 A와 폴링의 전기 음성도가 2.',\n",
    "#  '１０〜２． 54의 범위내인 원소 B만이 함유 되어 바나듐(100 atom%)에 대해, 전기 원소 A의 총함유량이 0.5~20 atom%의 범위내이며, 한편, 전기 원소 B의 총함유량이 0.05~20 atom%의 범위내인 이산화 바나듐 함유 입자.']\n",
    "#  위와 같이 소수점 앞 뒤로 공백이 존재하는데, 이로 인해 소수점에서 문장 분리가 되버린다. 오류 조치가 필요\n",
    "#  \"\"\"\n",
    "# 아래와 같이 구두점 앞뒤 블랭크를 제거하여 일단 구분\n",
    "# row[col] = unicodedata.normalize('NFKC', row[col])\n",
    "# https://blog.naver.com/PostView.nhn?blogId=duswl0319&logNo=221516880642&from=search&redirect=Log&widgetTypeCall=true&directAccess=false\n",
    "# row[col] = row[col].replace(' . ','.')\n",
    "# row[col] = row[col].replace(' ． ','.')\n",
    "# row[col] = row[col].replace('． ','.')\n",
    "# row[col] = row[col].replace('. ','.')\n",
    "# row[col] = row[col].replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c46427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to train the tokenizer from scratch (especially if you have custom\n",
    "# dataset loaded as datasets object), then run this cell to save it as files\n",
    "# but if you already have your custom data as text files, there is no point using this\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    \"\"\"Utility function to save dataset text to disk,\n",
    "    useful for using the texts to train the tokenizer \n",
    "    (as the tokenizer accepts files)\"\"\"\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for t in dataset[\"text\"]:\n",
    "            print(t, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train_dataset.features.keys()\n",
    "# # dict_keys(['text', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'])\n",
    "# total_length = len(list(chain(train_dataset['text'])))  # total_length = 14769\n",
    "# max_length  # 512\n",
    "# total_length = (total_length // max_length) * max_length\n",
    "# # total_length  # 14336, total_length // max_length = 28.8457..., 28 * 512 = 14336"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_pretrain",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
